================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors
文件名: dump_python_sources.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
递归导出目录下所有 Python 文件内容到一个 txt 文件

输出格式：
========================================
文件路径: <absolute_or_relative_path>
文件名: <filename.py>
----------------------------------------
<file content>
========================================
"""

import os
from pathlib import Path


def dump_python_files(
    root_dir: Path,
    out_txt: Path,
    encoding: str = "utf-8"
):
    root_dir = root_dir.resolve()

    with out_txt.open("w", encoding="utf-8") as fout:
        for py_file in sorted(root_dir.rglob("*.py")):
            # 可选：跳过虚拟环境 / 缓存目录
            if any(part in {"__pycache__", ".venv", "venv", "env"} for part in py_file.parts):
                continue

            fout.write("=" * 80 + "\n")
            fout.write(f"文件路径: {py_file.parent}\n")
            fout.write(f"文件名: {py_file.name}\n")
            fout.write("-" * 80 + "\n")

            try:
                content = py_file.read_text(encoding=encoding)
            except UnicodeDecodeError:
                # 如果不是 utf-8，尝试忽略错误读取
                content = py_file.read_text(encoding=encoding, errors="ignore")

            fout.write(content)
            if not content.endswith("\n"):
                fout.write("\n")

            fout.write("=" * 80 + "\n\n")


if __name__ == "__main__":
    # ===== 你只需要改这里 =====
    ROOT_DIR = Path(".")              # 要遍历的目录（当前目录）
    OUTPUT_TXT = Path("all_py_dump.txt")  # 输出文件名

    dump_python_files(ROOT_DIR, OUTPUT_TXT)
    print(f"[OK] Python 源码已导出到: {OUTPUT_TXT.resolve()}")
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: biometric_comparison.py
--------------------------------------------------------------------------------
from flx.data.dataset import (
    Identifier,
)


class BiometricComparison:
    def __init__(self, sample1: Identifier, sample2: Identifier):
        self.sample1: Identifier = sample1
        self.sample2: Identifier = sample2

    @property
    def mated(self) -> bool:
        return self.sample1.subject == self.sample2.subject

    def __str__(self) -> str:
        return f"BiometricComparison({self.sample1}, {self.sample2})"


class BiometricComparisonResult:
    def __init__(self, comparison: BiometricComparison, similarity: float):
        self.comparison: BiometricComparison = comparison
        self.similarity: float = similarity

    def __str__(self) -> str:
        return f"BiometricComparisonResult({self.comparison.sample1}, {self.comparison.sample2}, {self.similarity})"


def biometric_comparisons_to_json(comparisons: list[BiometricComparison]) -> None:
    return {
        "array_sample1": Identifier.ids_to_json([comp.sample1 for comp in comparisons]),
        "array_sample2": Identifier.ids_to_json([comp.sample2 for comp in comparisons]),
    }


def biometric_comparisons_from_json(jsn: dict) -> list[BiometricComparison]:
    array_sample1 = Identifier.ids_from_json(jsn["array_sample1"])
    array_sample2 = Identifier.ids_from_json(jsn["array_sample2"])
    return [
        BiometricComparison(sample1, sample2)
        for sample1, sample2 in zip(array_sample1, array_sample2)
    ]


def biometric_comparison_results_to_json(
    path: str, results: list[BiometricComparisonResult]
) -> None:
    return {
        "array_comparison": biometric_comparisons_to_json(
            [res.comparison for res in results]
        ),
        "array_similarity": [float(res.similarity) for res in results],
    }


def biometric_comparison_results_from_json(
    jsn: dict,
) -> list[BiometricComparisonResult]:
    comparisons = biometric_comparisons_from_json(jsn["array_comparison"])
    similarities = jsn["array_similarity"]
    return [
        BiometricComparisonResult(comp, sim)
        for comp, sim in zip(comparisons, similarities)
    ]
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: biometric_search.py
--------------------------------------------------------------------------------
import numpy as np

from flx.data.dataset import (
    Identifier,
)


class ExhaustiveSearch:
    """
    Uses numpy arrays for efficiency. The gallery should never be modified as all Searches share a reference to
    the same gallery.

    @param probe : identifier of the probe
    @param gallery : identifiers of biometric references in the enrolment dataset
    @param similarities : similarity scores for comparisons with the references in gallery
        can be left None when the search is created, but must be set before accessing the search statistics.

    """

    def __init__(
        self, probe: Identifier, gallery: np.ndarray[Identifier], is_mated: bool
    ):
        self.probe: Identifier = probe
        self.is_mated: bool = is_mated
        self.gallery: np.ndarray[Identifier] = gallery


def _calculate_rank(search: ExhaustiveSearch, similarities: np.ndarray):
    if not search.is_mated:
        return -1
    idxs_sorted: np.ndarray = np.argsort(similarities)
    rank = 0
    for idx in np.flip(idxs_sorted)[:]:
        rank += 1
        if search.gallery[idx].subject == search.probe.subject:
            return rank


class ExhaustiveSearchResult:
    def __init__(self, search: ExhaustiveSearch, similarity: float, rank: int):
        """
        'rank' is the position of the first mated comparison in a sorted list of all comparisons in the query
        (sorted by descending similarity).
        If the search was non-mated -1 is returned.
        """
        self.search: ExhaustiveSearch = search
        self.rank: int = rank
        self.similarity: float = similarity

    def is_positive_identification(self, threshold: float) -> bool:
        """
        Returns whether the identification decision using the specified threshold is positive or negative.
        """
        return threshold <= self.similarity

    @staticmethod
    def from_similarity_scores(
        search: ExhaustiveSearch, gallery_similarities: np.ndarray[np.float16]
    ) -> "ExhaustiveSearchResult":
        return ExhaustiveSearchResult(
            search,
            rank=_calculate_rank(search, gallery_similarities),
            similarity=float(np.amax(gallery_similarities)),
        )


def exhaustive_searches_to_json(searches: list[ExhaustiveSearch]) -> dict:
    assert len(searches) > 0
    gallery: np.ndarray[Identifier] = searches[0].gallery
    probes: list[Identifier] = [s.probe for s in searches]
    return {
        "gallery": Identifier.ids_to_json(gallery),
        "probes": Identifier.ids_to_json(probes),
        "mated": [s.is_mated for s in searches],
    }


def exhaustive_searches_from_json(jsn: dict) -> list[ExhaustiveSearch]:
    gallery: np.ndarray[Identifier] = np.squeeze(
        np.array(Identifier.ids_from_json(jsn["gallery"]))
    )
    probes: list[Identifier] = Identifier.ids_from_json(jsn["probes"])
    mated: list[bool] = jsn["mated"]
    return [
        ExhaustiveSearch(probe=p, gallery=gallery, is_mated=m)
        for p, m in zip(probes, mated)
    ]


def exhaustive_search_results_to_json(results: list[ExhaustiveSearchResult]) -> dict:
    return {
        "searches": exhaustive_searches_to_json([r.search for r in results]),
        "ranks": [r.rank for r in results],
        "similarities": [r.similarity for r in results],
    }


def exhaustive_search_results_from_json(jsn: dict) -> list[ExhaustiveSearchResult]:
    searches: list[ExhaustiveSearch] = exhaustive_searches_from_json(jsn["searches"])
    ranks: list[int] = jsn["ranks"]
    similarities: list[float] = jsn["similarities"]
    assert len(searches) == len(ranks)
    assert len(searches) == len(similarities)
    return [
        ExhaustiveSearchResult(se, rank=r, similarity=si)
        for se, r, si in zip(searches, ranks, similarities)
    ]
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: identification.py
--------------------------------------------------------------------------------
import json
import os
from os.path import join

import tqdm
import numpy as np

from flx.data.dataset import Identifier
from flx.benchmarks.matchers import BiometricMatcher, VectorizedMatcher
from flx.benchmarks.biometric_search import (
    ExhaustiveSearch,
    ExhaustiveSearchResult,
    exhaustive_searches_to_json,
    exhaustive_searches_from_json,
    exhaustive_search_results_to_json,
    exhaustive_search_results_from_json,
)


class FoldResult:
    def __init__(self, search_results: list[ExhaustiveSearchResult]):
        assert len(search_results) > 0
        self.gallery: np.ndarray[Identifier] = search_results[0].search.gallery
        self._mated_results: list[ExhaustiveSearchResult] = []
        self._non_mated_results: list[ExhaustiveSearchResult] = []
        for result in search_results:
            assert id(result.search.gallery) == id(self.gallery)
            if result.search.is_mated:
                self._mated_results.append(result)
            else:
                self._non_mated_results.append(result)
        self._mated_ranks: list[int] = [s.rank for s in self._mated_results]
        self._mated_similarities: np.ndarray = [
            s.similarity for s in self._mated_results
        ]
        self._non_mated_similarities: np.ndarray = [
            s.similarity for s in self._non_mated_results
        ]

    def false_positive_identification_rate(self, threshold: float) -> float:
        """
        Returns the rate of non-mated searches where an enrolled reference is returned.
        """
        assert len(self._non_mated_results) > 0
        return np.sum(self._non_mated_similarities >= threshold) / len(
            self._non_mated_results
        )

    def false_negative_identification_rate(
        self, threshold: float = None, fpir: float = None
    ) -> float:
        """
        Returns the rate of mated searches where the probe was not in the candidate list given the threshold.

        Can be used with either a fixed threshold or a number of candidates.
        """
        if threshold is not None:
            return np.sum(self._mated_similarities < threshold) / len(
                self._mated_results
            )
        assert fpir is not None
        nms_sorted = np.sort(self._non_mated_similarities)
        num_fm = int(len(self._non_mated_similarities) * fpir) + 1
        return self.false_negative_identification_rate(threshold=nms_sorted[-num_fm])

    def get_mated_ranks(self) -> list[int]:
        return self._mated_ranks

    def get_mated_similarities(self) -> list[float]:
        return self._mated_similarities

    def get_highest_non_mated_similarities(self) -> list[float]:
        return self._non_mated_similarities

    def save(self, path: str) -> None:
        results = self._mated_results + self._non_mated_results
        jsn = exhaustive_search_results_to_json(results)
        with open(os.path.join(path, "searches.json"), "w") as f:
            json.dump(jsn, f)

    @staticmethod
    def load(path: str) -> dict:
        with open(os.path.join(path, "searches.json"), "r") as f:
            jsn = json.load(f)
        results = exhaustive_search_results_from_json(jsn)
        return FoldResult(results)


class IdentificationResult:
    def __init__(self, results: list[FoldResult]):
        self._results = results

    def false_positive_identification_rate(self, threshold: float) -> float:
        rates = [
            result.false_positive_identification_rate(threshold)
            for result in self._results
        ]
        return sum(rates) / len(rates)

    def false_negative_identification_rate(
        self, threshold: float = None, fpir: int = None
    ) -> float:
        rates = [
            result.false_negative_identification_rate(threshold=threshold, fpir=fpir)
            for result in self._results
        ]
        return sum(rates) / len(rates)

    def get_mated_ranks(self) -> list[int]:
        # return the concatenated list of mated ranks for each result
        return [rank for result in self._results for rank in result.get_mated_ranks()]

    def get_mated_similarities(self) -> list[float]:
        # return the concatenated list of mated similarities for each result
        return [
            sim for result in self._results for sim in result.get_mated_similarities()
        ]

    def get_highest_non_mated_similarities(self) -> list[float]:
        # return the concatenated list of highest non-mated similarities for each result
        return [
            sim
            for result in self._results
            for sim in result.get_highest_non_mated_similarities()
        ]

    def save(self, path: str) -> None:
        for i, r in enumerate(self._results):
            fold_dir = os.path.join(path, f"fold_{i}")
            if not os.path.exists(fold_dir):
                os.makedirs(fold_dir)
            r.save(fold_dir)

    @staticmethod
    def load(path: str) -> dict:
        n_folds = len([f.name for f in os.scandir(path) if f.is_dir()])
        if n_folds == 0:
            raise RuntimeError(
                f"Cannot load identification results from {path}: The directory is empty!"
            )
        print(f"Loading identification results for {n_folds} cross-validation folds")
        return IdentificationResult(
            [FoldResult.load(join(path, f"fold_{i}")) for i in range(n_folds)]
        )


class IdentificationBenchmark:
    def __init__(self, folds: list[list[ExhaustiveSearch]]):
        self._folds: list[list[ExhaustiveSearch]] = folds

    def _run_single_fold(self, matcher: BiometricMatcher, fold: int) -> FoldResult:
        assert 0 <= fold and fold < len(self._folds)
        searches = self._folds[fold]

        gallery: np.ndarray[Identifier] = searches[0].gallery
        for s in searches:
            assert id(s.gallery) == id(gallery)

        results = []
        if isinstance(matcher, VectorizedMatcher):
            print("Running benchmark vectorized")
            matcher.preload_vectorized(gallery)
            for search in tqdm.tqdm(searches):
                similarities = matcher.vectorized_similarity(search.probe)
                results.append(
                    ExhaustiveSearchResult.from_similarity_scores(search, similarities)
                )
        else:
            print("Running non-vectorized")
            for search in tqdm.tqdm(searches):
                similarities = np.array(
                    [matcher.similarity(search.probe, sample) for sample in gallery]
                )
                results.append(
                    ExhaustiveSearchResult.from_similarity_scores(search, similarities)
                )
        return FoldResult(results)

    def run(self, matcher: BiometricMatcher) -> IdentificationResult:
        return IdentificationResult(
            [self._run_single_fold(matcher, i) for i in range(len(self._folds))]
        )

    def save(self, path: str) -> None:
        jsn = [exhaustive_searches_to_json(s) for s in self._folds]
        with open(path, "w") as f:
            json.dump(jsn, f)

    @staticmethod
    def load(path: str) -> "IdentificationBenchmark":
        with open(path, "r") as f:
            jsn = json.load(f)
        folds = [exhaustive_searches_from_json(s) for s in jsn]
        return IdentificationBenchmark(folds=folds)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: matchers.py
--------------------------------------------------------------------------------
from abc import ABC, abstractmethod

import numpy as np

from flx.data.dataset import Identifier
from flx.data.embedding_loader import EmbeddingLoader
from flx.data.dataset import Identifier


class BiometricMatcher(ABC):
    @abstractmethod
    def similarity(self, sample1: Identifier, sample2: Identifier) -> float:
        raise NotImplementedError()


class VectorizedMatcher(BiometricMatcher):
    @abstractmethod
    def preload_vectorized(self, samples: list[Identifier]) -> None:
        """
        Preloads all samples into one numpy ndarray for vectorized comparison.
        """
        raise NotImplementedError()

    @abstractmethod
    def vectorized_similarity(self, sample: Identifier) -> np.ndarray[float]:
        """
        Similarities with all the samples in the preloaded vector.
        """
        raise NotImplementedError()


class CosineSimilarityMatcher(VectorizedMatcher):
    def __init__(self, embedding_dataset: EmbeddingLoader):
        self._embeddings = embedding_dataset
        self._matrix = None

    def similarity(self, sample1: Identifier, sample2: Identifier) -> float:
        emb1 = self._embeddings.get(sample1)
        emb2 = self._embeddings.get(sample2)
        return np.dot(emb1, emb2)

    def preload_vectorized(self, samples: list[Identifier]) -> None:
        """
        Preloads all samples into one numpy ndarray for vectorized comparison.
        """
        vectors = [self._embeddings.get(s) for s in samples]
        self._matrix = np.stack(vectors)

    def vectorized_similarity(self, sample: Identifier) -> np.ndarray[float]:
        """
        Similarities for all the items in the preloaded vector.
        """
        emb = self._embeddings.get(sample)
        vals = np.matmul(self._matrix, emb.vector)
        # Negative similarity makes no sense, as a fingerprint does not have an opposite
        vals[vals < 0] = 0
        return vals
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/benchmarks
文件名: verification.py
--------------------------------------------------------------------------------
import os
import json

import tqdm
import numpy as np

from flx.benchmarks.matchers import BiometricMatcher
from flx.benchmarks.biometric_comparison import (
    BiometricComparison,
    BiometricComparisonResult,
    biometric_comparisons_to_json,
    biometric_comparisons_from_json,
    biometric_comparison_results_to_json,
    biometric_comparison_results_from_json,
)


class VerificationResult:
    def __init__(self, comparison_results: list[BiometricComparisonResult]):
        self._mated_scores: list[float] = []
        self._mated_comparisons: list[BiometricComparisonResult] = []
        self._non_mated_scores: list[float] = []
        self._non_mated_comparisons: list[BiometricComparisonResult] = []
        for res in comparison_results:
            if res.comparison.mated:
                self._mated_scores.append(res.similarity)
                self._mated_comparisons.append(res)
                continue
            self._non_mated_scores.append(res.similarity)
            self._non_mated_comparisons.append(res)

        self._mated_scores: np.ndarray = np.array(self._mated_scores)
        self._non_mated_scores: np.ndarray = np.array(self._non_mated_scores)

    def threshold_for_fmr(self, fmr: float):
        """
        Returns the threshold corresponding to the given False Match Rate (FMR).
        FMR := #(non-mated comparisons that matched) / #(non-mated comparisons).
        """
        nms_sorted = np.sort(self._non_mated_scores)
        num_fm = int(self._non_mated_scores.shape[0] * fmr) + 1
        return nms_sorted[-num_fm]

    def false_match_rate(self, thresholds: list[float]) -> list[float]:
        """
        Returns the false match rate when the given thresholds are applied
        """
        ratios = []
        for t in thresholds:
            is_match = self._non_mated_scores >= t
            ratios.append(np.sum(is_match) / self._non_mated_scores.shape[0])
        return ratios

    def false_non_match_rate(self, thresholds: list[float]) -> list[float]:
        """
        Returns the false non match rate when the given thresholds are applied
        """
        ratios = []
        for t in thresholds:
            is_no_match = self._mated_scores < t
            ratios.append(np.sum(is_no_match) / self._mated_scores.shape[0])
        return ratios

    def get_mated_scores(self) -> np.ndarray[float]:
        return self._mated_scores

    def get_non_mated_scores(self) -> np.ndarray[float]:
        return self._non_mated_scores

    def get_equal_error_rate(self) -> float:
        scores = np.concatenate([self._mated_scores, self._non_mated_scores])
        is_mated = np.concatenate(
            [
                np.ones(len(self._mated_scores), dtype=bool),
                np.zeros(len(self._non_mated_scores), dtype=bool),
            ]
        )
        # Sort by similarity (ascending)
        idxs = np.argsort(scores)
        scores = scores[idxs]
        is_mated = is_mated[idxs]

        # Calculate number of mated comparisons in subset of comparisons
        num_mated_cumulative = np.cumsum(is_mated, dtype=np.uint32)
        num_non_mated_cumulative = np.cumsum(np.logical_not(is_mated), dtype=np.uint32)
        fnmr = num_mated_cumulative / len(self._mated_comparisons)
        fmr = (len(self._non_mated_comparisons) - num_non_mated_cumulative) / len(
            self._non_mated_comparisons
        )
        diff_errors = np.abs(fnmr - fmr)
        idx = np.argmin(diff_errors)
        return fmr[idx]

    def save(self, path: str) -> None:
        jsn = biometric_comparison_results_to_json(
            path, self._mated_comparisons + self._non_mated_comparisons
        )
        with open(path, "w") as f:
            json.dump(jsn, f)

    @staticmethod
    def load(path: str) -> "VerificationResult":
        if not os.path.exists(path):
            raise RuntimeError(f"Cannot load results: File {path} does not exist")
        with open(path, "r") as f:
            jsn = json.load(f)
        return VerificationResult(biometric_comparison_results_from_json(jsn))


class VerificationBenchmark:
    def __init__(self, comparisons: list[BiometricComparison]):
        self._comparisons: list[BiometricComparison] = comparisons

    def run(self, matcher: BiometricMatcher) -> VerificationResult:
        results = []
        for comp in tqdm.tqdm(self._comparisons):
            similarity = matcher.similarity(comp.sample1, comp.sample2)
            results.append(BiometricComparisonResult(comp, similarity))
        return VerificationResult(results)

    def save(self, path: str) -> None:
        jsn = biometric_comparisons_to_json(self._comparisons)
        with open(path, "w") as f:
            json.dump(jsn, f)

    @staticmethod
    def load(path: str) -> "VerificationBenchmark":
        with open(path, "r") as f:
            jsn = json.load(f)
        comparisons = biometric_comparisons_from_json(jsn)
        return VerificationBenchmark(comparisons)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: dataset.py
--------------------------------------------------------------------------------
from typing import Iterable, Hashable
from abc import ABC, abstractmethod, abstractproperty
from collections import defaultdict


class Identifier:
    """
    An identifier for data related to a fingerprint sample.
        subject: An identifier for a specific fingerprint
        impression: An identifier for a sample taken of a fingerprint. E.g. if there are three different samples
            of the same fingerprint they all have the same value for 'subject' but different values for 'impression'
    """

    def __init__(self, subject: int, impression: int):
        self.subject: int = int(subject)
        self.impression: int = int(impression)

    def __hash__(self):
        return hash((self.subject, self.impression))

    def __eq__(self, other):
        return hash(self) == hash(other)

    def __str__(self) -> str:
        return f"Identifier({self.subject}, {self.impression})"

    def __lt__(self, other: "Identifier"):
        return self.subject < other.subject or (
            self.subject == other.subject and self.impression < other.impression
        )

    @staticmethod
    def ids_to_json(ids: Iterable["Identifier"]) -> dict:
        return {
            "array_subject": [biom_id.subject for biom_id in ids],
            "array_impression": [biom_id.impression for biom_id in ids],
        }

    @staticmethod
    def ids_from_json(jsn: dict) -> Iterable["Identifier"]:
        subjects = jsn["array_subject"]
        samples = jsn["array_impression"]
        assert len(subjects) == len(samples)
        return [Identifier(subject=s, impression=i) for s, i in zip(subjects, samples)]


class IdentifierSet:
    def __init__(self, all_ids: list[Identifier]):
        IdentifierSet._check_duplicates(all_ids)
        self._ids: list[Identifier] = sorted(all_ids)
        self._num_subjects: int = len(set(i.subject for i in self._ids))

        print(
            f"Created IdentifierSet with {self._num_subjects} "
            f"subjects and a total of {len(self)} samples."
        )

    def __getitem__(self, index: int) -> Identifier:
        return self._ids[index]

    def __len__(self) -> int:
        return len(self._ids)

    def __le__(self, other: "IdentifierSet") -> bool:
        return set(self.identifiers) <= set(other.identifiers)

    def __eq__(self, other: "IdentifierSet") -> bool:
        return set(self.identifiers) == set(other.identifiers)

    @property
    def identifiers(self) -> list[Identifier]:
        return self._ids

    @property
    def num_subjects(self) -> int:
        return self._num_subjects

    def filter_by_index(self, indices: list[int]) -> "IdentifierSet":
        IdentifierSet._check_duplicates(indices)
        if not set(indices) <= set(i for i in range(len(self))):
            raise ValueError(
                "The indices must be a subset of the indices in the dataset!"
            )
        return IdentifierSet([self._ids[i] for i in indices])

    def filter_by_id(self, ids: "IdentifierSet") -> "IdentifierSet":
        """
        Checks that the ids are a subset of self
        """
        IdentifierSet._check_duplicates(ids)
        if not ids <= self:
            raise ValueError("The ids must be a subset of the ids in the dataset!")
        return ids

    def filter_by_subject(self, subjects: list[int]) -> "IdentifierSet":
        IdentifierSet._check_duplicates(subjects)
        return IdentifierSet._filter_ids_by_subject(self._ids, subjects)

    @staticmethod
    def _check_duplicates(elements: list[Hashable]) -> None:
        num_duplicate = len(elements) - len(set(elements))
        if num_duplicate > 0:
            raise RuntimeError(
                f"Elements must be unique but there are {num_duplicate} duplicate elements!"
            )

    @staticmethod
    def _filter_ids_by_subject(
        ids: list[Identifier], subjects: list[int]
    ) -> "IdentifierSet":
        # Filter
        ids = [f for f in ids if f.subject in set(subjects)]
        # Check for missing subjects
        num_missing_subjects: int = len(subjects) - len(set(f.subject for f in ids))
        if num_missing_subjects > 0:
            print(
                f"IdentifierSet.filter_by_subject(): Out of the {len(subjects)} subjects in  "
                f"{num_missing_subjects} were not found among the given identifiers."
            )
        return IdentifierSet(ids)


class DataLoader(ABC):
    @abstractmethod
    def get(self, identifier: Identifier) -> object:
        pass


class ConstantDataLoader(DataLoader):
    """
    For any identifier, returns the same value.
    """

    def __init__(self, value: object):
        self.value: object = value

    def get(self, identifier: Identifier) -> object:
        return self.value


class ZippedDataLoader(DataLoader):
    """
    For an identifier, the ZippedDataLoader will return a tuple of the results of the individual data loaders.

    The individual data loaders should support the same identifiers, or at least the subset of identifiers which is accessed.
    """

    def __init__(self, loaders: list[DataLoader]):
        assert len(loaders) > 0
        self._loaders: list[DataLoader] = loaders

    def get(self, identifier: Identifier) -> list[object]:
        return [ds.get(identifier) for ds in self._loaders]


class ConcatenatedDataLoader(DataLoader):
    """
    Concatenates multiple data loaders into one. The idea is to have on (external) set of Identifiers and a one-to-one mapping to
    the Identifiers of the individual data loaders.

    Lookup in the concatenated data loader works as follows:
        - We first determine which data loader to use
        - Then we determine the identifier for this specific data loader
        - Then we use the data loader to get the data

    As input we need the mappings:
        External identifier -> data loader
        External identifier -> internal identifier

    """

    def __init__(
        self,
        id_to_loader: dict[Identifier, DataLoader],
        id_to_id: dict[Identifier, Identifier],
    ):
        self._id_to_id: dict[Identifier, Identifier] = id_to_id
        self._id_to_loader: dict[Identifier, DataLoader] = id_to_loader

    def get(self, identifier: Identifier) -> object:
        return self._id_to_loader[identifier].get(self._id_to_id[identifier])


class Dataset:
    """
    A set of identifiers and a data loader that can be used to load the data for these identifiers.

    Allows efficient set operations on the datasets.
    """

    def __init__(self, data_loader: DataLoader, identifier_set: IdentifierSet):
        if not isinstance(identifier_set, IdentifierSet):
            raise ValueError(
                "The identifier_set must be an instance of IdentifierSet!"
            )
        if not isinstance(data_loader, DataLoader):
            raise ValueError("The data_loader must be an instance of DataLoader!")
        
        self.identifier_set = identifier_set
        self.data_loader = data_loader

        if hasattr(data_loader, "ids") and not identifier_set <= data_loader.ids:
            raise ValueError(
                "The identifiers in the dataset must be a subset of the identifiers in the data loader!"
            )

    def __len__(self) -> int:
        return len(self.identifier_set)

    @property
    def ids(self) -> IdentifierSet:
        return self.identifier_set

    @property
    def num_subjects(self) -> int:
        return self.identifier_set.num_subjects

    def __getitem__(self, index: int) -> object:
        identifier = self.identifier_set[index]
        return self.get(identifier)

    def __str__(self) -> str:
        return (
            f"{type(self).__name__} with {self.num_subjects} "
            f"subjects and a total of {len(self)} samples."
        )

    def get(self, identifier: Identifier) -> object:
        return self.data_loader.get(identifier)

    @staticmethod
    def concatenate(*datasets: list["Dataset"], share_subjects: bool = True) -> "Dataset":
        """
        If share_subjects is set to true it is assumed that the underlying subjects in all datasets are the same.
        e.g. subject 1 in the first dataset is exactly the same subject as subject 1 in the second dataset.

        Otherwise it is assumed that the datasets all refer to disjoint sets of subjects.
        e.g. subject 1 in the first dataset is a different subject than subject 1 in the second dataset.
        """
        assert len(datasets) > 0

        ids = []
        id_to_loader: dict[Identifier, int] = {}
        id_to_id: dict[Identifier, Identifier] = {}

        if share_subjects:
            impression_count = defaultdict(int)
            for ds in datasets:
                for id in ds.ids:
                    new_impression = impression_count[id.subject]
                    impression_count[id.subject] += 1
                    new_id = Identifier(id.subject, new_impression)
                    ids.append(new_id)
                    id_to_loader[new_id] = ds.data_loader
                    id_to_id[new_id] = id
        else:
            subject_count = 0
            subject_map = defaultdict(lambda: None)
            for ds_idx, ds in enumerate(datasets):
                for id in ds.ids:
                    new_subject = subject_map[(ds_idx, id.subject)]
                    if new_subject is None:
                        subject_map[(ds_idx, id.subject)] = subject_count
                        new_subject = subject_count
                        subject_count += 1
                    new_id = Identifier(new_subject, id.impression)
                    ids.append(new_id)
                    id_to_loader[new_id] = ds.data_loader
                    id_to_id[new_id] = id
        return Dataset(
            ConcatenatedDataLoader(id_to_loader, id_to_id), IdentifierSet(ids)
        )

    @staticmethod
    def zip(*datasets: list["Dataset"]) -> "Dataset":
        """
        Combines multiple datasets into one by zipping their data loader return values.
        """
        assert len(datasets) > 0
        for ds in datasets[1:]:
            assert datasets[0].ids == ds.ids
        return Dataset(
            ZippedDataLoader([ds.data_loader for ds in datasets]),
            datasets[0].ids,
        )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: embedding_loader.py
--------------------------------------------------------------------------------
from typing import Union
from os.path import join
import json

import numpy as np

from flx.data.dataset import DataLoader, Identifier, IdentifierSet


class EmbeddingLoader(DataLoader):
    def __init__(self, identifiers: IdentifierSet, embeddings: np.ndarray):
        assert embeddings.ndim == 2
        assert len(identifiers) == embeddings.shape[0]
        self._id_to_idx = {id: idx for idx, id in enumerate(identifiers)}
        self._array = embeddings

    @property
    def ids(self) -> IdentifierSet:
        return IdentifierSet(self._id_to_idx.keys())

    @property
    def embedding_size(self) -> int:
        return self._array.shape[1]

    def get(self, id: Identifier) -> np.ndarray:
        return self._array[self._id_to_idx[id]]

    def numpy(self) -> np.ndarray:
        return self._array

    def save(self, outdir: str) -> None:
        """
        Saves embeddings and corresponding ids to disk

        @param outdir : absolute path of the output directory
        """
        outarr = self.numpy()
        outarr = outarr.astype(np.float32)
        np.save(join(outdir, "embeddings.npy"), outarr)
        with open(join(outdir, "ids.json"), "w") as file:
            json.dump(
                Identifier.ids_to_json(self._id_to_embedding.keys()), file, indent=None
            )

    @staticmethod
    def load(dir: str) -> "EmbeddingLoader":
        """
        Loads embedding dataset from disk

        @dir : absolute path of the embeddings dir

        @returns : Loaded embeddings dataset
        """
        with open(join(dir, "ids.json"), "r") as file:
            ids = Identifier.ids_from_json(json.load(file))
        array = np.load(join(dir, "embeddings.npy"))
        return EmbeddingLoader(ids, array)

    @staticmethod
    def combine(ds1: "EmbeddingLoader", ds2: "EmbeddingLoader") -> "EmbeddingLoader":
        if set(ds1.ids) != set(ds2.ids):
            raise ValueError(
                "To concatenate two EmbeddingLoaders they must contain the same ids!"
            )
        if ds1.embedding_size != ds2.embedding_size:
            raise ValueError(
                "To concatenate two EmbeddingLoaders they must have the same embedding size!"
            )
        return EmbeddingLoader(
            ds1.ids, np.concatenate([ds1.numpy(), ds2.numpy()], axis=1)
        )

    @staticmethod
    def combine_if_both_exist(
        ds1: "EmbeddingLoader", ds2: "EmbeddingLoader"
    ) -> "EmbeddingLoader":
        if ds1 is None:
            assert ds2 is not None
            return ds2
        if ds2 is None:
            assert ds1 is not None
            return ds1
        return EmbeddingLoader.combine(ds1, ds2)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: file_index.py
--------------------------------------------------------------------------------
from typing import Generator, Callable
import os

from flx.data.dataset import Identifier, IdentifierSet, DataLoader


def _get_subdirs_with_files(
    root_dir: str, extension: str
) -> Generator[tuple[str, list[str]], None, None]:
    """
    Yiels tuple of relative path from root to subdirectory and filenames in subdirectory
    for every subdirectory that contains files.
    """
    extension = extension.lower()
    for dir, _, files in os.walk(root_dir):
        files = [os.path.splitext(f) for f in files if os.path.splitext]
        files = [f for f, ext in files if ext.lower() == extension]
        if len(files) == 0:
            continue
        relative_path_from_root = os.path.relpath(dir, root_dir)
        yield (relative_path_from_root, files)


class FileIndex(DataLoader):
    def __init__(
        self,
        root_dir: str,
        file_extension: str,
        id_from_path: Callable[[str, str], Identifier],
    ):
        """
        A DataLoader for filepaths.

        Discovers all files with the given extension in `root_dir` and its subdirectories and
        handles mapping between relative paths and ids. For each path relative to `root_dir` there
        must be a unique id and vice versa.

        file_extension: e.g. ".png"
        file_to_id_fun: function that maps a relative path to an id.
            E.g. if the file path is "~/dataset_dir/person005/finger08/file000.png"
            and the root_dir is "~/dataset_dir", then the function will receive
            ("person005/finger08", "file000") as arguments.
            The id could be Identifier(subject=6*100 + 7, finger=7, sample=0)

        Folder structure:
        root_dir/
            subdir/
                ...
                    subdir/
                        file.extension
                        file.extension
                        ...
                    subdir/
                        ...
        """
        self._root_dir: str = os.path.normpath(os.path.abspath(root_dir))

        self._file_extension: str = file_extension
        if not self._file_extension.startswith("."):
            self._file_extension = "." + self._file_extension

        # Tuple is (subdir, filename) where subdir is relative to root_dir
        self._id_to_path_components: dict[Identifier, tuple[str, str]] = {}

        for subdir, files in _get_subdirs_with_files(
            self._root_dir, self._file_extension
        ):
            for f in files:
                identifier = id_from_path(subdir, f)
                self._id_to_path_components[identifier] = (subdir, f)

        if len(self._id_to_path_components) == 0:
            print(
                f"FileDataset with root_dir '{self._root_dir}' is empty. No files with extension {self._file_extension} found."
            )
        self._ids = IdentifierSet(self._id_to_path_components.keys())

    @property
    def ids(self) -> IdentifierSet:
        return self._ids

    def get(self, identifier: Identifier) -> str:
        subdir, file = self._id_to_path_components[identifier]
        relpath = file if subdir == "." else os.path.join(subdir, file)
        path = os.path.join(self._root_dir, relpath + self._file_extension)
        if not os.path.exists(path):
            raise FileNotFoundError(f"File in index not found: {path}")
        return path
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: image_helpers.py
--------------------------------------------------------------------------------
from typing import Union

import numpy as np
import torch
import torchvision.transforms.functional as VTF

from flx.setup.config import INPUT_SIZE


def get_input_resolution() -> tuple[int, int]:
    return (INPUT_SIZE, INPUT_SIZE)


def pad_and_resize(
    img: Union[np.ndarray, torch.Tensor],
    target_size: tuple[int, int] = None,
    fill: float = 0.0,
) -> torch.Tensor:
    if not isinstance(img, torch.Tensor):
        img = VTF.to_tensor(img)

    height = img.shape[1]
    width = img.shape[2]
    pad_width = 0 if width >= height else int((height - width) / 2)
    pad_height = 0 if height >= width else int((width - height) / 2)
    img = VTF.pad(
        img, padding=(pad_width, pad_height, pad_width, pad_height), fill=fill
    )  # left, top, right, bottom

    assert img.shape[1] == img.shape[2]

    return VTF.resize(img, target_size, antialias=True)


def pad_and_resize_to_deepprint_input_size(
    img: Union[np.ndarray, torch.Tensor],
    roi: Union[None, tuple[int, int]] = None,
    fill: float = 0.0,
) -> torch.Tensor:
    if not isinstance(img, torch.Tensor):
        img = VTF.to_tensor(img)

    if roi is not None:
        img = VTF.center_crop(img, roi)

    return pad_and_resize(img, (INPUT_SIZE, INPUT_SIZE), fill=fill)


def transform_to_input_size(
    minutia_points: np.ndarray,
    original_height: int,
    original_width: int,
    roi: Union[None, tuple[int, int]] = None,
) -> np.ndarray:
    """
    Transforms the pixel coordinates in the same way that the pixels in the original image would be
    transformed by pad_and_resize_to_deepprint_input_size.
    """
    minutia_points = minutia_points.astype(np.float16)

    if minutia_points.shape[0] == 0:
        return minutia_points

    if roi is not None:
        minutia_points -= np.array(
            [(original_width - roi[1]) / 2, (original_height - roi[0]) / 2]
        )
        height = roi[0]
        width = roi[1]
    else:
        height = original_height
        width = original_width

    pad_width = 0 if width >= height else (height - width) / 2
    pad_height = 0 if height >= width else (width - height) / 2

    minutia_points += np.array([pad_width, pad_height])
    minutia_points *= INPUT_SIZE / max(height, width)
    return minutia_points
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: image_loader.py
--------------------------------------------------------------------------------
from abc import abstractstaticmethod

# import wsq  # needed for loading nist sd 14 dataset
import torch
import torchvision.transforms.functional as VTF
import cv2
import numpy as np
from flx.data.image_helpers import (
    pad_and_resize_to_deepprint_input_size,
)
from flx.data.dataset import Identifier, IdentifierSet, DataLoader
from flx.data.file_index import FileIndex


class ImageLoader(DataLoader):
    def __init__(self, root_dir: str):
        self._files: FileIndex = FileIndex(
            root_dir, self._extension(), self._file_to_id_fun
        )

    @property
    def ids(self) -> IdentifierSet:
        return self._files.ids

    def get(self, identifier: Identifier) -> torch.Tensor:
        return self._load_image(self._files.get(identifier))

    @abstractstaticmethod
    def _extension() -> str:
        pass

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier:
        subject_id, impression_id = filename.split("_")
        return Identifier(int(subject_id) - 1, int(impression_id) - 1)

    @abstractstaticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        # 修复：传入 roi=(299, 299)，强制进行中心裁剪，而不是缩放
        # 这样保留了原始的纹理尺度
        return pad_and_resize_to_deepprint_input_size(img, roi=(299, 299), fill=1.0)


class SFingeLoader(ImageLoader):
    @staticmethod
    def _extension() -> str:
        return ".png"

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier:
        # Pattern: <dir>/<subject_id>_<impression_id>.png
        subject_id, impression_id = filename.split("_")
        # We must start indexing at 0 instead of 1 to be compatible with pytorch
        return Identifier(int(subject_id) - 1, int(impression_id) - 1)

    @staticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        img = cv2.imread(filepath, flags=cv2.IMREAD_GRAYSCALE)
        return VTF.to_tensor(img[:-32])


class FVC2004Loader(ImageLoader):
    @staticmethod
    def _extension() -> str:
        return ".tif"

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier: 
        subject_id, impression_id = filename.split("_")
        return Identifier(int(subject_id) - 1, int(impression_id) - 1)

    @staticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        # 1. 读取
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        
        # 2. 增强 (CLAHE)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8)) # 稍微加强一点 clip
        img = clahe.apply(img)

        # 3. 找重心和角度 (Auto-Rotation)
        blur = cv2.GaussianBlur(img, (5, 5), 0)
        _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        coords = np.argwhere(th > 0)

        if len(coords) > 0:
            # --- 进化：旋转校正 ---
            # 注意：OpenCV 的坐标是 (x, y)，numpy 是 (row, col)
            #我们需要把 (row, col) 转成 (x, y) 传给 minAreaRect
            points = np.fliplr(coords) # (y, x) -> (x, y)
            
            # 计算最小外接矩形 -> 得到角度
            rect = cv2.minAreaRect(points)
            center, size, angle = rect
            
            # FVC 数据大部分是竖着的，如果角度偏差太大(比如90度)，可能是算法算错了(把宽当长了)
            # 我们假设指纹主要方向是竖直的，进行微调
            if size[0] < size[1]: 
                angle = angle - 90
            
            # 限制旋转角度：防止转反了 (180度) 或者转太猛
            # 我们只纠正 +/- 30 度以内的倾斜
            if abs(angle) > 30:
                angle = 0 # 放弃旋转，太危险
            
            if abs(angle) > 1: # 只有角度明显才转
                h, w = img.shape
                M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
                img = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderValue=255)
                
                # 旋转后需要重新计算重心（因为图变了）
                blur = cv2.GaussianBlur(img, (5, 5), 0)
                _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
                coords = np.argwhere(th > 0)
                if len(coords) > 0:
                    center_y = int(np.mean(coords[:, 0]))
                    center_x = int(np.mean(coords[:, 1]))
                else:
                    center_y, center_x = h//2, w//2
            else:
                center_y = int(np.mean(coords[:, 0]))
                center_x = int(np.mean(coords[:, 1]))
                
            # --- 裁剪 ---
            # 稍微缩小一点 crop_size 到 380，让指纹在 299x299 里占比更大 (Zoom In)
            crop_size = 380 
            half = crop_size // 2
            
            h, w = img.shape
            start_y = max(0, min(center_y - half, h - crop_size))
            start_x = max(0, min(center_x - half, w - crop_size))
            
            start_y = max(0, start_y)
            start_x = max(0, start_x)
            
            img = img[start_y:start_y+crop_size, start_x:start_x+crop_size]

        return pad_and_resize_to_deepprint_input_size(img, fill=1.0)


class MCYTOpticalLoader(ImageLoader):
    @staticmethod
    def _extension() -> str:
        return ".bmp"

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier:
        # Pattern: <dir>/<person>_<finger>_<impression>.png
        _, person, finger, impression = filename.split("_")
        # 12 impressions per finger
        subject = (10 * int(person)) + int(finger)
        return Identifier(subject, int(impression))

    @staticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        width = img.shape[1]
        return pad_and_resize_to_deepprint_input_size(img, roi=(310, width), fill=1.0)


class MCYTCapacitiveLoader(ImageLoader):
    @staticmethod
    def _extension() -> str:
        return ".bmp"

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier:
        # Pattern: <dir>/<person:04d>_<finger>_<impression>.png
        _, person, finger, impression = filename.split("_")
        # 12 impressions per finger
        subject = (10 * int(person)) + int(finger)
        return Identifier(subject, int(impression))

    @staticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        return pad_and_resize_to_deepprint_input_size(img, fill=1.0)


class NistSD4Dataset(ImageLoader):
    @staticmethod
    def _extension() -> str:
        return ".png"

    @staticmethod
    def _file_to_id_fun(_: str, filename: str) -> Identifier:
        # Pattern: <dir>/[f|s]<subject:04d>_<finger:02d>.png
        sample = 0 if filename[0] == "f" else 1
        subject, _ = filename[1:].split("_")
        # We must start indexing at 0 instead of 1 to be compatible with pytorch
        return Identifier(
            subject=int(subject) - 1,
            impression=sample,
        )

    @staticmethod
    def _load_image(filepath: str) -> torch.Tensor:
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        return pad_and_resize_to_deepprint_input_size(img, fill=1.0)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data/iso_encoder_decoder
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data/iso_encoder_decoder
文件名: decode.py
--------------------------------------------------------------------------------
from collections import namedtuple

# This code is to decode the iso19794-2-2005 and iso-19794-2-2011 templates of fingerprint minutiae.
# VeriFinger SDK could recognize both of these two formats.

Minutiae = namedtuple(
    "Minutiae",
    ["type", "x", "y", "orientation", "quality", "label"],
    defaults=[None, None],
)


def load_iso19794(path, format):
    if format == "19794-2-2005":
        with open(path, "rb") as f:
            t = f.read()
        magic = int.from_bytes(t[0:4], "big")
        version = int.from_bytes(t[4:8], "big")
        total_bytes = int.from_bytes(t[8:12], "big")
        im_w = int.from_bytes(t[14:16], "big")
        im_h = int.from_bytes(t[16:18], "big")
        resolution_x = int.from_bytes(t[18:20], "big")
        resolution_y = int.from_bytes(t[20:22], "big")
        f_count = int.from_bytes(t[22:23], "big")
        reserved_byte = int.from_bytes(t[23:24], "big")
        fingerprint_quality = int.from_bytes(t[26:27], "big")
        minutiae_num = int.from_bytes(t[27:28], "big")
        minutiaes = []
        for i in range(minutiae_num):
            x = 28 + 6 * i
            min_type = (t[x] >> 6) & 0x3
            min_x = int.from_bytes([t[x] & 0x3F, t[x + 1]], "big")
            min_y = int.from_bytes(t[x + 2 : x + 4], "big")
            angle = 360 - t[x + 4] / 256 * 360
            min_quality = t[x + 5]
            minutiaes.append(Minutiae(min_type, min_x, min_y, angle, min_quality))
        return minutiaes

    if format == "19794-2-2011":
        with open(path, "rb") as f:
            t = f.read()
        magic = int.from_bytes(t[0:4], "big")
        version = int.from_bytes(t[4:8], "big")
        total_bytes = int.from_bytes(t[8:12], "big")
        fp_count = int.from_bytes(t[12:14], "big")
        HASCERTS = int.from_bytes(t[14:15], "big")
        fp_bytes = int.from_bytes(t[15:19], "big")
        year = int.from_bytes(t[19:21], "big")
        month = int.from_bytes(t[21:22], "big")
        day = int.from_bytes(t[22:23], "big")
        hour = int.from_bytes(t[23:24], "big")
        minute = int.from_bytes(t[24:25], "big")
        second = int.from_bytes(t[25:26], "big")
        millisecond = int.from_bytes(t[26:28], "big")
        dev_tech = int.from_bytes(t[28:29], "big")
        dev_vendor = int.from_bytes(t[29:31], "big")
        dev_id = int.from_bytes(t[31:33], "big")
        QCOUNT = int.from_bytes(t[33:34], "big")
        FP_quality = int.from_bytes(t[34:35], "big")
        Q_vendor = int.from_bytes(t[35:37], "big")
        Q_algo = int.from_bytes(t[37:39], "big")
        position = int.from_bytes(t[39:40], "big")
        view_offset = int.from_bytes(t[40:41], "big")
        resolution_x = int.from_bytes(t[41:43], "big")
        resolution_y = int.from_bytes(t[43:45], "big")
        sample_type = int.from_bytes(t[45:46], "big")
        im_w = int.from_bytes(t[46:48], "big")
        im_h = int.from_bytes(t[48:50], "big")
        MINBYTES, ENDINGTYPE = (t[50] >> 4) & 0xF, t[50] & 0xF
        minutiae_num = int.from_bytes(t[51:52], "big")
        minutiaes = []
        for i in range(minutiae_num):
            x = 52 + 6 * i
            min_type = (t[x] >> 6) & 0x3
            min_x = int.from_bytes([t[x] & 0x3F, t[x + 1]], "big")
            min_y = int.from_bytes(t[x + 2 : x + 4], "big")
            angle = 360 - t[x + 4] / 256 * 360
            min_quality = t[x + 5]
            minutiaes.append(Minutiae(min_type, min_x, min_y, angle, min_quality))
        return minutiaes


def main():
    minutiae_2005 = load_iso19794("iso2005template", format="19794-2-2005")
    print(minutiae_2005)

    minutiae_2011 = load_iso19794("iso2011template", format="19794-2-2011")
    print(minutiae_2011)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data/iso_encoder_decoder
文件名: encode.py
--------------------------------------------------------------------------------
from math import *

# convert minutiae template from txt format to iso19794-2-2005 format.


def to_iso19794(txtpath, isopath):
    width, height = 500, 610
    b_array = bytearray()
    with open(txtpath, "r") as textfile:
        lines = textfile.readlines()
    minutiae_num = len(lines)
    totalbytes = minutiae_num * 6 + 28 + 2

    b_array = bytearray(b"FMR\x00 20\x00")
    b_array += totalbytes.to_bytes(4, "big")
    b_array += bytearray(b"\x00\x00")
    b_array += width.to_bytes(2, "big")
    b_array += height.to_bytes(2, "big")
    b_array += bytearray(b"\x00\xc5\x00\xc5\x01\x00\x00\x00d")
    b_array += minutiae_num.to_bytes(1, "big")
    byte_list = [0, 0, 0, 0, 0, 0]
    for line in lines:
        line = line.split(" ")
        x, y, angle, min_type, quality = (
            int(float(line[0])),
            int(float(line[1])),
            int(float(line[2])),
            int(float(line[3])),
            int(float(line[4].strip())),
        )
        byte_list[1] = x % 256
        byte_list[0] = x // 256 + min_type * 64
        byte_list[2] = y // 256
        byte_list[3] = y % 256
        byte_list[4] = round((360 - angle) / 360 * 256)
        byte_list[5] = quality
        b_array += bytearray(byte_list)

    zero = 0
    b_array += zero.to_bytes(2, "big")

    with open(isopath, "wb") as istfile:
        istfile.write(b_array)


def main():
    txtpath = "txtTemplate.txt"
    isopath = "isoTemplate"
    to_iso19794(txtpath, isopath)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: label_index.py
--------------------------------------------------------------------------------
from copy import deepcopy

from flx.data.dataset import Identifier, IdentifierSet, DataLoader


class LabelIndex(DataLoader):
    """
    Defines a mapping between subject ids and labels.
    Subjects ids are not continuous while labels are always in range(0, num_subjects)
    """

    def __init__(self, ids: IdentifierSet):
        assert isinstance(ids, IdentifierSet)
        self._ids: IdentifierSet = ids
        subject_set = set(f.subject for f in self._ids)
        self._subject_to_label: dict[int, int] = {
            subject: i for i, subject in enumerate(sorted(subject_set))
        }

    @property
    def ids(self) -> IdentifierSet:
        return self._ids

    def get(self, identifier: Identifier) -> int:
        return self._subject_to_label[identifier.subject]
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: minutia_map.py
--------------------------------------------------------------------------------
import numpy as np


from flx.visualization.show_with_opencv import show_minutia_maps


def _remove_points_outside_image(
    coords: np.ndarray, oris: np.ndarray, height: int, width: int
) -> np.ndarray:
    mask_x = coords[:, 0] >= width
    mask_y = coords[:, 1] >= height
    mask = np.logical_or(mask_x, mask_y)
    coords = np.delete(coords, np.where(mask), axis=0)
    oris = np.delete(oris, np.where(mask))
    return coords, oris


def _rescale_points(
    points: np.ndarray,
    image_resolution: tuple[int, int],
    target_resolution: tuple[int, int],
):
    scale_factor = min(
        target_resolution[0] / image_resolution[0],
        target_resolution[1] / image_resolution[1],
    )

    padding_x = (target_resolution[1] - (image_resolution[1] * scale_factor)) / 2
    padding_y = (target_resolution[0] - (image_resolution[0] * scale_factor)) / 2

    assert padding_x >= 0
    assert padding_y >= 0

    padding = np.array([padding_x, padding_y], dtype=np.uint16)

    return points * scale_factor + padding


def _gaussian_mask(sigma: float, radius: int):
    x, y = np.meshgrid(
        np.linspace(-radius, radius, 2 * radius + 1),
        np.linspace(-radius, radius, 2 * radius + 1),
    )
    dst = x**2 + y**2

    return np.exp(-(dst / (2.0 * sigma**2)))


def _convert_orientations(orientations: np.ndarray[float]) -> np.ndarray:
    """
    Makes sure that the orientations in the output array are in range
    [0, 2 * pi]
    """
    out = orientations.copy()
    two_pi_inv = 1 / (2 * np.pi)
    out *= two_pi_inv
    out = out - np.floor(out)
    out *= 2 * np.pi
    return out


def _layer_weights_softmax(orientations: np.ndarray[float], n_layers: int):
    """
    To calculate the layer weights for a given orientation we first calculate the orientation
    difference to each layer's orientation. The final weight is then obtained by applying
    the softmax function over all layers.

    @returns :
        np.ndarray of type float16 and shape (orientations.shape[0], n_layers)
    """
    layer_orientations = np.linspace(
        0, 2 * np.pi, num=n_layers, endpoint=False, dtype=np.float16
    )
    layer_orientations = np.tile(layer_orientations, (orientations.shape[0], 1))
    orientation_diffs = np.abs(layer_orientations - orientations[:, np.newaxis])

    # If greater than pi -> We calculated the larger of the two angles
    # between the orientations -> Use   2 pi - ori   instead
    mask = orientation_diffs > np.pi
    orientation_diffs[mask] *= -1
    orientation_diffs[mask] += 2 * np.pi

    weights = np.exp(-orientation_diffs)
    norm = 1 / np.sum(weights, axis=1)
    return weights * norm[:, np.newaxis]


def create_minutia_map(
    minutia_locations: np.ndarray,
    minutia_orientations: np.ndarray,
    in_resolution: tuple[int, int],
    out_resolution: tuple[int, int],
    n_layers: int,
    sigma: int,
) -> np.ndarray:
    """
    Creates a minutia map representation of the given minutia points.
    Follows the procedure described in

    End-to-End Latent Fingerprint Search
    Kai Cao, Dinh-Luan Nguyen, Cori Tymoszek, A.K. Jain
    https://arxiv.org/abs/1812.10213v1

    First the points are rescaled to match the output resolution.

    The output matrix is initialized with zeros. Then for each minutia point, the density
    values of a gaussian distribution with the center at the location and a standard deviation of
    'sigma' are added to the layers. For each layer, these densities are further weighted
    according to the minutia orientation.

    For performance, the radius around each minutia point where this is applied is limited
    to 2 * sigma.

    We then rescale the image, so that the density at each minutia point location
    corresponds to a pixel value of 255.

    @param minutia_locations :
        numpy int array of shape (num_minutiae, 2)
        Contains the x, y pixel position of the minutia point. Assumes OpenCV coordinates
        (i.e. x from left to right, y from top to bottom)
        Like [[x1, y1], [x2, y2], [x3, y4], ...]
    @param minutia_orientations :
        numpy float array of shape (num_minutiae)
        Contains the corresponding minutia orientations where e.g.
            > 0.0    is orientation in the direction of the x-Axis (to the right)
            > 1/2 pi is the direction of the y-Axis (upwards)
            > pi (or -pi) is the direction of the negative x-Axis (to the left)
            > 3/2 pi (or - 1/2 pi) is the direction of the negative y-Axis (downwards)
    @param image_resolution :
        resolution of the fingerprint image (width, height)
    @param out_resolution :
        target resolution of the minutia map
        INFO: The resulting minutia map includes padding of size 2 * sigma
        INFO: The shape of the output matrix will be (height, width) not (width, height)
    @param n_layers :
        number of layers; The k-th layer corresponds to the orientation  k * 2 * pi / n_layers
    @param sigma :
        Determines the size of a minutia point in the target map; Higher value means larger minutia points

    @returns :
        np.ndarray of type np.uint8 and shape (out_resolution[1], out_resolution[0], n_layers)
    """
    radius = int(np.ceil(2 * sigma))
    out_image = np.zeros(
        shape=(
            out_resolution[1] + 2 * radius,
            out_resolution[0] + 2 * radius,
            n_layers,
        ),
        dtype=np.float16,
    )

    if minutia_locations.shape[0] == 0:
        return out_image[radius:-radius, radius:-radius].astype(dtype=np.uint8)

    if in_resolution != out_resolution:
        minutia_locations = _rescale_points(
            minutia_locations,
            image_resolution=in_resolution,
            target_resolution=out_resolution,
        ).astype(np.uint16)

    minutia_locations, minutia_orientations = _remove_points_outside_image(
        minutia_locations, minutia_orientations, out_resolution[1], out_resolution[0]
    )

    minu_layer_weights = _layer_weights_softmax(
        _convert_orientations(minutia_orientations), n_layers=n_layers
    )

    mask = _gaussian_mask(sigma, radius)
    base_density = np.reshape(
        np.repeat(mask, n_layers), newshape=(mask.shape[0], mask.shape[0], n_layers)
    )

    for minu_idx, layer_weights in enumerate(minu_layer_weights[:]):
        minu_density = base_density * layer_weights[np.newaxis, np.newaxis, :]

        out_loc = minutia_locations[minu_idx] + radius
        x_start = out_loc[0] - radius
        x_end = out_loc[0] + radius + 1
        y_start = out_loc[1] - radius
        y_end = out_loc[1] + radius + 1
        out_image[y_start:y_end, x_start:x_end, :] += minu_density

    out_image = np.clip(out_image, 0, 1.0)
    out_image *= 255
    return out_image[radius:-radius, radius:-radius].astype(dtype=np.uint8)


def main():
    # Locations: left-top, right-bottom, left-middle, right-top
    locations = np.array([(100, 100), (300, 300), (100, 200), (350, 50)])
    orientations = np.array([0.0, np.pi, 3 / 2 * np.pi, -1.0])
    image_resolution = (400, 400)
    output_resolution = (128, 128)
    maps = create_minutia_map(
        locations,
        orientations,
        image_resolution,
        output_resolution,
        n_layers=4,
        sigma=1.5,
    )
    print(maps.shape)
    show_minutia_maps(maps)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: minutia_map_loader.py
--------------------------------------------------------------------------------
import os
from abc import abstractmethod

import numpy as np
import torch
import torchvision.transforms.functional as VTF

from flx.data.dataset import Identifier, DataLoader, IdentifierSet
from flx.data.minutia_map import create_minutia_map
from flx.data.file_index import FileIndex
from flx.data.iso_encoder_decoder import decode

from flx.data.image_helpers import (
    get_input_resolution,
    transform_to_input_size,
)

"""
The size of the original images created by SFinge.
The minutia locations in the .ist files refer to the original image,
even if the images are resampled to a smaller resolution during generation.
Therefore we need to transform the minutia locations w.r.t. the original
resolution and not w.r.t. the resolution of the actual image.
"""

MINUTIA_MAP_CHANNELS = 6
MINUTIA_MAP_SIZE = 128


class MinutiaMapLoader(DataLoader):
    @abstractmethod
    def get_minutiae(self, identifier: Identifier) -> tuple[np.ndarray, np.ndarray]:
        """
        @returns : Two numpy arrays of equal length. The first array is a 2-D array where each row is
            one coordinate (x, y) of a minutia. The second array is a 1-D array containing the orientation
            in radians (values in range [0.0, 2 * pi]).
        """
        pass

    def get(self, identifier: Identifier) -> tuple[torch.Tensor, float]:
        """
        @returns : Minutia map generated from the minutia points in the given file
        """
        minutiae_loc, minutiae_ori = self.get_minutiae(identifier)
        # Minutiae are often extracted from a different resolution than
        # that of the resized images that serve as input to DeepPrint
        minu_map = create_minutia_map(
            minutiae_loc,
            minutiae_ori,
            in_resolution=get_input_resolution(),
            out_resolution=(MINUTIA_MAP_SIZE, MINUTIA_MAP_SIZE),
            n_layers=MINUTIA_MAP_CHANNELS,
            sigma=1.5,
        )
        return (VTF.to_tensor(minu_map), 1.0)


class SFingeMinutiaMapLoader(MinutiaMapLoader):
    def __init__(
        self,
        root_dir: str,
    ):
        def file_to_id_fun(_: str, filename: str) -> Identifier:
            # Pattern: <dir>/<subject_id>_<impression_id>.png
            filename, _ = os.path.splitext(filename)  # Remove extension(s)
            subject_id, impression_id = filename.split("_")
            # We must start indexing at 0 instead of 1 to be compatible with pytorch
            return Identifier(int(subject_id) - 1, int(impression_id) - 1)

        self._files = FileIndex(root_dir, ".ist", file_to_id_fun)

    @property
    def ids(self) -> IdentifierSet:
        return self._files.ids

    def get_minutiae(self, identifier: Identifier) -> tuple[np.ndarray, np.ndarray]:
        """
        @returns : Two numpy arrays of equal length. The first array is a 2-D array where each row is
            one coordinate (x, y) of a minutia. The second array is a 1-D array containing the orientation
            in radians (values in range [0.0, 2 * pi]).
        """
        minutiae = decode.load_iso19794(
            self._files.get(identifier), format="19794-2-2005"
        )[1:]
        locs = np.array([(m[1], m[2]) for m in minutiae])
        oris = np.array([m[3] for m in minutiae])
        return transform_to_input_size(locs, 560, 416), oris


def _read_mnt_file(filepath: str) -> tuple[np.ndarray, np.ndarray]:
    with open(filepath, "r") as file:
        lines = file.readlines()
        locs = []
        oris = []
        for line in lines[2:]:
            x, y, ori = line.split(" ")
            locs.append((float(x), float(y)))
            oris.append(float(ori))
    return np.array(locs, dtype=np.float16), np.array(oris, dtype=np.float16)


class MCYTOpticalMinutiaMapLoader(MinutiaMapLoader):
    def __init__(
        self,
        root_dir: str,
    ):
        def file_to_id_fun(_: str, filename: str) -> Identifier:
            # Pattern: <dir>/<person:04>_<finger>_<impression>.mnt
            _, person, finger, impression = filename.split("_")
            # 12 impressions per finger
            subject = (10 * int(person)) + int(finger)
            return Identifier(subject, int(impression), finger=int(finger))

        self._files: FileIndex = FileIndex(root_dir, ".mnt", file_to_id_fun)

    @property
    def ids(self) -> IdentifierSet:
        return self._files.ids

    def get_minutiae(self, identifier: Identifier) -> tuple[np.ndarray, np.ndarray]:
        """
        @returns : Two numpy arrays of equal length. The first array is a 2-D array where each row is
            one coordinate (x, y) of a minutia. The second array is a 1-D array containing the orientation
            in radians (values in range [0.0, 2 * pi]).
        """
        locs, oris = _read_mnt_file(self._files.get(identifier))
        return (
            transform_to_input_size(
                locs, original_height=400, original_width=256, roi=(310, 256)
            ),
            oris,
        )


class MCYTCapacitiveMinutiaMapLoader(MinutiaMapLoader):
    def __init__(
        self,
        root_dir: str,
    ):
        def file_to_id_fun(_: str, filename: str) -> Identifier:
            # Pattern: <dir>/<person:04>_<finger>_<impression>.bmp.mnt
            filename = filename[: len(filename) - len(".bmp")]
            _, person, finger, impression = filename.split("_")
            # 12 impressions per finger
            subject = (10 * int(person)) + int(finger)
            return Identifier(subject, int(impression), finger=int(finger))

        self._files: FileIndex = FileIndex(root_dir, ".mnt", file_to_id_fun)

    @property
    def ids(self) -> IdentifierSet:
        return self._files.ids

    def get_minutiae(self, identifier: Identifier) -> tuple[np.ndarray, np.ndarray]:
        """
        @returns : Two numpy arrays of equal length. The first array is a 2-D array where each row is
            one coordinate (x, y) of a minutia. The second array is a 1-D array containing the orientation
            in radians (values in range [0.0, 2 * pi]).
        """
        locs, oris = _read_mnt_file(self._files.get(identifier))
        return transform_to_input_size(locs, 300, 300), oris
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: pose_dataset.py
--------------------------------------------------------------------------------
import json


from flx.data.dataset import Identifier, IdentifierSet, DataLoader

from flx.image_processing.augmentation import PoseTransform


def _poses_to_json(poses: list[PoseTransform]) -> dict:
    return {
        "array_pad": [p.pad for p in poses],
        "array_angle": [p.angle for p in poses],
        "array_shift_horizontal": [p.shift_horizontal for p in poses],
        "array_shift_vertical": [p.shift_vertical for p in poses],
    }


def _poses_from_json(json: dict) -> list[PoseTransform]:
    paddings = json["array_pad"]
    angles = json["array_angle"]
    shifts_horizontal = json["array_shift_horizontal"]
    shifts_vertical = json["array_shift_vertical"]
    return [
        PoseTransform(p, r, sh, sv)
        for p, r, sh, sv in zip(paddings, angles, shifts_horizontal, shifts_vertical)
    ]


class PoseLoader(DataLoader):
    """
    Data loader for fingerprint poses to given
    fingerprint samples.

    Each pose consists of a rotation and a shift in x and y direction.
    The dataset can be save to or loaded from json.
    """

    def __init__(self, ids: IdentifierSet, poses: list[PoseTransform]):
        self._ids = ids
        self._id_to_pose = {bid: pose for bid, pose in zip(ids, poses)}

    @property
    def ids(self) -> IdentifierSet:
        return self._ids

    def get(self, identifier: Identifier) -> PoseTransform:
        return self._id_to_pose[identifier]

    def save(self, path: str) -> None:
        with open(path, "w") as file:
            obj = {
                "poses": _poses_to_json(self._id_to_pose.values()),
                "ids": Identifier.ids_to_json(self._id_to_pose.keys()),
            }
            json.dump(obj, file)

    @staticmethod
    def load(path: str) -> "PoseLoader":
        with open(path, "r") as file:
            obj = json.load(file)
            ids = Identifier.ids_from_json(obj["ids"])
            poses = _poses_from_json(obj["poses"])
            return PoseLoader(ids, poses)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/data
文件名: transformed_image_loader.py
--------------------------------------------------------------------------------
from typing import Union, Callable

import torch

from flx.data.dataset import Identifier, IdentifierSet, DataLoader
from flx.data.image_loader import ImageLoader
from flx.data.pose_dataset import PoseLoader
from flx.image_processing.augmentation import (
    RandomPoseTransform,
)


class TransformedImageLoader(DataLoader):
    def __init__(
        self,
        images: ImageLoader,
        poses: Union[None, PoseLoader, RandomPoseTransform] = None,
        transforms: list[Callable[[torch.Tensor], torch.Tensor]] = [],
    ):
        """
        A lightweight wrapper for applying image augmentation to an existing fingerprint dataset.

        First, if 'augment_image_pose' is True, a pose transform can be applied, it consists of:
            - Padding
            - Rotation
            - Shift in x and y direction
        If 'poses' is specified the rotation and shift are taken from this FingerprintPoseDataset,
        otherwise they are randomly generated.

        Then, if 'augment_image_quality' is True, the following properties are randomly transformed:
            - Gain (aka brightness)
            - Contrast
            - Gamma

        """
        self._images: ImageLoader = images
        self._pose_distribution: RandomPoseTransform = None
        self._pose_dataset: PoseLoader = None
        self._transforms: list[Callable[[torch.Tensor], torch.Tensor]] = transforms

        if not (
            poses is None
            or isinstance(poses, PoseLoader)
            or isinstance(poses, RandomPoseTransform)
        ):
            raise TypeError(
                "Parameter 'poses' must be None, a PoseDataset or a PoseDistribution"
            )

        if isinstance(poses, PoseLoader):
            if not set(images.ids).issubset(set(poses.ids)):
                raise RuntimeError("Received an incomplete pose dataset")
            self._pose_dataset = poses

        if isinstance(poses, RandomPoseTransform):
            self._pose_distribution = poses

    @property
    def ids(self) -> IdentifierSet:
        return self._images.ids

    def get(self, identifier: Identifier) -> torch.Tensor:
        """
        Get a fingerprint sample from the underlying dataset with the described augmentations applied.

        The transforms are only applied to the fingerprint image, not the minutia map.

        @returns tuple of fingerprint image, minutia map, label, subject, impression
        """
        img = self._images.get(identifier)

        if self._pose_dataset is not None:
            img = self._pose_dataset.get(identifier)(img)
        elif self._pose_distribution is not None:
            img = self._pose_distribution.sample()(img)

        for transform in self._transforms:
            img = transform(img)

        return img
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/extractor
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/extractor
文件名: extract_embeddings.py
--------------------------------------------------------------------------------
from typing import Union

import numpy as np
import torch
import tqdm

from flx.models.torch_helpers import get_dataloader_args, get_device
from flx.data.embedding_loader import EmbeddingLoader
from flx.data.dataset import IdentifierSet
from flx.data.dataset import Dataset
from flx.models.deep_print_arch import DeepPrintOutput


def _to_numpy(embeddings: torch.Tensor):
    if embeddings is None:
        return None
    return embeddings.detach().to(device=torch.device("cpu")).numpy()


def _concatenate_embeddings_if_exist(
    ids: IdentifierSet, embeddings: list[Union[None, np.ndarray]]
) -> torch.Tensor:
    embeddings_filtered = [e for e in embeddings if e is not None]
    if len(embeddings_filtered) == 0:
        return None
    if len(embeddings_filtered) != len(embeddings):
        raise ValueError("Some embeddings are None, others are not!")
    return EmbeddingLoader(ids, np.concatenate(embeddings, axis=0))


def extract_embeddings(
    model: torch.nn.Module, fingerprint_dataset: Dataset
) -> tuple[EmbeddingLoader, EmbeddingLoader]:
    texture_embeddings = []
    minutia_embeddings = []

    model = model.to(get_device())
    dataloader = torch.utils.data.DataLoader(
        fingerprint_dataset, **get_dataloader_args(train=False)
    )
    model.eval()  # No longer outputs logits and minutia map in eval mode
    with torch.no_grad():
        for vals in tqdm.tqdm(dataloader):
            fp_imgs = vals
            fp_imgs: torch.Tensor = fp_imgs.to(get_device())
            output: DeepPrintOutput = model(fp_imgs)

            texture_embeddings.append(_to_numpy(output.texture_embeddings))
            minutia_embeddings.append(_to_numpy(output.minutia_embeddings))

    return (
        _concatenate_embeddings_if_exist(fingerprint_dataset.ids, texture_embeddings),
        _concatenate_embeddings_if_exist(fingerprint_dataset.ids, minutia_embeddings),
    )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/extractor
文件名: fixed_length_extractor.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
from os.path import exists

import torch

from flx.benchmarks.verification import VerificationBenchmark
from flx.data.dataset import IdentifierSet
from flx.data.embedding_loader import EmbeddingLoader
from flx.data.image_loader import ImageLoader
from flx.extractor.extract_embeddings import extract_embeddings
from flx.models.model_training import train_model
from flx.models.deep_print_arch import (
    DeepPrint_Tex,
    DeepPrint_LocTexMinu,
    DeepPrint_TexMinu,
    DeepPrint_LocTex,
    DeepPrint_LocMinu,
    DeepPrint_Minu,
)
from flx.models.deep_print_loss import (
    DeepPrintLoss_Tex,
    DeepPrintLoss_Minu,
    DeepPrintLoss_TexMinu,
)
from flx.setup.paths import get_best_model_file
from flx.data.dataset import Dataset
from flx.models.torch_helpers import load_model_parameters
from flx.data.dataset import ConstantDataLoader


@dataclass
class DeepPrintExtractor:
    training_with_minutia_map: bool
    model: torch.nn.Module
    loss: torch.nn.Module

    def fit(
        self,
        fingerprints: Dataset,
        minutia_maps: Dataset,
        labels: Dataset,
        validation_fingerprints: Dataset,
        validation_benchmark: VerificationBenchmark,
        num_epochs: int,
        out_dir: str,
    ) -> None:
        if not self.training_with_minutia_map:
            minutia_maps = Dataset(ConstantDataLoader((torch.tensor([]), 0.0)), fingerprints.ids)
        train_model(
            model=self.model,
            loss=self.loss,
            fingerprints=fingerprints,
            minutia_maps=minutia_maps,
            labels=labels,
            validation_fingerprints=validation_fingerprints,
            validation_benchmark=validation_benchmark,
            num_epochs=num_epochs,
            out_dir=out_dir,
        )

    def extract(self, dataset: Dataset) -> tuple[EmbeddingLoader, EmbeddingLoader]:
        return extract_embeddings(self.model, dataset)

    def load_best_model(self, model_dir: str) -> None:
        model_path = get_best_model_file(model_dir)
        if exists(model_path):
            print(f"Loaded best model from {model_path}")
            load_model_parameters(model_path, self.model, None, None)
        else:
            print(f"No best model file found at {model_path}")


def get_DeepPrint_Tex(
    num_training_subjects: int, num_texture_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_Tex(num_training_subjects, num_texture_dims)
    loss = DeepPrintLoss_Tex(num_training_subjects, num_texture_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=False,
        model=model,
        loss=loss,
    )


def get_DeepPrint_Minu(
    num_training_subjects: int, num_minutia_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_Minu(num_training_subjects, num_minutia_dims)
    loss = DeepPrintLoss_Minu(num_training_subjects, num_minutia_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=True,
        model=model,
        loss=loss,
    )


def get_DeepPrint_LocTex(
    num_training_subjects: int, num_texture_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_LocTex(num_training_subjects, num_texture_dims)
    loss = DeepPrintLoss_Tex(num_training_subjects, num_texture_dims)
    optimizer = torch.optim.Adam(params=model.parameters())
    return DeepPrintExtractor(
        training_with_minutia_map=False,
        model=model,
        loss=loss,
    )


def get_DeepPrint_TexMinu(
    num_training_subjects: int, num_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_TexMinu(num_training_subjects, num_dims, num_dims)
    loss = DeepPrintLoss_TexMinu(num_training_subjects, num_dims, num_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=True,
        model=model,
        loss=loss,
    )


def get_DeepPrint_LocTex(
    num_training_subjects: int, num_texture_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_LocTex(num_training_subjects, num_texture_dims)
    loss = DeepPrintLoss_Tex(num_training_subjects, num_texture_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=False,
        model=model,
        loss=loss,
    )


def get_DeepPrint_LocMinu(
    num_training_subjects: int, num_texture_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_LocMinu(num_training_subjects, num_texture_dims)
    loss = DeepPrintLoss_Minu(num_training_subjects, num_texture_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=True,
        model=model,
        loss=loss,
    )


def get_DeepPrint_LocTexMinu(
    num_training_subjects: int, num_dims: int
) -> DeepPrintExtractor:
    model = DeepPrint_LocTexMinu(num_training_subjects, num_dims, num_dims)
    loss = DeepPrintLoss_TexMinu(num_training_subjects, num_dims, num_dims)
    return DeepPrintExtractor(
        training_with_minutia_map=True,
        model=model,
        loss=loss,
    )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/image_processing
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/image_processing
文件名: augmentation.py
--------------------------------------------------------------------------------
import os
from dataclasses import dataclass
import random
import json

import numpy as np
import cv2
import torch
import torchvision
import torchvision.transforms.functional as VTF


@dataclass
class PoseTransform:
    pad: int = 0  # Pad the image with this amount of white pixels in every direction.
    angle: float = 0  # Rotation in degrees, range [-180, 180]
    shift_horizontal: int = (
        0  # Shift the image this number of pixels to the right; negative value for left
    )
    shift_vertical: int = (
        0  # Shift the image this number of pixels downwards; negative value for upwards
    )

    @torch.no_grad()
    def __call__(self, img: torch.Tensor) -> torch.Tensor:
        if self.pad != 0:
            img = VTF.pad(img, padding=self.pad, fill=1.0)
        if not (
            self.angle == 0 and self.shift_horizontal == 0 and self.shift_vertical == 0
        ):
            img = VTF.affine(
                img,
                angle=self.angle,
                translate=(self.shift_horizontal, self.shift_vertical),
                scale=1,
                shear=0,
                fill=1.0,
            )
        return img

    def transform_coordinates(self, coordinates: np.ndarray) -> np.ndarray:
        """
        Transfoms coordinates that belong to points in the original image to
        the coordinates of the points in the transformed image.

        @param coordinates : np.ndarray of the coordinates ((x_0, y_0), (x_1, y_1), ...)

        @returns np.ndarray of the transformed coordinates
        """

        # use homogenout coordinates
        homg_coords = np.append(
            coordinates, np.ones(shape=(coordinates.shape[0])), axis=1
        )
        affine_mat = np.array(
            [
                [
                    np.cos(self.angle),
                    -np.sin(self.angle),
                    self.pad + self.shift_horizontal,
                ],
                [
                    np.sin(self.angle),
                    np.cos(self.angle),
                    self.pad + self.shift_vertical,
                ],
                [0, 0, 1],
            ],
            dtype=float,
        )
        transformed_homg = np.matmul(affine_mat, homg_coords)
        transformed_homg = transformed_homg / transformed_homg[:, 2]
        return transformed_homg[:, :2]

    def __str__(self) -> str:
        return f"PoseTransform(angle={self.angle}, shift_horizontal={self.shift_horizontal}, shift_vertical={self.shift_vertical}, pad={self.pad})"


@dataclass
class RandomPoseTransform:
    """
    Resembles a distribution of pose transforms, where angle, vertical and horizontal shift are
    drawn from uniform random distributions. Pad is treated as a constant.
    """

    pad: int = 80
    angle_min: float = -60.0
    angle_max: float = 60.0
    shift_horizontal_min: float = -80.0
    shift_horizontal_max: float = 80.0
    shift_vertical_min: float = -80.0
    shift_vertical_max: float = 80.0

    def sample(self) -> PoseTransform:
        angle = random.uniform(self.angle_min, self.angle_max)
        shift_horizontal = random.uniform(
            self.shift_horizontal_min, self.shift_horizontal_max
        )
        shift_vertical = random.uniform(
            self.shift_vertical_min, self.shift_vertical_max
        )
        return PoseTransform(
            angle=angle,
            shift_vertical=shift_vertical,
            shift_horizontal=shift_horizontal,
            pad=self.pad,
        )

    def __call__(self, img: torch.Tensor) -> torch.Tensor:
        return self.sample()(img)

    def save(self, path: str) -> None:
        with open(path, "w") as file:
            json.dump(self.__dict__, file)

    @staticmethod
    def load(path: str) -> "RandomPoseTransform":
        with open(path, "r") as file:
            dist = RandomPoseTransform()
            dist.__dict__ = json.load(file)
            return dist


@dataclass
class RandomQualityTransform:
    gain_min: float = 0.9
    gain_max: float = 1.1
    contrast_min: float = 0.9
    contrast_max: float = 2.0

    @torch.no_grad()
    def __call__(self, img: torch.Tensor) -> torch.Tensor:
        gain = random.uniform(self.gain_min, self.gain_max)
        img = VTF.adjust_brightness(img, brightness_factor=gain)
        contrast = random.uniform(self.contrast_min, self.contrast_max)
        img = VTF.adjust_contrast(img, contrast_factor=contrast)
        return img


def main():
    from flx.visualization.show_with_opencv import show_tensor_as_image
    from flx.setup.paths import get_fingerprint_dataset_path

    img_path = os.path.join(get_fingerprint_dataset_path("SFingev2Example"), "1_1.png")
    img = cv2.imread(img_path, flags=cv2.IMREAD_GRAYSCALE)
    toTensor = torchvision.transforms.ToTensor()
    img: torch.Tensor = toTensor(img)
    show_tensor_as_image(img)

    pose_dist1 = RandomPoseTransform()
    pose_dist1.save("pose_test.json")
    pose_dist2 = RandomPoseTransform.load("pose_test.json")

    quality_trafo = RandomQualityTransform()

    while True:
        pose_trafo = pose_dist2.sample()
        print(pose_trafo)
        transf = pose_trafo(img)

        transf = quality_trafo(transf)
        show_tensor_as_image(transf)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/image_processing
文件名: binarization.py
--------------------------------------------------------------------------------
import math

import torchvision.transforms.functional as VTF
import torch

from flx.visualization.show_with_opencv import (
    save_3Dtensor_as_image_grid,
    save_2Dtensor_as_image,
    show_tensor_as_image,
)

from flx.models.torch_helpers import get_device


def _is_even(n: int) -> bool:
    return n % 2 == 0


def _round_uneven(n: int) -> int:
    n = int(n)
    if _is_even(n):
        return n + 1
    return n


def _reshape_first_dim_1(img: torch.Tensor) -> torch.Tensor:
    if len(img.shape) == 2:
        return torch.reshape(img, shape=(1, img.shape[0], img.shape[1]))
    return img


def _make_wave_pattern_scaled(ridge_width: float, n_ridges: int) -> torch.Tensor:
    assert type(n_ridges) == int
    assert n_ridges >= 1

    range_size = n_ridges * torch.pi / 2
    x = torch.linspace(
        -range_size - (torch.pi / 2),
        range_size - (torch.pi / 2),
        _round_uneven(ridge_width * n_ridges),
    )
    x = _reshape_first_dim_1(x.repeat(x.shape[0], 1))
    x = torch.sin(x)
    return x


def _make_gaussian_kernel_2d(
    sigma_x: float, sigma_y: float, size_x: int, size_y: int
) -> torch.Tensor:
    support_y = torch.arange(0, size_y, dtype=torch.float) - (size_y / 2)
    kernel_y = torch.exp(
        torch.distributions.Normal(loc=0, scale=sigma_y).log_prob(support_y)
    )
    support_x = torch.arange(0, size_x, dtype=torch.float) - (size_x / 2)
    kernel_x = torch.exp(
        torch.distributions.Normal(loc=0, scale=sigma_x).log_prob(support_x)
    )
    kernel2d = torch.outer(kernel_y, kernel_x)
    return _reshape_first_dim_1(kernel2d / torch.sum(kernel2d))


@torch.no_grad()
def _make_rotated_filters(
    num_levels: int, ridge_width: float, size: int
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Creates rotated versions of the given ridge filter.
    """
    n_ridges: int = size * 2 + 1
    # Image must be larger by factor of at least 1.41 to show pattern and not fill color in corners when rotated
    n_ridges_full_pattern = _round_uneven(int(n_ridges * 1.41))
    pattern = _make_wave_pattern_scaled(ridge_width, n_ridges_full_pattern)
    gauss = _make_gaussian_kernel_2d(
        sigma_x=pattern.shape[1] * 0.12,
        sigma_y=pattern.shape[2] * 0.24,
        size_x=pattern.shape[1],
        size_y=pattern.shape[2],
    )

    outsize = _round_uneven(n_ridges * ridge_width)
    pattern_rot = []
    angle_per_level = 180.0 / num_levels
    for i in range(num_levels):
        gr = VTF.center_crop(
            VTF.rotate(
                gauss, angle_per_level * i, interpolation=VTF.InterpolationMode.BILINEAR
            ),
            output_size=[outsize, outsize],
        )
        gr = gr / torch.sum(gr)

        pr = VTF.center_crop(
            VTF.rotate(
                pattern,
                angle_per_level * i,
                interpolation=VTF.InterpolationMode.BILINEAR,
            ),
            output_size=[outsize, outsize],
        )
        pr = pr * gr
        pr = pr - (torch.sum(pr) / pr.numel())
        pr = pr / torch.sum(torch.abs(pr))
        pattern_rot.append(pr)
    pattern_rot = torch.concat(pattern_rot)
    pattern_rot = torch.reshape(pattern_rot, shape=(num_levels, 1, outsize, outsize))
    return pattern_rot


def _normalize_0_1(img: torch.Tensor) -> torch.Tensor:
    img_min = torch.min(img)
    img_max = torch.max(img)
    if not img_max > img_min:
        return torch.zeros_like(img)
    return (img - img_min) / (img_max - img_min)


class _GaborFilter:
    def __init__(self, ridge_width: float):
        self._pattern = _make_rotated_filters(16, ridge_width, 2)

    @torch.no_grad()
    def __call__(self, img: torch.Tensor) -> torch.Tensor:
        img_filtered = torch.nn.functional.conv2d(
            img, self._pattern, stride=1, padding="valid"
        )

        ridges = -torch.amin(img_filtered, dim=0)
        ridges = torch.threshold(ridges, 0.0, 0.0)
        ridges = _reshape_first_dim_1(ridges)
        assert torch.amin(0.0 <= ridges)
        assert torch.amax(ridges <= 1.0)
        size_diff = img.shape[-1] - ridges.shape[-1]
        assert size_diff % 2 == 0
        return VTF.pad(ridges, padding=int(size_diff / 2), fill=0.0)


def _pad_to_match_shape(img: torch.Tensor, other: torch.Tensor) -> torch.Tensor:
    x_diff = other.shape[-1] - img.shape[-1]
    assert x_diff > 0
    y_diff = other.shape[-2] - img.shape[-2]
    assert y_diff > 0
    pad_left = int(x_diff / 2)
    pad_right = x_diff - pad_left
    pad_top = int(y_diff / 2)
    pad_bot = y_diff - pad_top
    return VTF.pad(img, [pad_left, pad_top, pad_right, pad_bot], 0.0)


class _FingerprintBinarizer:
    SMOOTHING_FACTOR = 0.6

    def __init__(self, ridge_width: float):
        self._gabor_filters = [
            _GaborFilter(ridge_width * 0.6),
            _GaborFilter(ridge_width),
            _GaborFilter(ridge_width / 0.6),
        ]

        self._blur_kernel_size: int = _round_uneven(ridge_width)
        self._blur_kernel_sigma: float = self._blur_kernel_size * self.SMOOTHING_FACTOR

    @torch.no_grad()
    def binarize(self, img: torch.Tensor) -> torch.Tensor:
        img = _normalize_0_1(img)
        img_inv = 1.0 - img
        # show_tensor_as_image(img, "IN", wait=False)

        ridges = self._gabor_filters[0](img)
        valleys = self._gabor_filters[0](-img)
        for f in self._gabor_filters[1:]:
            ridges = torch.maximum(ridges, f(img))
            valleys = torch.maximum(valleys, f(img_inv))
        ridges = _normalize_0_1(ridges)
        valleys = _normalize_0_1(valleys)

        # show_tensor_as_image(ridges, "ridges", wait=False)
        # show_tensor_as_image(valleys, "valleys", wait=False)

        segmented = VTF.gaussian_blur(
            ridges + valleys, self._blur_kernel_size * 7, self._blur_kernel_sigma * 7
        )
        segmented = (segmented > 0.1).float()
        # Ignore areas at the edge, they are not reliable
        segmented = VTF.resize(
            segmented,
            [int(segmented.shape[-2] * 0.9), int(segmented.shape[-1] * 0.9)],
            antialias=True,
        )
        segmented = _pad_to_match_shape(segmented, ridges)
        # show_tensor_as_image(segmented, "segmented", wait=False)

        ridges = VTF.adjust_contrast(ridges - valleys, 8.0) * segmented
        # show_tensor_as_image(ridges, "ridges minus valleys after seg", wait=False)

        ridges = (ridges > 0.2).float()
        # show_tensor_as_image(ridges, "OUT")

        return ridges


class LazilyAllocatedBinarizer:
    def __init__(self, ridge_width: float):
        self._ridge_width: float = ridge_width
        self._binarizer: _FingerprintBinarizer = None

    def __call__(self, img: torch.Tensor):
        if self._binarizer is None:
            self._binarizer = _FingerprintBinarizer(self._ridge_width)
        return self._binarizer.binarize(img)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: InceptionV4.py
--------------------------------------------------------------------------------
"""
BSD 3-Clause License

Copyright (c) 2017, Remi Cadene
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name of the copyright holder nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

from __future__ import print_function, division, absolute_import
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as model_zoo


class BasicConv2d(nn.Module):
    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(
            in_planes,
            out_planes,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            bias=False,
        )  # verify bias false
        self.bn = nn.BatchNorm2d(
            out_planes,
            eps=0.001,  # value found in tensorflow
            momentum=0.1,  # default pytorch value
            affine=True,
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class Mixed_3a(nn.Module):
    def __init__(self):
        super(Mixed_3a, self).__init__()
        self.maxpool = nn.MaxPool2d(3, stride=2)
        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)

    def forward(self, x):
        x0 = self.maxpool(x)
        x1 = self.conv(x)
        out = torch.cat((x0, x1), 1)
        return out


class Mixed_4a(nn.Module):
    def __init__(self):
        super(Mixed_4a, self).__init__()

        self.branch0 = nn.Sequential(
            BasicConv2d(160, 64, kernel_size=1, stride=1),
            BasicConv2d(64, 96, kernel_size=3, stride=1),
        )

        self.branch1 = nn.Sequential(
            BasicConv2d(160, 64, kernel_size=1, stride=1),
            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),
            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),
            BasicConv2d(64, 96, kernel_size=(3, 3), stride=1),
        )

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        out = torch.cat((x0, x1), 1)
        return out


class Mixed_5a(nn.Module):
    def __init__(self):
        super(Mixed_5a, self).__init__()
        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)
        self.maxpool = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.conv(x)
        x1 = self.maxpool(x)
        out = torch.cat((x0, x1), 1)
        return out


class Inception_A(nn.Module):
    def __init__(self):
        super(Inception_A, self).__init__()
        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)

        self.branch1 = nn.Sequential(
            BasicConv2d(384, 64, kernel_size=1, stride=1),
            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),
        )

        self.branch2 = nn.Sequential(
            BasicConv2d(384, 64, kernel_size=1, stride=1),
            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),
            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1),
        )

        self.branch3 = nn.Sequential(
            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
            BasicConv2d(384, 96, kernel_size=1, stride=1),
        )

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Reduction_A(nn.Module):
    def __init__(self):
        super(Reduction_A, self).__init__()
        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)

        self.branch1 = nn.Sequential(
            BasicConv2d(384, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),
            BasicConv2d(224, 256, kernel_size=3, stride=2),
        )

        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Inception_B(nn.Module):
    def __init__(self):
        super(Inception_B, self).__init__()
        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)

        self.branch1 = nn.Sequential(
            BasicConv2d(1024, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0)),
        )

        self.branch2 = nn.Sequential(
            BasicConv2d(1024, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),
            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),
            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),
            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),
        )

        self.branch3 = nn.Sequential(
            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
            BasicConv2d(1024, 128, kernel_size=1, stride=1),
        )

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        x3 = self.branch3(x)
        out = torch.cat((x0, x1, x2, x3), 1)
        return out


class Reduction_B(nn.Module):
    def __init__(self):
        super(Reduction_B, self).__init__()

        self.branch0 = nn.Sequential(
            BasicConv2d(1024, 192, kernel_size=1, stride=1),
            BasicConv2d(192, 192, kernel_size=3, stride=2),
        )

        self.branch1 = nn.Sequential(
            BasicConv2d(1024, 256, kernel_size=1, stride=1),
            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),
            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),
            BasicConv2d(320, 320, kernel_size=3, stride=2),
        )

        self.branch2 = nn.MaxPool2d(3, stride=2)

    def forward(self, x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)
        out = torch.cat((x0, x1, x2), 1)
        return out


class Inception_C(nn.Module):
    def __init__(self):
        super(Inception_C, self).__init__()

        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)

        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
        self.branch1_1a = BasicConv2d(
            384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1)
        )
        self.branch1_1b = BasicConv2d(
            384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0)
        )

        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)
        self.branch2_1 = BasicConv2d(
            384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0)
        )
        self.branch2_2 = BasicConv2d(
            448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1)
        )
        self.branch2_3a = BasicConv2d(
            512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1)
        )
        self.branch2_3b = BasicConv2d(
            512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0)
        )

        self.branch3 = nn.Sequential(
            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
            BasicConv2d(1536, 256, kernel_size=1, stride=1),
        )

    def forward(self, x):
        x0 = self.branch0(x)

        x1_0 = self.branch1_0(x)
        x1_1a = self.branch1_1a(x1_0)
        x1_1b = self.branch1_1b(x1_0)
        x1 = torch.cat((x1_1a, x1_1b), 1)

        x2_0 = self.branch2_0(x)
        x2_1 = self.branch2_1(x2_0)
        x2_2 = self.branch2_2(x2_1)
        x2_3a = self.branch2_3a(x2_2)
        x2_3b = self.branch2_3b(x2_2)
        x2 = torch.cat((x2_3a, x2_3b), 1)

        x3 = self.branch3(x)

        out = torch.cat((x0, x1, x2, x3), 1)
        return out
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: center_loss.py
--------------------------------------------------------------------------------
from os.path import join

import torch
import torch.nn as nn


class CenterLoss(nn.Module):
    """Center loss.

    Reference:
    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.

    Args:
        num_classes (int): number of classes.
        feat_dim (int): feature dimension.
        alpha: "learning rate" of the centers. For each datapoint centers are updated by center + alpha * (datapoint - center)
    """

    def __init__(self, num_classes: int, feat_dim: int, alpha: float = 0.01):
        super(CenterLoss, self).__init__()
        self.alpha = alpha
        self.register_buffer(
            "centers",
            torch.nn.functional.normalize(torch.randn(num_classes, feat_dim), dim=1),
            persistent=True,
        )
        self.counter = 0
        self.nupdate = 0

    def forward(self, x: torch.Tensor, labels: torch.LongTensor):
        """
        Args:
            x: feature matrix with shape (batch_size, feat_dim).
            labels: ground truth labels with shape (batch_size).
        """
        # Copy the centers into temporary matrix
        with torch.no_grad():
            batch_centers = torch.index_select(self.centers, 0, labels)
        # Get difference of x from batch centers
        diff = x - batch_centers
        # Update current centers
        with torch.no_grad():
            self.centers.index_add_(0, labels, diff, alpha=self.alpha)
            # self._disperse_centers()
        # Return mean loss
        return torch.sum(diff**2)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: deep_print_arch.py
--------------------------------------------------------------------------------
from flx.models import InceptionV4
import torch
from torch import nn

from flx.models.localization_network import LocalizationNetwork

"""
Terminology:

Each subject in out biometric dataset (i.e. each fingerprint)
corresponds to one class in the logits that are calculated from the embedding.

We employ a loss function called the "center loss" (see the corresponding class)
that encourages the model to generate similar embeddings for samples from the
same subject.

"""

DEEPPRINT_INPUT_SIZE = 299

"""
  _ __ ___   ___   __| | ___| |   ___ ___  _ __ ___  _ __   ___  _ __   ___ _ __ | |_ ___ 
 | '_ ` _ \ / _ \ / _` |/ _ \ |  / __/ _ \| '_ ` _ \| '_ \ / _ \| '_ \ / _ \ '_ \| __/ __|
 | | | | | | (_) | (_| |  __/ | | (_| (_) | | | | | | |_) | (_) | | | |  __/ | | | |_\__ \ 
 |_| |_| |_|\___/ \__,_|\___|_|  \___\___/|_| |_| |_| .__/ \___/|_| |_|\___|_| |_|\__|___/
                                                    |_|                                   
"""


class _InceptionV4_Stem(nn.Module):
    def __init__(self):
        super(_InceptionV4_Stem, self).__init__()
        # Modules
        self.features = nn.Sequential(
            InceptionV4.BasicConv2d(1, 32, kernel_size=3, stride=2),
            InceptionV4.BasicConv2d(32, 32, kernel_size=3, stride=1),
            InceptionV4.BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),
            InceptionV4.Mixed_3a(),
            InceptionV4.Mixed_4a(),
            InceptionV4.Mixed_5a(),
        )

    def forward(self, input):
        assert input.shape[-1] == DEEPPRINT_INPUT_SIZE
        assert input.shape[-2] == DEEPPRINT_INPUT_SIZE
        x = self.features(input)
        return x


class _Branch_TextureEmbedding(nn.Module):
    def __init__(self, texture_embedding_dims: int):
        super(_Branch_TextureEmbedding, self).__init__()
        self._0_block = nn.Sequential(
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Reduction_A(),
        )

        self._1_block = nn.Sequential(
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Inception_B(),
            InceptionV4.Reduction_B(),
        )

        self._2_block = nn.Sequential(
            InceptionV4.Inception_C(),
            InceptionV4.Inception_C(),
            InceptionV4.Inception_C(),
        )

        self._3_avg_pool2d = nn.AvgPool2d(
            kernel_size=8
        )  # Might need adjustment if the input size is changed
        self._4_flatten = nn.Flatten()
        self._5_dropout = nn.Dropout(p=0.2)
        self._6_linear = nn.Linear(1536, texture_embedding_dims)

    def forward(self, input):
        x = self._0_block(input)
        x = self._1_block(x)
        x = self._2_block(x)
        x = self._3_avg_pool2d(x)
        x = self._4_flatten(x)
        x = self._5_dropout(x)
        x = self._6_linear(x)
        x = torch.nn.functional.normalize(torch.squeeze(x), dim=1)
        return x


class _Branch_MinutiaStem(nn.Module):
    def __init__(self):
        super().__init__()
        # Modules
        self.features = nn.Sequential(
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
            InceptionV4.Inception_A(),
        )

    def forward(self, input):
        return self.features(input)


class _Branch_MinutiaEmbedding(nn.Module):
    def __init__(self, minutia_embedding_dims: int):
        super().__init__()
        # Modules
        self._0_block = nn.Sequential(
            nn.Conv2d(384, 768, kernel_size=3, stride=1, padding=1),
            nn.Conv2d(768, 768, kernel_size=3, stride=2, padding=1),
            nn.Conv2d(768, 896, kernel_size=3, stride=1, padding=1),
            nn.Conv2d(896, 1024, kernel_size=3, stride=2, padding=1),
        )
        self._1_max_pool2d = nn.MaxPool2d(kernel_size=9, stride=1)
        self._2_flatten = nn.Flatten()
        self._3_dropout = nn.Dropout(p=0.2)
        self._4_linear = nn.Linear(1024, minutia_embedding_dims)

    def forward(self, input):
        x = self._0_block(input)
        x = self._1_max_pool2d(x)
        x = self._2_flatten(x)
        x = self._3_dropout(x)
        x = self._4_linear(x)
        x = torch.nn.functional.normalize(x, dim=1)
        return x


class _Branch_MinutiaMap(nn.Module):
    def __init__(self):
        super().__init__()
        # Modules
        self.features = nn.Sequential(
            nn.ConvTranspose2d(384, 128, kernel_size=3, stride=2),
            nn.Conv2d(128, 128, kernel_size=7, stride=1),
            nn.ConvTranspose2d(128, 32, kernel_size=3, stride=2),
            nn.Conv2d(32, 6, kernel_size=3, stride=1),
        )

    def forward(self, input):
        # The network produces maps of size 129x129 but we want maps of size 128x128
        # so we remove the last row / column from each map
        return self.features(input)[:, :, :-1, :-1]


"""
  _ __ ___   ___   __| | ___| | __   ____ _ _ __(_) __ _ _ __ | |_ ___ 
 | '_ ` _ \ / _ \ / _` |/ _ \ | \ \ / / _` | '__| |/ _` | '_ \| __/ __|
 | | | | | | (_) | (_| |  __/ |  \ V / (_| | |  | | (_| | | | | |_\__ \ 
 |_| |_| |_|\___/ \__,_|\___|_|   \_/ \__,_|_|  |_|\__,_|_| |_|\__|___/                                                                    
"""


class DeepPrintOutput:
    def __init__(
        self,
        minutia_embeddings: torch.Tensor = None,
        texture_embeddings: torch.Tensor = None,
    ):
        self.minutia_embeddings: torch.Tensor = minutia_embeddings
        self.texture_embeddings: torch.Tensor = texture_embeddings

    @staticmethod
    def training():
        return False


class DeepPrintTrainingOutput(DeepPrintOutput):
    def __init__(
        self,
        minutia_logits: torch.Tensor = None,
        texture_logits: torch.Tensor = None,
        combined_logits: torch.Tensor = None,
        minutia_maps: torch.Tensor = None,
        **kwargs
    ):
        self.minutia_logits: torch.Tensor = minutia_logits
        self.texture_logits: torch.Tensor = texture_logits
        self.combined_logits: torch.Tensor = combined_logits
        self.minutia_maps: torch.Tensor = minutia_maps
        super().__init__(**kwargs)

    @staticmethod
    def training():
        return True


class DeepPrint_Tex(nn.Module):
    """
    Model with only the texture branch.

    In training mode:
        Outputs the texture embedding AND
        A vector of propabilities over all classes (each class being one subject)
    In evaluation mode:
        Outputs the texture embedding
    """

    def __init__(self, num_fingerprints: int, texture_embedding_dims: int):
        super().__init__()
        # Modules
        self.stem = _InceptionV4_Stem()
        self.texture_branch = _Branch_TextureEmbedding(
            texture_embedding_dims=texture_embedding_dims
        )
        self.texture_logits = nn.Sequential(
            nn.Linear(texture_embedding_dims, num_fingerprints), nn.Dropout(p=0.2)
        )

    def forward(self, input) -> torch.Tensor:
        if self.training:
            x = self.stem(input)
            x = self.texture_branch.forward(x)
            logits = self.texture_logits.forward(x)
            return DeepPrintTrainingOutput(texture_logits=logits, texture_embeddings=x)

        with torch.no_grad():
            x = self.stem(input)
            x = self.texture_branch.forward(x)
            return DeepPrintOutput(texture_embeddings=x)


class DeepPrint_Minu(nn.Module):
    """
    Model with only the minutia branch.

    In training mode:
        Outputs the minutia embedding AND
        A vector of propabilities over all classes (each class being one subject) AND
        Predicted minutia maps
    In evaluation mode:
        Outputs the minutia embeddings
    """

    def __init__(self, num_fingerprints: int, minutia_embedding_dims: int):
        super().__init__()
        # Modules
        self.stem = _InceptionV4_Stem()
        self.minutia_stem = _Branch_MinutiaStem()
        self.minutia_map = _Branch_MinutiaMap()
        self.minutia_embedding = _Branch_MinutiaEmbedding(minutia_embedding_dims)
        self.minutia_logits = nn.Sequential(
            nn.Linear(minutia_embedding_dims, num_fingerprints), nn.Dropout(p=0.2)
        )

    def forward(self, input: torch.Tensor) -> DeepPrintOutput:
        if self.training:
            x = self.stem(input)

            x_minutia = self.minutia_stem.forward(x)
            x_minutia_emb = self.minutia_embedding.forward(x_minutia)
            x_minutia_logits = self.minutia_logits(x_minutia_emb)
            x_minutia_map = self.minutia_map(x_minutia)

            return DeepPrintTrainingOutput(
                minutia_logits=x_minutia_logits,
                minutia_maps=x_minutia_map,
                minutia_embeddings=x_minutia_emb,
            )

        with torch.no_grad():
            x = self.stem(input)
            x_minutia = self.minutia_stem.forward(x)

            x_minutia_emb = self.minutia_embedding.forward(x_minutia)
            return DeepPrintOutput(minutia_embeddings=x_minutia_emb)


class DeepPrint_TexMinu(nn.Module):
    """
    Model with texture and minutia branch

    In training mode:
        Outputs the texture embedding AND
        A vector of propabilities over all classes predicted from the texture embedding
    In evaluation mode:
        Outputs the texture embedding

    """

    def __init__(
        self, num_fingerprints, texture_embedding_dims: int, minutia_embedding_dims: int
    ):
        super().__init__()
        # Modules
        self.stem = _InceptionV4_Stem()
        self.texture_branch = _Branch_TextureEmbedding(texture_embedding_dims)
        self.texture_logits = nn.Sequential(
            nn.Linear(texture_embedding_dims, num_fingerprints), nn.Dropout(p=0.2)
        )
        self.minutia_stem = _Branch_MinutiaStem()
        self.minutia_map = _Branch_MinutiaMap()
        self.minutia_embedding = _Branch_MinutiaEmbedding(minutia_embedding_dims)
        self.minutia_logits = nn.Sequential(
            nn.Linear(minutia_embedding_dims, num_fingerprints), nn.Dropout(p=0.2)
        )

    def forward(self, input: torch.Tensor) -> DeepPrintOutput:
        if self.training:
            x = self.stem(input)

            x_texture_emb = self.texture_branch.forward(x)
            x_texture_logits = self.texture_logits(x_texture_emb)

            x_minutia = self.minutia_stem.forward(x)
            x_minutia_emb = self.minutia_embedding.forward(x_minutia)
            x_minutia_logits = self.minutia_logits(x_minutia_emb)
            x_minutia_map = self.minutia_map(x_minutia)

            return DeepPrintTrainingOutput(
                minutia_logits=x_minutia_logits,
                texture_logits=x_texture_logits,
                minutia_maps=x_minutia_map,
                minutia_embeddings=x_minutia_emb,
                texture_embeddings=x_texture_emb,
            )

        with torch.no_grad():
            x = self.stem(input)
            x_texture_emb = self.texture_branch.forward(x)
            x_minutia = self.minutia_stem.forward(x)

            x_minutia_emb = self.minutia_embedding.forward(x_minutia)
            return DeepPrintOutput(x_minutia_emb, x_texture_emb)


class DeepPrint_LocTex(nn.Module):
    """
    Model with texture branch and localization network.

    In training mode:
        Outputs the texture embedding AND
        A vector of propabilities over all classes predicted from the texture embedding
    In evaluation mode:
        Outputs the texture embedding

    """

    def __init__(self, num_fingerprints, texture_embedding_dims: int):
        super().__init__()
        # Special attributs
        self.input_space = None

        # Modules
        self.localization = LocalizationNetwork()
        self.embeddings = DeepPrint_Tex(
            num_fingerprints=num_fingerprints,
            texture_embedding_dims=texture_embedding_dims,
        )

    def forward(self, input: torch.Tensor) -> DeepPrintOutput:
        if self.training:
            x = self.localization(input)
            self.embeddings.train()
            return self.embeddings(x)

        with torch.no_grad():
            x = self.localization(input)
            self.embeddings.eval()
            return self.embeddings(x)


class DeepPrint_LocMinu(nn.Module):
    """
    Model with minutia branch and localization network.

    In training mode:
        Outputs the minutia embedding AND
        A vector of propabilities over all classes predicted from the minutia embedding AND
        The generated minutia maps
    In evaluation mode:
        Outputs the minutia embedding

    """

    def __init__(self, num_fingerprints, minutia_embedding_dims: int):
        super().__init__()
        # Special attributs
        self.input_space = None

        # Modules
        self.localization = LocalizationNetwork()
        self.embeddings = DeepPrint_Minu(
            num_fingerprints=num_fingerprints,
            minutia_embedding_dims=minutia_embedding_dims,
        )

    def forward(self, input: torch.Tensor) -> DeepPrintOutput:
        if self.training:
            x = self.localization(input)
            self.embeddings.train()
            return self.embeddings(x)

        with torch.no_grad():
            x = self.localization(input)
            self.embeddings.eval()
            return self.embeddings(x)


class DeepPrint_LocTexMinu(nn.Module):
    """
    Model with texture and minutia branch and localization network.

    In training mode:
        Outputs the texture embedding AND
        A vector of propabilities over all classes predicted from the texture embedding
    In evaluation mode:
        Outputs the texture embedding

    """

    def __init__(
        self, num_fingerprints, texture_embedding_dims: int, minutia_embedding_dims: int
    ):
        super().__init__()
        # Special attributs
        self.input_space = None

        # Modules
        self.localization = LocalizationNetwork()
        self.embeddings = DeepPrint_TexMinu(
            num_fingerprints=num_fingerprints,
            texture_embedding_dims=texture_embedding_dims,
            minutia_embedding_dims=minutia_embedding_dims,
        )

    def forward(self, input: torch.Tensor) -> DeepPrintOutput:
        if self.training:
            x = self.localization(input)
            self.embeddings.train()
            return self.embeddings(x)

        with torch.no_grad():
            x = self.localization(input)
            self.embeddings.eval()
            return self.embeddings(x)


def main():
    model = DeepPrint_Tex(1000, 100)
    model = DeepPrint_Minu(1000, 100)
    model = DeepPrint_TexMinu(1000, 100, 100)
    model = DeepPrint_LocTex(1000, 100)
    model = DeepPrint_LocMinu(1000, 100)
    model = DeepPrint_LocTexMinu(1000, 100, 100)
    print("no syntax errors")


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: deep_print_loss.py
--------------------------------------------------------------------------------
import torch
from torch import nn

from flx.models.deep_print_arch import DeepPrintTrainingOutput
from flx.models.center_loss import CenterLoss

W_CROSS_ENTROPY = 1.0
W_CENTER_LOSS = 0.125
W_MINUTIA_MAP_LOSS = 0.3


def _compute_minutia_map_loss(
    x: torch.Tensor,
    minutia_maps: torch.Tensor,
    minutia_map_weights: torch.Tensor,
) -> torch.Tensor:
    mm_squared_diff = (x - minutia_maps) ** 2
    mm_mse = mm_squared_diff.reshape(minutia_map_weights.shape[0], -1).mean(dim=1)
    return (mm_mse * minutia_map_weights).mean()


class _DeepPrint_Embedding_Loss(nn.Module):
    def __init__(self, num_classes: int, num_embedding_dims: int):
        """
        @param center_loss_weight : Must be greater than zero. The center loss is multiplied by this term.
        """
        super().__init__()
        self.crossent_loss_fun = nn.CrossEntropyLoss()
        self.center_loss_fun = CenterLoss(num_classes, num_embedding_dims)
        self.crossent_loss_sum: float = 0
        self.center_loss_sum: float = 0
        self.n_datapoints: int = 0

    def reset_recorded_loss(self) -> None:
        self.crossent_loss_sum = 0
        self.center_loss_sum = 0

    def get_recorded_loss(self) -> dict:
        if self.n_datapoints == 0:
            return {"crossent_loss_sum": 0, "center_loss_sum": 0}
        return {
            "crossent_loss_sum": self.crossent_loss_sum / self.n_datapoints,
            "center_loss_sum": self.center_loss_sum / self.n_datapoints,
        }

    def forward(
        self, embeddings: torch.Tensor, logits: torch.Tensor, labels: torch.Tensor
    ) -> torch.Tensor:
        crossent_loss = self.crossent_loss_fun(logits, labels)
        center_loss = self.center_loss_fun(embeddings, labels)
        self.crossent_loss_sum += float(crossent_loss) * W_CROSS_ENTROPY
        self.center_loss_sum += float(center_loss) * W_CENTER_LOSS
        self.n_datapoints += labels.shape[0]
        return W_CROSS_ENTROPY * crossent_loss + W_CENTER_LOSS * center_loss


class DeepPrintLoss_Tex(nn.Module):
    """
    The weights that are use for the

    """

    def __init__(self, num_classes: int, texture_embedding_dims: int):
        """
        @param center_loss_weight : Must be greater than zero. The center loss is multiplied by this term.
        """
        super().__init__()
        self.texture_loss_fun: _DeepPrint_Embedding_Loss = _DeepPrint_Embedding_Loss(
            num_classes=num_classes, num_embedding_dims=texture_embedding_dims
        )

    def forward(
        self,
        output: DeepPrintTrainingOutput,
        labels: torch.Tensor,
        minutia_maps: torch.Tensor,
        minutia_map_weights: torch.Tensor,
    ) -> torch.Tensor:
        """
        @param output : Output of the model for the given fingerprint images; Object of type DeepPrintTrainingOutput
        @param labels : Ground truth subject ids for fingerprints images
        @param minutia_maps : Ground truth minutia maps for the fingerprint images
        """
        if minutia_maps.shape[1] != 0:
            raise RuntimeWarning(
                "DeepPrint_TextureLoss received non-empty minutia_maps!"
            )
        texture_loss = self.texture_loss_fun(
            output.texture_embeddings, output.texture_logits, labels
        )
        return texture_loss

    def get_recorded_loss(self) -> dict:
        return self.texture_loss_fun.get_recorded_loss()

    def reset_recorded_loss(self) -> None:
        self.texture_loss_fun.reset_recorded_loss()


class DeepPrintLoss_Minu(nn.Module):
    def __init__(
        self,
        num_classes: int,
        minutia_embedding_dims: int,
    ):
        """
        @param center_loss_weight : Must be greater than zero. The center loss is multiplied by this term.
        """
        super().__init__()
        self.minu_loss_fun: _DeepPrint_Embedding_Loss = _DeepPrint_Embedding_Loss(
            num_classes=num_classes,
            num_embedding_dims=minutia_embedding_dims,
        )
        self.minu_map_loss_sum: float = 0

    def forward(
        self,
        output: DeepPrintTrainingOutput,
        labels: torch.Tensor,
        minutia_maps: torch.Tensor,
        minutia_map_weights: torch.Tensor,
    ) -> torch.Tensor:
        """
        @param output : Output of the model for the given fingerprint images; Object of type DeepPrintTrainingOutput
        @param labels : Ground truth subject ids for fingerprints images
        @param minutia_maps : Ground truth minutia maps for the fingerprint images
        """
        minutia_loss = self.minu_loss_fun(
            output.minutia_embeddings, output.minutia_logits, labels
        )
        mm_loss = _compute_minutia_map_loss(
            output.minutia_maps, minutia_maps, minutia_map_weights
        )
        self.minu_map_loss_sum += W_MINUTIA_MAP_LOSS * float(mm_loss)
        return minutia_loss + W_MINUTIA_MAP_LOSS * mm_loss

    def get_recorded_loss(self) -> dict:
        return {
            "minutia_loss": self.minu_loss_fun.get_recorded_loss(),
            "minutia_map_loss": self.minu_map_loss_sum / self.minu_loss_fun.n_datapoints
            if self.minu_loss_fun.n_datapoints > 0
            else 0,
        }

    def reset_recorded_loss(self) -> None:
        self.minu_loss_fun.reset_recorded_loss()
        self.minu_map_loss_sum = 0


class DeepPrintLoss_TexMinu(nn.Module):
    """
    The weights that are use for the

    """

    def __init__(
        self,
        num_classes: int,
        texture_embedding_dims: int,
        minutia_embedding_dims: int,
    ):
        """
        @param center_loss_weight : Must be greater than zero. The center loss is multiplied by this term.
        """
        super().__init__()
        self.minu_loss_fun: _DeepPrint_Embedding_Loss = _DeepPrint_Embedding_Loss(
            num_classes=num_classes,
            num_embedding_dims=texture_embedding_dims,
        )
        self.texture_loss_fun: _DeepPrint_Embedding_Loss = _DeepPrint_Embedding_Loss(
            num_classes=num_classes,
            num_embedding_dims=minutia_embedding_dims,
        )
        self.minu_map_loss_sum: float = 0

    def forward(
        self,
        output: DeepPrintTrainingOutput,
        labels: torch.Tensor,
        minutia_maps: torch.Tensor,
        minutia_map_weights: torch.Tensor,
    ) -> torch.Tensor:
        """
        @param output : Output of the model for the given fingerprint images; Object of type DeepPrintTrainingOutput
        @param labels : Ground truth subject ids for fingerprints images
        @param minutia_maps : Ground truth minutia maps for the fingerprint images
        """
        minutia_loss = self.minu_loss_fun(
            output.minutia_embeddings, output.minutia_logits, labels
        )
        texture_loss = self.texture_loss_fun(
            output.texture_embeddings, output.texture_logits, labels
        )
        mm_loss = _compute_minutia_map_loss(
            output.minutia_maps, minutia_maps, minutia_map_weights
        )
        self.minu_map_loss_sum += W_MINUTIA_MAP_LOSS * float(mm_loss)
        return texture_loss + minutia_loss + W_MINUTIA_MAP_LOSS * mm_loss

    def get_recorded_loss(self) -> dict:
        return {
            "texture_loss": self.texture_loss_fun.get_recorded_loss(),
            "minutia_loss": self.minu_loss_fun.get_recorded_loss(),
            "minutia_map_loss": self.minu_map_loss_sum
            / self.texture_loss_fun.n_datapoints
            if self.texture_loss_fun.n_datapoints > 0
            else 0,
        }

    def reset_recorded_loss(self) -> None:
        self.texture_loss_fun.reset_recorded_loss()
        self.minu_loss_fun.reset_recorded_loss()
        self.minu_map_loss_sum = 0


def main():
    loss = DeepPrintLoss_Tex(100)
    loss = DeepPrintLoss_TexMinu(100)
    print("No syntax errors")


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: localization_network.py
--------------------------------------------------------------------------------
import torch
import torchvision


class LocalizationNetwork(torch.torch.nn.Module):
    def __init__(self):
        super().__init__()
        # The localization net uses a downsampled version of the image for performance
        self.input_size = (128, 128)
        self.resize = torchvision.transforms.Resize(
            size=self.input_size, antialias=True
        )
        # Spatial transformer localization-network
        self.localization = torch.nn.Sequential(
            torch.nn.Conv2d(1, 24, kernel_size=5, stride=1, padding=2),
            torch.nn.MaxPool2d(2, stride=2),
            torch.nn.Conv2d(24, 32, kernel_size=3, stride=1, padding=1),
            torch.nn.MaxPool2d(2, stride=2),
            torch.nn.Conv2d(32, 48, kernel_size=3, stride=1, padding=1),
            torch.nn.MaxPool2d(2, stride=2),
            torch.nn.Conv2d(48, 64, kernel_size=3, stride=1, padding=1),
            torch.nn.MaxPool2d(2, stride=2),
        )

        # Regressor for the 3 * 2 affine matrix
        self.fc_loc = torch.nn.Sequential(
            torch.nn.Linear(8 * 8 * 64, 64), torch.nn.ReLU(), torch.nn.Linear(64, 3)
        )

        # Initialize the weights/bias with identity transformation
        self.fc_loc[2].weight.data.zero_()
        self.fc_loc[2].bias.data.copy_(torch.tensor([0, 0, 0], dtype=torch.float))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        resized_x = self.resize(x)
        xs = self.localization(resized_x)
        xs = xs.view(-1, 8 * 8 * 64)
        theta_x_y = self.fc_loc(xs)
        theta_x_y = theta_x_y.view(-1, 3)
        theta = theta_x_y[:, 0]  # Rotation angle
        # Construct rotation and scaling matrix
        m11 = torch.cos(theta)
        m12 = -torch.sin(theta)
        m13 = theta_x_y[:, 1]  # offset x
        m21 = torch.sin(theta)
        m22 = torch.cos(theta)
        m23 = theta_x_y[:, 2]  # offset y

        mat = torch.concatenate((m11, m12, m13, m21, m22, m23))
        mat = mat.view(-1, 2, 3)
        grid = torch.nn.functional.affine_grid(mat, x.size(), align_corners=False)
        x = torch.nn.functional.grid_sample(x, grid, align_corners=False)
        return x


def main():
    model = LocalizationNetwork()
    print("no syntax errors")


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: model_training.py
--------------------------------------------------------------------------------
from dataclasses import dataclass
import json
from os.path import join, exists

import tqdm
import shutil

import torch
import torchmetrics

from flx.setup.paths import get_best_model_file
from flx.setup.paths import get_newest_model_file
from flx.benchmarks.matchers import CosineSimilarityMatcher
from flx.benchmarks.verification import VerificationBenchmark, VerificationResult
from flx.data.embedding_loader import EmbeddingLoader
from flx.data.dataset import Dataset, ZippedDataLoader
from flx.extractor.extract_embeddings import extract_embeddings
from flx.models.deep_print_arch import DeepPrintTrainingOutput
from flx.setup.config import LEARNING_RATE
from flx.models.torch_helpers import (
    get_device,
    load_model_parameters,
    save_model_parameters,
    get_dataloader_args,
)


@dataclass
class TrainingLogEntry:
    epoch: int
    training_loss: float
    loss_statistics: float
    training_accuracy: float
    validation_equal_error_rate: float

    def __str__(self):
        s = "TrainingLogEntry(\n"
        for k, v in self.__dict__.items():
            s += f"    {k}={v},\n"
        return s + "}"


class TrainingLog:
    def __init__(self, path: str, reset: bool = False):
        self._path: str = path
        self._entries: list[TrainingLogEntry] = []
        if not reset and exists(path):
            self._load()
        else:
            self._save()

    def _save(self):
        with open(self._path, "w") as file:
            obj = {"entries": [e.__dict__ for e in self._entries]}
            json.dump(obj, file)

    def _load(self):
        with open(self._path, "r") as file:
            obj = json.load(file)
            self._entries = [TrainingLogEntry(**dct) for dct in obj["entries"]]

    @property
    def best_entry(self) -> TrainingLogEntry:
        return min(self._entries, key=lambda e: e.validation_equal_error_rate)

    def __len__(self) -> int:
        return len(self._entries)

    def add_entry(self, entry: TrainingLogEntry):
        self._entries.append(entry)
        self._save()


def _train(
    model: torch.nn.Module,
    loss_fun: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    train_set: Dataset,
) -> float:
    """
    Trains the model for one epoch.

    Returns
        - overall average epoch loss
        - a dict with the epoch loss of individual loss components
    """
    metric = torchmetrics.classification.MulticlassAccuracy(
        num_classes=train_set.num_subjects
    ).to(device=get_device())

    train_dataloader = torch.utils.data.DataLoader(
        train_set, **get_dataloader_args(train=True)
    )
    model.train()  # Outputs minutia maps and logits
    epoch_loss = 0
    loss_fun.reset_recorded_loss()
    for vals in tqdm.tqdm(train_dataloader):
        fp_imgs, minu_map_tpl, fp_labels = vals
        minu_maps, minu_map_weights = minu_map_tpl
        fp_imgs = fp_imgs.to(device=get_device())
        fp_labels = fp_labels.to(device=get_device())
        minu_maps = minu_maps.to(device=get_device())
        minu_map_weights = minu_map_weights.to(device=get_device())
        # Forward pass
        optimizer.zero_grad()
        output: DeepPrintTrainingOutput = model(fp_imgs)
        loss = loss_fun.forward(
            output=output,
            labels=fp_labels,
            minutia_maps=minu_maps,
            minutia_map_weights=minu_map_weights,
        )
        # Backward pass
        loss.backward()
        optimizer.step()

        # Record accuracy and loss
        epoch_loss += float(loss) * fp_labels.shape[0]
        if output.combined_logits is not None:
            logits = output.combined_logits
        elif output.minutia_logits is None:
            logits = output.texture_logits
        elif output.texture_logits is None:
            logits = output.minutia_logits
        else:
            logits = output.texture_logits + output.minutia_logits
        metric(logits, fp_labels)

    mean_loss = epoch_loss / len(train_set)
    multiclass_accuracy = float(metric.compute())
    return mean_loss, loss_fun.get_recorded_loss(), multiclass_accuracy


def _validate(
    model: torch.nn.Module,
    validation_set: Dataset,
    benchmark: VerificationBenchmark,
) -> float:
    """
    Validates the model.

    Returns equal error rate
    """
    texture_embeddings, minutia_embeddings = extract_embeddings(model, validation_set)
    embeddings = EmbeddingLoader.combine_if_both_exist(
        texture_embeddings, minutia_embeddings
    )

    matcher = CosineSimilarityMatcher(embeddings)
    result: VerificationResult = benchmark.run(matcher, save=False)
    return result.get_equal_error_rate()


def train_model(
    fingerprints: Dataset,
    minutia_maps: Dataset,
    labels: Dataset,
    validation_fingerprints: Dataset,
    validation_benchmark: VerificationBenchmark,
    model: torch.nn.Module,
    loss: torch.nn.Module,
    num_epochs: int,
    out_dir: str,
) -> None:
    """
    Trains model for num_iter and saves results (training log and model parameters)

    Automatically loads model parameters from "model.pyt" file if exists
    """
    print(f"Using device {get_device()}")
    # Create output directory and log file
    best_model_path = get_best_model_file(out_dir)
    model_path = get_newest_model_file(out_dir)
    log = TrainingLog(join(out_dir, "log.json"))

    model = model.to(device=get_device())
    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
    loss = loss.to(device=get_device())
    if exists(model_path):
        print(f"Loaded existing model from {model_path}")
        load_model_parameters(model_path, model, loss, optimizer)
    else:
        print(f"No model file found at {model_path}")

    training_set = Dataset.zip(fingerprints, minutia_maps, labels)

    for epoch in range(len(log) + 1, num_epochs + 1):
        print(f"\n\n --- Starting Epoch {epoch} of {num_epochs} ---")
        # Train
        print("\nTraining:")
        train_loss, loss_stats, accuracy = _train(model, loss, optimizer, training_set)
        print(f"Average Loss: {train_loss}")
        print(f"Multiclass accuracy: {accuracy}")

        save_model_parameters(model_path, model, loss, optimizer)

        if validation_fingerprints is None:
            # Use training accuracy as validation accuracy
            validation_eer = accuracy
        else:
            # Validate
            print("\nValidation:")
            validation_eer = _validate(model, validation_fingerprints, validation_benchmark)
            print(f"Equal Error Rate: {validation_eer}\n")


        # Log and determine if new model is best model
        entry = TrainingLogEntry(
            epoch, train_loss, loss_stats, accuracy, validation_eer
        )
        log.add_entry(entry)
        print(entry)

        if validation_eer <= log.best_entry.validation_equal_error_rate:
            shutil.copyfile(model_path, best_model_path)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/models
文件名: torch_helpers.py
--------------------------------------------------------------------------------
import os
import time

import torch


CUDA_DEVICE = 0
TRAIN_ON_A_100 = True


def get_dataloader_args(train: bool) -> dict:
    batch_size = 16
    if not train:
        batch_size *= 2  # More memory available without gradients
    if not torch.cuda.is_available():
        return {
            "batch_size": batch_size,
            "shuffle": train,
            "num_workers": 4,
            "prefetch_factor": 1,
        }
    if TRAIN_ON_A_100:  # Use 40GB graphics ram by preloading to pinned memory
        return {
            "batch_size": batch_size,
            "shuffle": train,
            "num_workers": 16,
            "prefetch_factor": 2,
            "pin_memory": True,
            "pin_memory_device": f"cuda:{CUDA_DEVICE}",
        }
    return {
        "batch_size": batch_size,
        "shuffle": train,
        "num_workers": 4,
        "prefetch_factor": 1,
        "pin_memory": True,
        "pin_memory_device": f"cuda:{CUDA_DEVICE}",
    }


def get_device() -> str:
    if torch.cuda.is_available():
        return torch.device(f"cuda:{CUDA_DEVICE}")
    return torch.device("cpu")


def save_model_parameters(
    full_param_path: str,
    model: torch.nn.Module,
    loss: torch.nn.Module,
    optim: torch.optim.Optimizer,
) -> None:
    """
    Tries to save the parameters of model and optimizer in the given path
    """
    try:
        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "loss_state_dict": loss.state_dict(),
                "optimizer_state_dict": optim.state_dict(),
            },
            full_param_path,
        )
    except KeyboardInterrupt:
        print("\n>>>>>>>>> Model is being saved! Will exit when done <<<<<<<<<<\n")
        save_model_parameters(full_param_path, model, optim)
        time.sleep(10)
        raise KeyboardInterrupt()


def load_model_parameters(
    full_param_path: str,
    model: torch.nn.Module,
    loss: torch.nn.Module,
    optim: torch.optim.Optimizer,
) -> None:
    """
    Tries to load the parameters stored in the given path
    into the given model and optimizer.
    """
    if not os.path.exists(full_param_path):
        raise FileNotFoundError(f"Model file {full_param_path} did not exist.")
    checkpoint = torch.load(full_param_path, map_location=get_device())
    model.load_state_dict(checkpoint["model_state_dict"])
    if loss is not None:
        loss.load_state_dict(checkpoint["loss_state_dict"])
    if optim is not None:
        optim.load_state_dict(checkpoint["optimizer_state_dict"])
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/reweighting
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/reweighting
文件名: linear_regression_reweighting.py
--------------------------------------------------------------------------------
import numpy as np
from sklearn.linear_model import SGDClassifier

from sklearn.preprocessing import normalize, StandardScaler

import numpy as np
from itertools import groupby


def _mated_pairs(indices: np.ndarray, labels: np.ndarray) -> np.ndarray:
    sorted_indices = indices[np.argsort(labels)]
    sorted_labels = labels[np.argsort(labels)]
    pairs = []
    for label, group in groupby(zip(sorted_indices, sorted_labels), key=lambda x: x[1]):
        group_indices = [x[0] for x in group]
        for i in range(len(group_indices)):
            for j in range(i + 1, len(group_indices)):
                pairs.append((group_indices[i], group_indices[j]))
    return np.array(pairs)


def _non_mated_pairs(indices: np.ndarray, labels: np.ndarray) -> np.ndarray:
    np.random.seed(50)
    unique_labels = np.unique(labels)
    pairs = []
    for label in unique_labels:
        label_indices = indices[labels == label]
        n = len(label_indices)
        other_indices = indices[labels != label]
        for _ in range(n * (n - 1)):
            i = np.random.choice(label_indices)
            j = np.random.choice(other_indices)
            pairs.append((i, j))
    return np.array(pairs)


def _pairwise_elementwise_product(
    embeddings: np.ndarray, index_pairs: np.ndarray
) -> np.ndarray:
    i_indices = index_pairs[:, 0]
    j_indices = index_pairs[:, 1]
    i_embeddings = embeddings[i_indices]
    j_embeddings = embeddings[j_indices]
    return i_embeddings * j_embeddings


def _pairwise_target_similarity(
    labels: np.ndarray, index_pairs: np.ndarray
) -> np.ndarray:
    i_indices = index_pairs[:, 0]
    j_indices = index_pairs[:, 1]
    i_labels = labels[i_indices]
    j_labels = labels[j_indices]
    return i_labels == j_labels


def _reweight_embedding_dimensions(embeddings: np.ndarray, w: np.ndarray) -> np.ndarray:
    assert w.ndim == 1, "w must be a 1D array"
    assert (
        embeddings.shape[1] == w.shape[0]
    ), "Number of columns in embeddings must match the length of w"

    w = np.maximum(0, w)
    return embeddings * np.sqrt(w)


def _linear_regression(embeddings: np.ndarray, labels: np.ndarray) -> np.ndarray:
    indices = np.array(range(embeddings.shape[0]))
    index_pairs = np.concatenate(
        [_mated_pairs(indices, labels), _non_mated_pairs(indices, labels)], axis=0
    )
    X = _pairwise_elementwise_product(embeddings, index_pairs)
    y = _pairwise_target_similarity(labels, index_pairs)
    clf = SGDClassifier()
    clf.fit(X, y)
    return np.squeeze(clf.coef_)


def reweight_and_normalize_embeddings(
    training_embeddings: np.ndarray,
    embeddings_to_reweight: np.ndarray,
    labels: np.ndarray,
) -> np.ndarray:
    if not isinstance(labels, np.ndarray):
        labels = np.array(labels)

    # Create a StandardScaler object and fit the scaler on the first array
    scaler = StandardScaler()
    scaler.fit(training_embeddings)
    training_embeddings: np.ndarray = scaler.transform(training_embeddings)
    embeddings_to_reweight: np.ndarray = scaler.transform(embeddings_to_reweight)

    w: np.ndarray = _linear_regression(training_embeddings, labels)
    reweighted_embeddings: np.ndarray = _reweight_embedding_dimensions(
        embeddings_to_reweight, w
    )
    normalized_embeddings: np.ndarray = normalize(reweighted_embeddings, norm="l2")
    return normalized_embeddings
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: generate_benchmarks.py
--------------------------------------------------------------------------------
from typing import Union
from os.path import join
import random
import tqdm
import numpy as np

from flx.setup.paths import (
    get_verification_benchmark_file,
    get_closed_set_benchmark_file,
    get_open_set_benchmark_file,
    BENCHMARKS_DIR,
)


from flx.data.dataset import Identifier
from flx.data.image_loader import MCYTCapacitiveLoader, MCYTOpticalLoader
from flx.benchmarks.biometric_comparison import BiometricComparison
from flx.benchmarks.biometric_search import ExhaustiveSearch
from flx.benchmarks.verification import VerificationBenchmark
from flx.benchmarks.identification import IdentificationBenchmark
from flx.setup.paths import get_fingerprint_dataset_path


def create_verification_benchmark(subjects: list[int], impressions_per_subject: list[int]) -> VerificationBenchmark:
    """
    Compare each mated pair and for each sample do as many non-mated comparisons as there are impressions per sample.
    """
    random.seed(3984)
    comps_mated = []
    for i in subjects:
        for idx, j in enumerate(impressions_per_subject):
            comps_mated += [
                BiometricComparison(Identifier(i, j), Identifier(i, k))
                for k in impressions_per_subject[idx + 1 :]
            ]

    comps_non_mated = []
    the_grid: list[set] = {
        x: set((i, k) for k in impressions_per_subject for i in subjects if i != x)
        for x in subjects
    }
    for impression2 in impressions_per_subject:
        for subject1 in tqdm.tqdm(subjects):
            baseset = the_grid[subject1]
            js = random.sample(list(baseset), len(impressions_per_subject))
            for subject2, impression2 in js:
                comps_non_mated.append(
                    BiometricComparison(
                        Identifier(subject1, impression2),
                        Identifier(subject2, impression2),
                    )
                )
                try:
                    the_grid[subject2].remove((subject1, impression2))
                except:
                    pass
    return VerificationBenchmark(comps_mated + comps_non_mated)


def _make_verification(
    name: str, subjects: list[int], impressions_per_subject: list[int]
):
    print(f"Making verification benchmark: {name}")
    bm = create_verification_benchmark(subjects, impressions_per_subject)
    bm.save(get_verification_benchmark_file(name))
    

def create_identification_benchmark(
    subjects_gallery: list[int],
    subjects_impostor: list[int],
    impressions: list[int],
    gallery_impression_idx: int = 0,
) -> list[ExhaustiveSearch]:
    """
    Makes a gallery containing the first impression [first element of 'impressions', of each of the 'subjects_gallery'.
    For each of the subjects in the gallery and for each except the first impressions one query is added
    For each of the impostor subjects one query is added per impression in range [0, impressions_per_subject - 1]
    """
    random.seed(3984)
    assert len(impressions) > 1
    assert len(subjects_gallery) > 0
    searches = []
    gallery_samples = np.array(
        [Identifier(s, impressions[gallery_impression_idx]) for s in subjects_gallery]
    )
    for s in subjects_gallery:
        searches += [
            ExhaustiveSearch(Identifier(s, i), gallery_samples, True)
            for i in impressions[1:]
        ]
    for s in subjects_impostor:
        searches += [
            ExhaustiveSearch(Identifier(s, i), gallery_samples, False)
            for i in impressions
        ]
    return searches


def _make_identification_closed_set(
    name: str,
    subjects_gallery: list[int],
    impressions: list[int],
) -> IdentificationBenchmark:
    print(f"make closed-set identification: {name}")
    searches = [create_identification_benchmark(
        subjects_gallery, [], impressions, gallery_impression_idx=i
    ) for i in range(len(impressions))]
    bm = IdentificationBenchmark(searches)
    bm.save(get_closed_set_benchmark_file(name))


def _make_identification_open_set(
    name: str, subjects: list[int], impressions: list[int], folds: int = 10
):
    random.shuffle(subjects)

    l = len(subjects) // 10
    sublists = [subjects[i * l : min((i + 1) * l, len(subjects))] for i in range(folds)]

    folds: list[IdentificationBenchmark] = []
    for impostor in tqdm.tqdm(sublists):
        gallery = [
            item for sublist in sublists if sublist != impostor for item in sublist
        ]
        folds.append(create_identification_benchmark(gallery, impostor, impressions))
    bm = IdentificationBenchmark(folds)
    bm.save(get_open_set_benchmark_file(name))


def _make_identification(*args, **kwargs) -> IdentificationBenchmark:
    _make_identification_closed_set(*args, **kwargs)
    _make_identification_open_set(*args, **kwargs)


def make_benchmarks_SFinge():
    # Verification
    _make_verification(
        "SFingev2ValidationSeparateSubjects", list(range(42000, 44000)), list(range(4))
    )
    # Identification
    _make_identification(
        "SFingev2ValidationSeparateSubjects",
        list(range(42000, 44000)),
        list(range(4)),
    )


def _make_verification_FVC2004():
    """
    Expects the file with the FVC2004 comparisons (mated and non-mated) as input.
    """
    print("make_verification_FVC2004")
    comps = []
    with open(join(BENCHMARKS_DIR, "verification", "comparisons_FVC2004.txt")) as file:
        l = 1
        for line in file.readlines():
            l += 1
            a, b = line.split(" ")
            a_sid = int(a.split("_")[0]) - 1
            a_iid = int(a.split("_")[1].split(".")[0]) - 1
            aid = Identifier(subject=a_sid, impression=a_iid)
            b_sid = int(b.split("_")[0]) - 1
            b_iid = int(b.split("_")[1].split(".")[0]) - 1
            bid = Identifier(subject=b_sid, impression=b_iid)
            comps.append(BiometricComparison(aid, bid))

    bm = VerificationBenchmark(comps)
    bm.save(get_verification_benchmark_file("FVC2004_DB1A"))


def make_benchmarks_FVC2004():
    """
    Verification:
        Uses the comparisons published with FVC2004
    Ident. closed set:
        One sample of each subject in gallery. 7 other samples as probes
    Ident. open set:
        One sample from first 50 subjects as gallery and rest as impostor
    """
    _make_verification_FVC2004
    _make_identification("FVC2004_DB1A", list(range(100)), list(range(8)))


def make_benchmarks_NISTSD4():
    """
    Verification:
        Compare each mated pair and for each sample do ten non-mated comparisons randomly.
    Ident. closed set:
       Use first sample of each subject in gallery and second as probe
    Ident. open set:
       Use first sample of first 1000 subjects in gallery and rest as probes
    """
    _make_verification("NIST SD4", range(2000), range(2))
    _make_identification("NIST SD4", list(range(2000)), list(range(2)))


def make_benchmarks_mcyt():
    """
    Uses last 1300 subjects (of total 3300)

    Verification:
        Compare each mated pair and for each sample do ten non-mated comparisons randomly.
    Ident. closed set:
       Use first sample of each subject in gallery and second as probe
    Ident. open set:
       Use first sample of last 250 subjects in gallery and rest as probes
    """
    N_TOTAL = 3300
    N_LAST = 1300
    # Verification
    def get_subjects(bids):
        return sorted(list({id.subject for id in bids}))

    ds_optical = MCYTOpticalLoader(get_fingerprint_dataset_path("mcyt330_optical"))
    ds_capacitive = MCYTCapacitiveLoader(
        get_fingerprint_dataset_path("mcyt330_capacitive")
    )
    optical_subjects = get_subjects(ds_optical.ids)
    capacitive_subjects = get_subjects(ds_capacitive.ids)
    optical_subjects = optical_subjects[N_TOTAL - N_LAST :]
    capacitive_subjects = capacitive_subjects[N_TOTAL - N_LAST :]
    assert len(optical_subjects) == N_LAST
    assert len(capacitive_subjects) == N_LAST
    _make_verification("mcyt330_optical", optical_subjects, list(range(12)))
    _make_verification("mcyt330_capacitive", capacitive_subjects, list(range(12)))
    _make_identification(
        "mcyt330_optical",
        optical_subjects,
        list(range(12)),
    )
    _make_identification(
        "mcyt330_capacitive",
        capacitive_subjects,
        list(range(12)),
    )


def main():
    make_benchmarks_SFinge()
    make_benchmarks_mcyt()


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: make_result_table_ident.py
--------------------------------------------------------------------------------
from flx.benchmarks.verification import VerificationResult
from flx.benchmarks.identification import IdentificationResult
from flx.setup.experiments import (
    get_experiments,
    get_reweighting_experiments,
    Experiment,
    DatasetLoader,
)


def _get_experiments_with_rw(
    testset_keys: list[str], extractor_keys: list[str]
) -> list[Experiment]:
    _, _, experiments = get_experiments(
        testset_keys=testset_keys, extractor_keys=extractor_keys
    )
    rw_experiments = get_reweighting_experiments(experiments)
    return list(experiments.values()) + list(rw_experiments.values())


def _get_rank1_identification_rate(exp: Experiment) -> float:
    result = exp.load_closed_set_benchmark_results()
    all_ranks = result.get_mated_ranks()
    return (sum([1 if r == 1 else 0 for r in all_ranks]) / len(all_ranks)) * 100


def _get_fnir_at_fpir_in_percent(exp: Experiment, fpir: float) -> float:
    result: IdentificationResult = exp.load_open_set_benchmark_results()
    return result.false_negative_identification_rate(fpir=fpir / 100) * 100


def _format_multirow(nrows, val) -> str:
    return "\\multirow{" + str(nrows) + "}{*}{" + str(val) + "}"


def _underline_if(val, cond: bool) -> str:
    fval = f"{val:.2f}"
    if cond:
        return "\\underline{" + fval + "}"
    return fval


def main():
    # Table for different preprocessing methods
    EXTRACTOR_KEYS = [
        "DeepPrint_Tex_512",
        "DeepPrint_Minu_512",
        "DeepPrint_TexMinu_512",
    ]

    LABELS = ["texture branch", "minutia branch", "texture and minutia branch"]
    # Num. Dimensions, Num. Operations, Reweighted, EER (capacitive), EER (optical)

    outstr = ""
    for e, label in zip(EXTRACTOR_KEYS, LABELS):
        optical, optical_rw = _get_experiments_with_rw(["mcyt330_optical"], [e])
        capacitive, capacitive_rw = _get_experiments_with_rw(
            ["mcyt330_capacitive"], [e]
        )

        opt_rank1 = _get_rank1_identification_rate(optical)
        opt_fnir = _get_fnir_at_fpir_in_percent(optical, 0.1)

        cap_rank1 = _get_rank1_identification_rate(capacitive)
        cap_fnir = _get_fnir_at_fpir_in_percent(capacitive, 0.1)

        opt_rw_rank1 = _get_rank1_identification_rate(optical_rw)
        opt_rw_fnir = _get_fnir_at_fpir_in_percent(optical_rw, 0.1)

        cap_rw_rank1 = _get_rank1_identification_rate(capacitive_rw)
        cap_rw_fnir = _get_fnir_at_fpir_in_percent(capacitive_rw, 0.1)

        dims = 512
        outstr += "\\midrule \n"
        outstr += (
            "\\multirow{2}{*}{"
            + str(label)
            + "} & \\multirow{2}{*}{"
            + str(dims)
            + "} & No &"
        )
        outstr += f"{_underline_if(opt_rank1, opt_rank1 > opt_rw_rank1)} \\% & "
        outstr += f"{_underline_if(opt_fnir, opt_fnir < opt_rw_fnir)} \\% & "
        outstr += f"{_underline_if(cap_rank1, cap_rank1 > cap_rw_rank1)} \\% & "
        outstr += f"{_underline_if(cap_fnir, cap_fnir < cap_rw_fnir)} \\% "
        outstr += "\\\\ \n"
        outstr += " & & Yes & "
        outstr += f"{_underline_if(opt_rw_rank1, opt_rank1 < opt_rw_rank1)} \\% & "
        outstr += f"{_underline_if(opt_rw_fnir, opt_fnir > opt_rw_fnir)} \\% & "
        outstr += f"{_underline_if(cap_rw_rank1, cap_rank1 < cap_rw_rank1)} \\% & "
        outstr += f"{_underline_if(cap_rw_fnir, cap_fnir > cap_rw_fnir)} \\% "
        outstr += "\\\\ \n"

    with open("table.tex", "w") as f:
        f.write(outstr)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: make_result_table_ndims.py
--------------------------------------------------------------------------------
from flx.benchmarks.verification import VerificationResult
from flx.benchmarks.identification import IdentificationResult
from flx.setup.experiments import (
    get_experiments,
    get_reweighting_experiments,
    Experiment,
    DatasetLoader,
)

PERC_SYM = ""


def _get_experiments_with_rw(
    testset_keys: list[str], extractor_keys: list[str]
) -> list[Experiment]:
    _, _, experiments = get_experiments(
        testset_keys=testset_keys, extractor_keys=extractor_keys
    )
    rw_experiments = get_reweighting_experiments(experiments)
    return list(experiments.values()) + list(rw_experiments.values())


def _get_fnmr_at_fmr_in_percent(exp: Experiment, fmr: float) -> float:
    result = exp.load_verification_benchmark_results()
    t = result.threshold_for_fmr(fmr / 100)
    return result.false_non_match_rate([t])[0] * 100


def _get_eer_in_percent(exp: Experiment) -> float:
    result = exp.load_verification_benchmark_results()
    return result.get_equal_error_rate() * 100


def _format_multirow(nrows, val) -> str:
    return "\\multirow{" + str(nrows) + "}{*}{" + str(val) + "}"


def _underline_if(val, cond: bool) -> str:
    fval = f"{val:.2f}"
    if cond:
        return "\\underline{" + fval + "}"
    return fval


def _get_eer_percent(exp: Experiment) -> float:
    res = exp.load_verification_benchmark_results()
    return res.get_equal_error_rate() * 100


def _make_section(name: str, nrows: int) -> str:
    outstr = "\n\\midrule\n"
    outstr += "\\multirow{" + str(nrows) + "}{*}{ \\textbf{" + name + " } }\n"
    return outstr


def _make_row(
    dims: int, ops: int, fnmr: float, fnmr_rw: float, eer: float, eer_rw: float
) -> str:
    outstr = f"& {dims} & {ops} & "
    outstr += f"{_underline_if(fnmr, fnmr < fnmr_rw)} {PERC_SYM} & "
    outstr += f"{_underline_if(fnmr_rw, fnmr > fnmr_rw)} {PERC_SYM} & "
    outstr += f"{_underline_if(eer, eer < eer_rw)} {PERC_SYM} & "
    outstr += f"{_underline_if(eer_rw, eer > eer_rw)} {PERC_SYM} "
    outstr += "\\\\ \n"
    return outstr


def main():
    # Table for different preprocessing methods
    EXTRACTOR_KEYS = [
        "DeepPrint_Tex_32",
        "DeepPrint_Tex_64",
        "DeepPrint_Tex_128",
        "DeepPrint_Tex_256",
        "DeepPrint_Tex_512",
        "DeepPrint_Tex_1024",
        "DeepPrint_Tex_2048",
    ]

    # Num. Dimensions, Num. Operations, FNMR@0.1%, FNMR@0.1% (rw), EER, EER (rw)

    outstr_opt = _make_section("Optical Database", len(EXTRACTOR_KEYS))
    outstr_cap = _make_section("Capacitive Database", len(EXTRACTOR_KEYS))
    for e in EXTRACTOR_KEYS:
        optical, optical_rw = _get_experiments_with_rw(["mcyt330_optical"], [e])
        capacitive, capacitive_rw = _get_experiments_with_rw(
            ["mcyt330_capacitive"], [e]
        )

        dims = int(e.split("_")[-1])
        ops = 2 * dims - 1

        opt_fnmr = _get_fnmr_at_fmr_in_percent(optical, 0.1)
        cap_fnmr = _get_fnmr_at_fmr_in_percent(capacitive, 0.1)
        opt_rw_fnmr = _get_fnmr_at_fmr_in_percent(optical_rw, 0.1)
        cap_rw_fnmr = _get_fnmr_at_fmr_in_percent(capacitive_rw, 0.1)

        opt_eer = _get_eer_in_percent(optical)
        cap_eer = _get_eer_in_percent(capacitive)
        opt_rw_eer = _get_eer_in_percent(optical_rw)
        cap_rw_eer = _get_eer_in_percent(capacitive_rw)

        outstr_opt += _make_row(dims, ops, opt_fnmr, opt_rw_fnmr, opt_eer, opt_rw_eer)
        outstr_cap += _make_row(dims, ops, cap_fnmr, cap_rw_fnmr, cap_eer, cap_rw_eer)

    with open("table.tex", "w") as f:
        f.write(outstr_opt + "\n\n" + outstr_cap)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: plot_alignment_experiments.py
--------------------------------------------------------------------------------
from os.path import join

import numpy as np

from flx.benchmarks.verification import VerificationResult
from flx.benchmarks.identification import IdentificationResult
from flx.setup.experiments import (
    get_experiments,
    get_reweighting_experiments,
    Experiment,
    DatasetLoader,
)
from flx.generate_embeddings_alignment import get_alignment_dataset_name
from flx.visualization.plot_heatmap import plot_heatmap
from flx.setup.paths import get_figures_dir

import random


def _get_experiment_with_rw(
    testset_key: str, extractor_key: str, rotation: int, shift: int
) -> list[Experiment]:
    _, _, experiments = get_experiments(
        testset_keys=[testset_key], extractor_keys=[extractor_key]
    )
    rw_experiments = get_reweighting_experiments(experiments)

    experiment: Experiment = list(experiments.values())[0]
    experiment.dataset_name = get_alignment_dataset_name(testset_key, rotation, shift)

    rw_experiment: Experiment = list(rw_experiments.values())[0]
    rw_experiment.dataset_name = get_alignment_dataset_name(
        testset_key, rotation, shift
    )

    return experiment, rw_experiment


def _get_fnmr_at_fmr_in_percent(exp: Experiment, fmr: float) -> float:
    return random.random()
    result = exp.load_verification_benchmark_results()
    t = result.threshold_for_fmr(fmr / 100)
    return result.false_non_match_rate([t])[0] * 100


def _get_eer_in_percent(exp: Experiment) -> float:
    result = exp.load_verification_benchmark_results()
    return result.get_equal_error_rate() * 100


def main():
    # Table for different preprocessing methods
    EXTRACTOR_KEY = "DeepPrint_TexMinu_512"
    DATASET_NAME = "mcyt330_optical"

    ROTATION_MAGNITUDES = [0, 15, 30, 45, 60, 90]
    SHIFT_MAGNITUDES = [10, 20, 30, 40, 60, 80]

    results_mat: np.array = np.zeros((len(ROTATION_MAGNITUDES), len(SHIFT_MAGNITUDES)))
    rw_results_mat: np.array = np.zeros(
        (len(ROTATION_MAGNITUDES), len(SHIFT_MAGNITUDES))
    )
    for i, r in enumerate(ROTATION_MAGNITUDES):
        for j, s in enumerate(SHIFT_MAGNITUDES):
            experiment, rw_experiment = _get_experiment_with_rw(
                DATASET_NAME, EXTRACTOR_KEY, r, s
            )
            results_mat[i, j] = _get_fnmr_at_fmr_in_percent(experiment, 0.1)
            rw_results_mat[i, j] = _get_fnmr_at_fmr_in_percent(rw_experiment, 0.1)

    xlabel = "Max. shift in pixels"
    ylabel = "Max. rotation in degrees"
    ytick_labels = [f"{r}" for r in ROTATION_MAGNITUDES]
    xtick_labels = [f"{s}" for s in SHIFT_MAGNITUDES]

    plotdir = get_figures_dir("alignment", DATASET_NAME)

    for mat, filename in [
        (results_mat, f"{EXTRACTOR_KEY}.png"),
        (rw_results_mat, f"{EXTRACTOR_KEY}_rw.png"),
    ]:
        plot_heatmap(
            mat,
            xtick_labs=xtick_labels,
            ytick_labs=ytick_labels,
            xlabel=xlabel,
            ylabel=ylabel,
            title=None,
            scale_label="FNMR (%) at FMR=0.1%",
            filename=join(plotdir, filename),
        )


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: plot_results.py
--------------------------------------------------------------------------------
from typing import Union
from dataclasses import dataclass
from os.path import join
from enum import Enum

from flx.benchmarks.verification import VerificationResult
from flx.benchmarks.identification import IdentificationResult
from flx.setup.experiments import (
    get_experiments,
    get_reweighting_experiments,
    Experiment,
    DatasetLoader,
)
from flx.setup.paths import get_figures_dir
from flx.visualization.plot_DET_curve import (
    plot_verification_results,
    plot_identification_results,
)
from flx.visualization.plot_ranks import plot_rank_n_identification_rates
from flx.visualization.plot_distribution_scores import plot_similarity_scores_results


class EmbeddingType(Enum):
    Original = 0
    Reweighted = 1
    Both = 2


@dataclass
class PlotConfig:
    figures_subdir: str
    extractor_keys: list[str]
    label: str
    plot_distributions: bool = False
    plot_verification: bool = False
    plot_closed_set: bool = False
    plot_open_set: bool = False
    embedding_types: Union[EmbeddingType, list[EmbeddingType]] = EmbeddingType.Original


PLOT_CONFIGS: list[PlotConfig] = [
    PlotConfig(
        "synthetic_vs_mixed",
        [
            "DeepPrint_Texture_first4000",
            "DeepPrint_Texture_Mixed_contrast",
        ],
        "Purely synthetic training set compared to\ntraining set with real and synthetic data",
        plot_verification=True,
    ),
    PlotConfig(
        "preprocessing",
        [
            "DeepPrint_Texture_Mixed_contrast",
            "DeepPrint_Texture_Mixed_gabor",
            "DeepPrint_Texture_Mixed_gabor3",
        ],
        "Models with different preprocessing approaches",
        plot_verification=True,
    ),
    PlotConfig(
        "reweighting",
        [
            "DeepPrint_Tex_512",
        ],
        "Different variants",
        embedding_types=EmbeddingType.Both,
        plot_verification=True,
        plot_distributions=True,
    ),
    PlotConfig(
        "embedding_sizes",
        [
            "DeepPrint_Tex_32",
            "DeepPrint_Tex_64",
            "DeepPrint_Tex_128",
            "DeepPrint_Tex_256",
            "DeepPrint_Tex_512",
            "DeepPrint_Tex_1024",
            "DeepPrint_Tex_2048",
        ],
        "Different embedding sizes",
        plot_verification=True,
    ),
    PlotConfig(
        "embedding_sizes_rw",
        [
            "DeepPrint_Tex_32",
            "DeepPrint_Tex_64",
            "DeepPrint_Tex_128",
            "DeepPrint_Tex_256",
            "DeepPrint_Tex_512",
            "DeepPrint_Tex_1024",
            "DeepPrint_Tex_2048",
        ],
        "Different embedding sizes (reweighted)",
        embedding_types=EmbeddingType.Reweighted,
        plot_verification=True,
    ),
    PlotConfig(
        "variants",
        [
            "DeepPrint_Tex_512",
            "DeepPrint_Minu_512",
            "DeepPrint_TexMinu_512",
        ],
        "Different variants",
        embedding_types=[
            EmbeddingType.Reweighted,
            EmbeddingType.Original,
            EmbeddingType.Original,
        ],
        plot_verification=True,
        plot_distributions=False,
        plot_open_set=False,
        plot_closed_set=True,
    ),
]
PLOT_CONFIGS = PLOT_CONFIGS[-1:]


TESTSET_KEYS = [
    "mcyt330_optical",
    "mcyt330_capacitive",
]

MAKE_TITLE = False
SKIP_DISTRIBUTIONS = False
SKIP_VERIFICATION = False
SKIP_CLOSED_SET = False
SKIP_OPEN_SET = False


def _plot_results_on_dataset(
    config: PlotConfig, dataset: DatasetLoader, experiments: list[Experiment]
) -> None:
    experiments_ds: list[Experiment] = [
        e for e in experiments if e.dataset_name == dataset.name
    ]
    plotdir = get_figures_dir(config.figures_subdir, dataset.name)
    model_labels = [e.model_label for e in experiments_ds]

    if config.plot_distributions and not SKIP_DISTRIBUTIONS:
        print(f"Plotting distributions on {dataset.name}")
        results: list[VerificationResult] = [
            e.load_verification_benchmark_results() for e in experiments_ds
        ]
        plot_similarity_scores_results(
            paths=[join(plotdir, f"{e.model_name}.png") for e in experiments_ds],
            results=results,
            plot_titles=[
                f"Distribution of mated and non-mated similarities\nfor {m}\non {dataset.label}"
                for m in model_labels
            ]
            if MAKE_TITLE
            else [""] * len(model_labels),
        )

    if config.plot_verification and not SKIP_VERIFICATION:
        print(f"Plotting FMR-FNMR of {dataset.name}")
        results: list[VerificationResult] = [
            e.load_verification_benchmark_results() for e in experiments_ds
        ]
        plot_verification_results(
            join(plotdir, "verification"),
            results=results,
            model_labels=model_labels,
            plot_title=f"Verification performance on {dataset.label}"
            if MAKE_TITLE
            else "",
        )

    if config.plot_closed_set and not SKIP_CLOSED_SET:
        print(f"Plotting rank-N identification rates of {dataset.name}")
        results: list[IdentificationResult] = [
            e.load_closed_set_benchmark_results() for e in experiments_ds
        ]
        plot_rank_n_identification_rates(
            join(plotdir, "identification_closed_set"),
            results=results,
            model_labels=model_labels,
            plot_title=f"Rank-N identification rates on {dataset.label}"
            if MAKE_TITLE
            else "",
        )

    if config.plot_open_set and not SKIP_OPEN_SET:
        print(f"Plotting FPIR-FNIR of {dataset.name}")
        results: list[IdentificationResult] = [
            e.load_open_set_benchmark_results() for e in experiments_ds
        ]
        plot_identification_results(
            join(plotdir, "identification_open_set"),
            results=results,
            model_labels=model_labels,
            plot_title=f"Identification performance on {dataset.label}"
            if MAKE_TITLE
            else "",
        )


def _get_experiments_with_rw(
    testset_keys: list[str],
    extractor_keys: list[str],
    embedding_types: Union[EmbeddingType, list[EmbeddingType]],
):
    if isinstance(embedding_types, EmbeddingType):
        embedding_types = [embedding_types for _ in extractor_keys]
    embedding_types = {k: t for k, t in zip(extractor_keys, embedding_types)}

    testsets, _, experiments = get_experiments(
        testset_keys=testset_keys, extractor_keys=extractor_keys
    )
    orig_experiments = {
        k: e
        for k, e in experiments.items()
        if embedding_types[k[0]] == EmbeddingType.Original
        or embedding_types[k[0]] == EmbeddingType.Both
    }
    rw_experiments = get_reweighting_experiments(
        {
            k: e
            for k, e in experiments.items()
            if embedding_types[k[0]] == EmbeddingType.Reweighted
            or embedding_types[k[0]] == EmbeddingType.Both
        }
    )

    return testsets, list(orig_experiments.values()) + list(rw_experiments.values())


def main():
    for plot_config in PLOT_CONFIGS:
        datasets, experiments_lst = _get_experiments_with_rw(
            TESTSET_KEYS, plot_config.extractor_keys, plot_config.embedding_types
        )

        for dataset in datasets:
            _plot_results_on_dataset(plot_config, dataset, experiments_lst)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: run_extraction.py
--------------------------------------------------------------------------------
from flx.setup.experiments import (
    get_experiments,
    DatasetLoader,
    FixedLengthExtractorLoader,
    Experiment,
)


def generate_embeddings(
    extractor_loader: FixedLengthExtractorLoader,
    dataset_loader: DatasetLoader,
    experiment: Experiment,
):
    extractor = extractor_loader.load()
    texture_embeddings, minutia_embeddings = extractor.extract(dataset_loader.load())
    experiment.save_embeddings(texture_embeddings, minutia_embeddings)


def main() -> None:
    EXTRACTOR_KEYS = ["DeepPrint_Tex_256"]

    TESTSET_KEYS = [
        "mcyt330_optical",
        "mcyt330_capacitive",
    ]

    testsets, extractors, experiments = get_experiments(
        testset_keys=TESTSET_KEYS, extractor_keys=EXTRACTOR_KEYS
    )

    for extractor in extractors:
        for dataset in testsets:
            print(f"\nGenerating embeddings of {dataset.name} with {extractor.name}.")
            generate_embeddings(
                extractor, dataset, experiments[(extractor.name, dataset.name)]
            )


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: run_extraction_with_pose_variation.py
--------------------------------------------------------------------------------
from flx.generate_embeddings import generate_embeddings
from flx.image_processing.augmentation import RandomPoseTransform
from flx.setup.experiments import (
    get_experiments,
    DatasetLoader,
    FixedLengthExtractorLoader,
    Experiment,
)
from copy import deepcopy
from flx.scripts.run_reweighting import reweight_embeddings


def _get_rotation_transforms(
    angle_deviation: int, shift_deviation: int
) -> list[RandomPoseTransform]:
    return RandomPoseTransform(
        angle_min=-angle_deviation,
        angle_max=angle_deviation,
        shift_horizontal_min=-shift_deviation,
        shift_horizontal_max=shift_deviation,
        shift_vertical_min=-shift_deviation,
        shift_vertical_max=shift_deviation,
    )


def get_alignment_dataset_name(
    dataset_name: str, angle_deviation: int, shift_deviation: int
) -> str:
    return f"{dataset_name}_rot_{angle_deviation}_shift_{shift_deviation}"


def main():
    ROTATION_MAGNITUDES = [0, 15, 30, 60, 120]
    SHIFT_MAGNITUDES = [0, 10, 20, 40, 80]

    testsets, extractors, experiments = get_experiments(
        testset_keys=[
            "mcyt330_optical",
            "mcyt330_capacitive",
        ],
        extractor_keys=["DeepPrint_TexMinu_512"],
    )

    new_experiments = {}

    for dataloader in testsets:
        for extractor in extractors:
            for angle_deviation in ROTATION_MAGNITUDES:
                for shift_deviation in SHIFT_MAGNITUDES:
                    print(
                        f"\nGenerating embeddings of {dataloader.name} with {extractor.name}."
                    )
                    # Adjust the dataset loader to include the given level of rotation and translation
                    dataloader_mod: DatasetLoader = deepcopy(dataloader)
                    dataloader_mod.kwargs = {
                        "poses": _get_rotation_transforms(
                            angle_deviation, shift_deviation
                        ),
                        "only_last_n": 1300,
                    }

                    # Create new experiment with modified dataset name
                    experiment_mod: Experiment = experiments[
                        (extractor.name, dataloader.name)
                    ]
                    experiment_mod.dataset_name = get_alignment_dataset_name(
                        dataloader.name, angle_deviation, shift_deviation
                    )
                    new_experiments[
                        (extractor.name, experiment_mod.dataset_name)
                    ] = experiment_mod

                    generate_embeddings(extractor, dataloader_mod, experiment_mod)

    reweight_embeddings(new_experiments)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: run_extractor_training.py
--------------------------------------------------------------------------------
from flx.setup.experiments import TESTSETS, EXTRACTORS

from flx.setup.datasets import get_training_set
from flx.setup.paths import get_fingerprint_dataset_path

# EXTRACTOR = EXTRACTORS["DeepPrint_Tex_1024"]
EXTRACTOR = EXTRACTORS["DeepPrint_Tex_512"]

VALIDATION_SET = TESTSETS["SFingev2ValidationSeparateSubjects"]
NUM_EPOCHS = 75


def main():
    extractor = EXTRACTOR.load()
    output_dir = EXTRACTOR.get_dir()
    fingerprints, minutia_maps, labels = get_training_set(
        get_fingerprint_dataset_path("SFingev2"),
        get_fingerprint_dataset_path("mcyt330_optical"),
        get_fingerprint_dataset_path("mcyt330_capacitive"),
    )

    validation_set = VALIDATION_SET.load()
    validation_benchmark = VALIDATION_SET.load_verification_benchmark()
    extractor.fit(
        fingerprints=fingerprints,
        minutia_maps=minutia_maps,
        labels=labels,
        validation_fingerprints=validation_set,
        validation_benchmark=validation_benchmark,
        num_epochs=NUM_EPOCHS,
        out_dir=output_dir,
    )


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/scripts
文件名: run_reweighting.py
--------------------------------------------------------------------------------
from flx.data.embedding_loader import EmbeddingLoader, Embedding
from flx.setup.experiments import (
    get_experiments,
    get_reweighting_experiments,
    ReweightingExperiment,
    Experiment,
)
from flx.reweighting.linear_regression_reweighting import (
    reweight_and_normalize_embeddings,
)


def reweight_embeddings(experiments: dict[tuple[str, str], Experiment]) -> None:
    rw_experiments: dict[
        tuple[str, str], ReweightingExperiment
    ] = get_reweighting_experiments(experiments)
    for key, exp in experiments.items():
        print(f"\nReweighting embeddings of {exp.dataset_name} with {exp.model_name}.")
        ds_train = exp.load_training_embeddings()
        ds_all = exp.load_embeddings()
        new_embeddings = reweight_and_normalize_embeddings(
            ds_train.numpy(), ds_all.numpy(), [bid.subject for bid in ds_train.ids]
        )

        new_ds = EmbeddingLoader(
            [Embedding(bid, vec) for bid, vec in zip(ds_all.ids, new_embeddings)]
        )
        rw_experiments[key].save_embeddings(new_ds)


def main() -> None:
    EXTRACTOR_KEYS = [
        "DeepPrint_Tex_256",
        "DeepPrint_Tex_512",
        "DeepPrint_NoLoc_Mixed",
    ]

    TESTSET_KEYS = [
        "mcyt330_optical",
        "mcyt330_capacitive",
    ]

    _, _, experiments = get_experiments(
        testset_keys=TESTSET_KEYS, extractor_keys=EXTRACTOR_KEYS
    )
    reweight_embeddings(experiments)


if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: _experiment.py
--------------------------------------------------------------------------------
from typing import Union
from dataclasses import dataclass

import os

from flx.setup.paths import (
    get_verification_benchmark_file,
    get_verification_benchmark_results_file,
    get_closed_set_benchmark_file,
    get_open_set_benchmark_file,
    get_closed_set_benchmark_results_dir,
    get_open_set_benchmark_results_dir,
    get_generated_embeddings_dir,
    get_texture_embedding_dataset_dir,
    get_minutia_embedding_dataset_dir,
    get_reweighted_embedding_dataset_dir,
)

from flx.benchmarks.matchers import CosineSimilarityMatcher
from flx.benchmarks.verification import VerificationBenchmark, VerificationResult
from flx.benchmarks.identification import IdentificationBenchmark, IdentificationResult
from flx.data.embedding_loader import EmbeddingLoader


@dataclass
class Experiment:
    model_name: str
    model_label: str
    dataset_name: str
    dataset_label: str
    reweighting_training_indices: list[int] = None

    @staticmethod
    def _load_embeddings_if_exist(path: str) -> Union[None, EmbeddingLoader]:
        if not os.path.exists(path) or len(os.listdir(path)) == 0:
            return None
        return EmbeddingLoader.load(path)

    def load_training_embeddings(self) -> EmbeddingLoader:
        assert self.reweighting_training_indices is not None
        all_embeddings = self.load_embeddings()
        train_ids = (
            all_embeddings.ids[idx] for idx in self.reweighting_training_indices
        )
        return EmbeddingLoader([all_embeddings.get(bid) for bid in train_ids])

    def load_embeddings(self) -> EmbeddingLoader:
        embeddings_base_dir = get_generated_embeddings_dir(
            self.model_name, self.dataset_name
        )
        tex_embeddings = self._load_embeddings_if_exist(
            get_texture_embedding_dataset_dir(embeddings_base_dir)
        )
        minu_embeddings = self._load_embeddings_if_exist(
            get_minutia_embedding_dataset_dir(embeddings_base_dir)
        )
        return EmbeddingLoader.combine_if_both_exist(tex_embeddings, minu_embeddings)

    def save_embeddings(
        self,
        texture_embeddings: EmbeddingLoader,
        minutia_embeddings: EmbeddingLoader,
    ):
        embeddings_base_dir = get_generated_embeddings_dir(
            self.model_name, self.dataset_name
        )
        if texture_embeddings is not None:
            texture_embeddings.save(
                get_texture_embedding_dataset_dir(embeddings_base_dir)
            )
        if minutia_embeddings is not None:
            minutia_embeddings.save(
                get_minutia_embedding_dataset_dir(embeddings_base_dir)
            )

    def load_verification_benchmark(self) -> VerificationBenchmark:
        return VerificationBenchmark.load(
            get_verification_benchmark_file(self.dataset_name)
        )

    def load_verification_benchmark_results(self) -> VerificationResult:
        try:
            return VerificationResult.load(
                get_verification_benchmark_results_file(
                    self.model_name, self.dataset_name
                )
            )
        except RuntimeError:
            benchmark = self.load_verification_benchmark()
            matcher = CosineSimilarityMatcher(self.load_embeddings())
            results = benchmark.run(matcher, save=True)
            self.save_verification_benchmark_results(results)
            return results

    def save_verification_benchmark_results(self, results: VerificationResult) -> None:
        return results.save(
            get_verification_benchmark_results_file(self.model_name, self.dataset_name)
        )

    def load_closed_set_benchmark(self) -> IdentificationBenchmark:
        return IdentificationBenchmark.load(
            get_closed_set_benchmark_file(self.dataset_name)
        )

    def load_closed_set_benchmark_results(self) -> IdentificationResult:
        try:
            return IdentificationResult.load(
                get_closed_set_benchmark_results_dir(self.model_name, self.dataset_name)
            )
        except RuntimeError:
            benchmark = self.load_closed_set_benchmark()
            matcher = CosineSimilarityMatcher(self.load_embeddings())
            results = benchmark.run(matcher)
            self.save_closed_set_benchmark_results(results)
            return results

    def save_closed_set_benchmark_results(self, results: IdentificationResult) -> None:
        return results.save(
            get_closed_set_benchmark_results_dir(self.model_name, self.dataset_name)
        )

    def load_open_set_benchmark(self) -> IdentificationBenchmark:
        return IdentificationBenchmark.load(
            get_open_set_benchmark_file(self.dataset_name)
        )

    def load_open_set_benchmark_results(self) -> IdentificationResult:
        try:
            return IdentificationResult.load(
                get_open_set_benchmark_results_dir(self.model_name, self.dataset_name)
            )
        except RuntimeError:
            benchmark = self.load_open_set_benchmark()
            matcher = CosineSimilarityMatcher(self.load_embeddings())
            results = benchmark.run(matcher)
            self.save_open_set_benchmark_results(results)
            return results

    def save_open_set_benchmark_results(self, results: IdentificationResult) -> None:
        return results.save(
            get_open_set_benchmark_results_dir(self.model_name, self.dataset_name)
        )


class ReweightingExperiment(Experiment):
    REWEIGHTED_EXTRACTOR_NAME_POSTFIX = "_reweighted"
    REWEIGHTED_EXTRACTOR_LABEL_POSTFIX = " (reweighted)"

    def __init__(self, model_name: str, model_label: str, **kwargs):
        super().__init__(
            model_name=model_name + self.REWEIGHTED_EXTRACTOR_NAME_POSTFIX,
            model_label=model_label + self.REWEIGHTED_EXTRACTOR_LABEL_POSTFIX,
            **kwargs
        )

    def load_embeddings(self) -> EmbeddingLoader:
        embeddings_base_dir = get_generated_embeddings_dir(
            self.model_name[: -len(self.REWEIGHTED_EXTRACTOR_NAME_POSTFIX)],
            self.dataset_name,
        )
        return EmbeddingLoader.load(
            get_reweighted_embedding_dataset_dir(embeddings_base_dir)
        )

    def save_embeddings(
        self,
        reweighted_embeddings: EmbeddingLoader,
    ):
        embeddings_base_dir = get_generated_embeddings_dir(
            self.model_name[: -len(self.REWEIGHTED_EXTRACTOR_NAME_POSTFIX)],
            self.dataset_name,
        )
        reweighted_embeddings.save(
            get_reweighted_embedding_dataset_dir(embeddings_base_dir)
        )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: config.py
--------------------------------------------------------------------------------
INTERACTIVE_VIS = False

LEARNING_RATE = 0.025

# Change this when switching models DeepPrint has 299 while ViT has 224
INPUT_SIZE = 299
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: datasets.py
--------------------------------------------------------------------------------
from typing import Iterable

import torch

from flx.data.transformed_image_loader import TransformedImageLoader

from flx.data.image_helpers import (
    pad_and_resize_to_deepprint_input_size,
)
from flx.image_processing.binarization import (
    LazilyAllocatedBinarizer,
)
from flx.image_processing.augmentation import RandomPoseTransform, RandomQualityTransform
from flx.data.dataset import (
    Identifier,
    IdentifierSet,
    Dataset,
    ConstantDataLoader,
)
from flx.data.image_loader import (
    SFingeLoader,
    FVC2004Loader,
    MCYTCapacitiveLoader,
    MCYTOpticalLoader,
    NistSD4Dataset,
)
from flx.data.label_index import LabelIndex
from flx.data.minutia_map_loader import (
    SFingeMinutiaMapLoader,
    MCYTCapacitiveMinutiaMapLoader,
    MCYTOpticalMinutiaMapLoader,
)


def _make_identifiers(
    subjects: Iterable[int], impressions: Iterable[int]
) -> IdentifierSet:
    return IdentifierSet([Identifier(s, i) for s in subjects for i in impressions])


#  ----------------- TRAINING ----------------------

QUALITY_AUGMENTATION = RandomQualityTransform(
    contrast_min=1.3, contrast_max=2.0, gain_min=0.95, gain_max=1.05
)

POSE_AUGMENTATION = RandomPoseTransform(
    pad=0,
    angle_min=-15,
    angle_max=15,
    shift_horizontal_min=-25,
    shift_horizontal_max=25,
    shift_vertical_min=-25,
    shift_vertical_max=25,
)

SFINGE_BINARIZATION = LazilyAllocatedBinarizer(5.0)
MCYT_CAPACITIVE_BINARIZATION = LazilyAllocatedBinarizer(4.8)
MCYT_OPTICAL_BINARIZATION = LazilyAllocatedBinarizer(3.8)
FVC_BINARIZATION = LazilyAllocatedBinarizer(1.8)
NIST_SD4_BINARIZATION = LazilyAllocatedBinarizer(4.0)


def get_training_set(
    sfinge_dir: str, mcyt_optical_dir: str, mcyt_capacitive_dir: str
) -> Dataset:
    # SFinge images
    NUM_SFINGE_SUBJECTS = 6000
    sfinge_ids = _make_identifiers(range(NUM_SFINGE_SUBJECTS), range(10))
    sfinge_images = TransformedImageLoader(
        images=SFingeLoader(sfinge_dir),
        poses=POSE_AUGMENTATION,
        transforms=[
            QUALITY_AUGMENTATION,
            SFINGE_BINARIZATION,
            pad_and_resize_to_deepprint_input_size,
        ],
    )
    sfinge_images = Dataset(sfinge_images, sfinge_ids)

    # MCYT images
    NUM_MCYT_SUBJECTS = 2000
    mcyt_optical_images = TransformedImageLoader(
        images=MCYTOpticalLoader(mcyt_optical_dir),
        poses=POSE_AUGMENTATION,
        transforms=[QUALITY_AUGMENTATION, MCYT_OPTICAL_BINARIZATION],
    )
    mcyt_optical_images = Dataset(
        mcyt_optical_images,
        mcyt_optical_images.ids.filter_by_index(range(NUM_MCYT_SUBJECTS * 12)),
    )

    mcyt_capacitive_images = TransformedImageLoader(
        images=MCYTCapacitiveLoader(mcyt_capacitive_dir),
        poses=POSE_AUGMENTATION,
        transforms=[QUALITY_AUGMENTATION, MCYT_CAPACITIVE_BINARIZATION],
    )
    mcyt_capacitive_images = Dataset(
        mcyt_capacitive_images,
        mcyt_capacitive_images.ids.filter_by_index(range(NUM_MCYT_SUBJECTS * 12)),
    )

    mcyt_images = Dataset.concatenate(
        mcyt_optical_images, mcyt_capacitive_images, share_subjects=True
    )
    ds = Dataset.concatenate(sfinge_images, mcyt_images, share_subjects=False)

    # Minutia Maps
    sfinge_minumaps = Dataset(SFingeMinutiaMapLoader(sfinge_dir), sfinge_ids)

    # mcyt_optical_minumaps = MCYTOpticalMinutiaMapLoader(mcyt_optical_dir)
    # mcyt_optical_minumaps = Dataset(
    #     mcyt_optical_minumaps, mcyt_optical_minumaps.ids.filter_by_index(range(NUM_MCYT_SUBJECTS * 12))
    # )
    #
    # mcyt_capacitive_minumaps = MCYTCapacitiveMinutiaMapLoader(mcyt_capacitive_dir)
    # mcyt_capacitive_minumaps = Dataset(
    #     mcyt_capacitive_minumaps,
    #     mcyt_capacitive_minumaps.ids.filter_by_index(range(NUM_MCYT_SUBJECTS * 12)),
    # )
    # assert mcyt_capacitive_minumaps.ids == mcyt_capacitive_images.ids
    #
    # ds_mm_mcyt = MergedDataset([ds_mm_optical, ds_mm_capacitive], share_subjects=True)

    mcyt_minumaps = Dataset(ConstantDataLoader(torch.tensor([])), mcyt_images.ids)
    minumaps = Dataset.concatenate(
        sfinge_minumaps, mcyt_minumaps, share_subjects=False
    )

    # Labels
    labels = Dataset(LabelIndex(ds.ids), ds.ids)

    assert ds.num_subjects == NUM_SFINGE_SUBJECTS + NUM_MCYT_SUBJECTS
    assert len(ds) == NUM_SFINGE_SUBJECTS * 10 + NUM_MCYT_SUBJECTS * 12 * 2
    assert ds.ids == minumaps.ids
    return (ds, minumaps, labels)


#  ----------------- TESTING ----------------------


def _make_sfinge_no_background_testing(
    root_dir: str, subjects: Iterable[int], impressions: Iterable[int]
) -> Dataset:
    loader = TransformedImageLoader(
        images=SFingeLoader(root_dir),
        poses=None,
        transforms=[SFINGE_BINARIZATION, pad_and_resize_to_deepprint_input_size],
    )
    return Dataset(loader, _make_identifiers(subjects, impressions))


def get_sfinge_example(root_dir: str) -> Dataset:
    return _make_sfinge_no_background_testing(
        root_dir=root_dir, subjects=range(4), impressions=range(2)
    )


def get_sfinge_validation_separate_subjects(root_dir: str) -> Dataset:
    return _make_sfinge_no_background_testing(
        root_dir=root_dir, subjects=range(42000, 44000), impressions=range(4)
    )


def get_sfinge_test(root_dir: str) -> TransformedImageLoader:
    return _make_sfinge_no_background_testing(
        root_dir=root_dir, subjects=range(1000), impressions=range(4)
    )


def _get_mcyt(
    loader: Dataset,
    poses: RandomPoseTransform = None,
    only_last_n: int = None,
) -> TransformedImageLoader:
    loader = TransformedImageLoader(
        images=loader,
        poses=poses,
        transforms=[MCYT_OPTICAL_BINARIZATION, pad_and_resize_to_deepprint_input_size],
    )

    NUM_MCYT = 3300
    if only_last_n is None:
        only_last_n = NUM_MCYT
    start_index = NUM_MCYT - only_last_n
    dataset = Dataset(
        loader,
        loader.ids.filter_by_index(range(start_index * 12, NUM_MCYT * 12)),
    )
    assert dataset.num_subjects == only_last_n
    assert len(dataset) == only_last_n * 12
    return dataset


def get_mcyt_optical(
    root_dir: str, poses: RandomPoseTransform = None, only_last_n: int = None
) -> TransformedImageLoader:
    ds = MCYTOpticalLoader(root_dir)
    return _get_mcyt(ds, poses, only_last_n)


def get_mcyt_capacitive(
    root_dir: str, poses: RandomPoseTransform = None, only_last_n: int = None
) -> TransformedImageLoader:
    ds = MCYTCapacitiveLoader(root_dir)
    return _get_mcyt(ds, poses, only_last_n)


def get_fvc2004_db1a(root_dir: str) -> TransformedImageLoader:
    NUM_SUBJECTS = 100
    loader = TransformedImageLoader(
        images=FVC2004Loader(root_dir),
        poses=None,
        transforms=[FVC_BINARIZATION],
    )
    assert loader.ids.num_subjects == NUM_SUBJECTS
    assert len(loader.ids) == NUM_SUBJECTS * 8
    return Dataset(loader, loader.ids)


def get_nist_sd4(root_dir: str) -> TransformedImageLoader:
    NUM_SUBJECTS = 2000
    loader = TransformedImageLoader(
        images=NistSD4Dataset(root_dir),
        poses=None,
        transforms=[NIST_SD4_BINARIZATION],
    )
    assert loader.ids.num_subjects == NUM_SUBJECTS
    assert len(loader.ids) == NUM_SUBJECTS * 2
    return Dataset(loader, loader.ids)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: experiments.py
--------------------------------------------------------------------------------
from typing import Callable, Union
from dataclasses import dataclass

import os

from flx.setup.paths import (
    get_model_dir,
    get_fingerprint_dataset_path,
    get_verification_benchmark_file,
)
from flx.setup._experiment import Experiment, ReweightingExperiment

from flx.benchmarks.verification import VerificationBenchmark
from flx.data.dataset import Dataset
from flx.setup.datasets import (
    get_sfinge_example,
    get_sfinge_validation_separate_subjects,
    get_sfinge_test,
    get_fvc2004_db1a,
    get_mcyt_optical,
    get_mcyt_capacitive,
    get_nist_sd4,
)
from flx.extractor.fixed_length_extractor import (
    DeepPrintExtractor,
    get_DeepPrint_Tex,
    get_DeepPrint_Minu,
    get_DeepPrint_TexMinu,
    get_DeepPrint_LocTex,
    get_DeepPrint_LocMinu,
    get_DeepPrint_LocTexMinu,
)


@dataclass
class FixedLengthExtractorLoader:
    name: str
    label: str
    constructor: Callable[[], DeepPrintExtractor]

    def load(self) -> DeepPrintExtractor:
        extractor = self.constructor()
        model_dir = get_model_dir(self.name)
        extractor.load_best_model(model_dir)
        return extractor

    def get_dir(self) -> str:
        return get_model_dir(self.name)


@dataclass
class DatasetLoader:
    name: str
    label: str
    constructor: Callable[[str], Dataset]

    def load(self) -> Dataset:
        return self.constructor(get_fingerprint_dataset_path(self.name))

    def load_verification_benchmark(self) -> VerificationBenchmark:
        return VerificationBenchmark.load(get_verification_benchmark_file(self.name))


@dataclass
class BenchmarkLoader:
    model_name: str
    model_label: str


EXTRACTORS = {
    loader.name: loader
    for loader in [
        # Different embedding sizes
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_32",
            "DeepPrint (texture branch, 32 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 32),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_64",
            "DeepPrint (texture branch, 64 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 64),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_128",
            "DeepPrint (texture branch, 128 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 128),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_256",
            "DeepPrint (texture branch, 256 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 256),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_512",
            "DeepPrint (texture branch, 512 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 512),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_1024",
            "DeepPrint (texture branch, 1024 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 1024),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_Tex_2048",
            "DeepPrint (texture branch, 2048 dims)",
            lambda: get_DeepPrint_Tex(2000 + 6000, 2048),
        ),
        # Different variants
        FixedLengthExtractorLoader(
            "DeepPrint_Minu_512",
            "DeepPrint (minutia branch, 512 dims)",
            lambda: get_DeepPrint_Minu(2000 + 6000, 512),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_TexMinu_512",
            "DeepPrint (texture and minutia branch, 512 dims)",
            lambda: get_DeepPrint_TexMinu(2000 + 6000, 256),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_LocTex_512",
            "DeepPrint (localization module and texture branch)",
            lambda: get_DeepPrint_LocTex(2000 + 6000, 512),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_LocMinu_512",
            "DeepPrint (localization module and minutia branch)",
            lambda: get_DeepPrint_LocMinu(2000 + 6000, 512),
        ),
        FixedLengthExtractorLoader(
            "DeepPrint_LocTexMinu_512",
            "DeepPrint (full architecture, separate logits for both branches)",
            lambda: get_DeepPrint_LocTexMinu(2000 + 6000, 256),
        ),
    ]
}

TESTSETS = {
    loader.name: loader
    for loader in [
        DatasetLoader(
            "SFingev2Example",
            "SFinge (small example)",
            get_sfinge_example,
        ),
        DatasetLoader(
            "SFingev2ValidationSeparateSubjects",
            "SFinge (separate subjects and samples from the training set\nused for validation)",
            get_sfinge_validation_separate_subjects,
        ),
        DatasetLoader(
            "SFingev2TestNone",
            "SFinge testset (white background)",
            get_sfinge_test,
        ),
        DatasetLoader(
            "SFingev2TestCapacitive",
            "SFinge testset (capactive sensor)",
            get_sfinge_test,
        ),
        DatasetLoader(
            "SFingev2TestOptical",
            "SFinge testset (optical sensor)",
            get_sfinge_test,
        ),
        DatasetLoader(
            "FVC2004_DB1A",
            "FVC2004 DB1 A",
            get_fvc2004_db1a,
        ),
        DatasetLoader(
            "NIST SD4",
            "NIST SD4",
            get_nist_sd4,
        ),
        DatasetLoader(
            "mcyt330_optical",
            "MCYT330 (optical sensor, last 1300 subjects)",
            lambda x: get_mcyt_optical(x, poses=None, only_last_n=1300),
        ),
        DatasetLoader(
            "mcyt330_capacitive",
            "MCYT330 (capacitive sensor, last 1300 subjects)",  
            lambda x: get_mcyt_capacitive(x, poses=None, only_last_n=1300),
        ),
    ]
}

REWEIGHTING_TRAINING_INDICES = {
    "mcyt330_optical": list(range(2000 * 12)),
    "mcyt330_capacitive": list(range(2000 * 12)),
}

def get_experiments(
    extractor_keys: Union[list[str], None] = None,
    testset_keys: Union[list[str], None] = None,
) -> tuple[
    list[DatasetLoader],
    list[FixedLengthExtractorLoader],
    dict[tuple[str, str], Experiment],
]:
    testsets = (
        TESTSETS.values()
        if testset_keys is None
        else [TESTSETS[k] for k in testset_keys]
    )
    extractors = (
        EXTRACTORS.values()
        if extractor_keys is None
        else [EXTRACTORS[k] for k in extractor_keys]
    )
    experiments = {
        (e.name, d.name): Experiment(
            e.name,
            e.label,
            d.name,
            d.label,
            reweighting_training_indices=REWEIGHTING_TRAINING_INDICES.get(d.name),
        )
        for e in extractors
        for d in testsets
    }
    return testsets, extractors, experiments


def get_reweighting_experiments(
    experiments: dict[tuple[str, str], Experiment]
) -> dict[tuple[str, str], ReweightingExperiment]:
    return {
        k: ReweightingExperiment(
            model_name=e.model_name,
            model_label=e.model_label,
            dataset_name=e.dataset_name,
            dataset_label=e.dataset_label,
            reweighting_training_indices=e.reweighting_training_indices,
        )
        for k, e in experiments.items()
    }
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/setup
文件名: paths.py
--------------------------------------------------------------------------------
from os.path import join, dirname, abspath, exists, isdir
from os import makedirs, remove, removedirs
from shutil import rmtree
from typing import Callable

BASE_DIR = dirname(dirname(dirname(abspath(__file__))))
MODELS_DIR = join(BASE_DIR, "models")
REPORTS_DIR = join(BASE_DIR, "reports")
BENCHMARKS_DIR = join(BASE_DIR, "data", "benchmarks")
FINGERPRINTS_DIR = join(BASE_DIR, "data", "fingerprints")
EMBEDDINGS_DIR = join(BASE_DIR, "data", "embeddings")
POSES_DIR = join(BASE_DIR, "data", "poses")
DEBUG_DIR = join(BASE_DIR, "debug")


def created_parent_dir(pathfun: Callable) -> str:
    def make_parent_and_return(*args, **kwargs):
        path = pathfun(*args, **kwargs)
        if not exists(dirname(path)):
            makedirs(dirname(path))
        return path

    return make_parent_and_return


def created_dir(pathfun: Callable) -> str:
    def make_parent_and_return(*args, **kwargs):
        path = pathfun(*args, **kwargs)
        if not exists(path):
            makedirs(path)
        return path

    return make_parent_and_return


def remove_path(path: str) -> None:
    """
    Removes the object, no matter if it is a file or directory. Does nothing if it
    does not exist.
    """
    try:
        remove(path)
        removedirs(dirname(path))  # Remove directory if now empty
        return
    except OSError:
        pass
    try:
        rmtree(path, ignore_errors=True)
        removedirs(dirname(path))  # Remove directory if now empty
        return
    except OSError:
        pass


# Dataset paths
def get_fingerprint_dataset_path(dataset_name: str) -> str:
    return join(FINGERPRINTS_DIR, dataset_name)


@created_dir
def get_model_dir(model_name: str) -> str:
    return join(MODELS_DIR, model_name)


@created_parent_dir
def get_best_model_file(model_dir: str) -> str:
    return join(model_dir, "best_model.pyt")


@created_parent_dir
def get_newest_model_file(model_dir: str) -> str:
    return join(model_dir, "model.pyt")


# Embedding paths
@created_dir
def get_generated_embeddings_dir(model_name: str, dataset_name: str) -> str:
    return join(EMBEDDINGS_DIR, model_name, dataset_name)


@created_dir
def get_texture_embedding_dataset_dir(embedding_base_dir: str) -> str:
    return join(
        embedding_base_dir,
        "texture",
    )


@created_dir
def get_minutia_embedding_dataset_dir(embedding_base_dir: str) -> str:
    return join(
        embedding_base_dir,
        "minutia",
    )


@created_dir
def get_reweighted_embedding_dataset_dir(embedding_base_dir: str) -> str:
    return join(
        embedding_base_dir,
        "reweighted",
    )


# Pose paths
@created_parent_dir
def get_pose_dataset_path(dataset_name: str) -> str:
    return join(POSES_DIR, f"{dataset_name}.json")


# Benchmark paths
@created_parent_dir
def get_verification_benchmark_file(testset_name: str) -> str:
    return join(BENCHMARKS_DIR, "verification", testset_name + ".json")


@created_parent_dir
def get_open_set_benchmark_file(testset_name: str) -> str:
    return join(BENCHMARKS_DIR, "identification_open_set", testset_name + ".json")


@created_parent_dir
def get_closed_set_benchmark_file(testset_name: str) -> str:
    return join(BENCHMARKS_DIR, "identification_closed_set", testset_name + ".json")


# Result and report paths
@created_parent_dir
def get_benchmark_results_dir(model_name: str, testset_name: str) -> str:
    return join(REPORTS_DIR, model_name, testset_name)


@created_parent_dir
def get_verification_benchmark_results_file(model_name: str, testset_name: str) -> str:
    return join(
        get_benchmark_results_dir(model_name, testset_name), "verification.json"
    )


@created_dir
def get_closed_set_benchmark_results_dir(model_name: str, testset_name: str) -> str:
    return join(
        get_benchmark_results_dir(model_name, testset_name),
        "identification_closed_set",
    )


@created_dir
def get_open_set_benchmark_results_dir(model_name: str, testset_name: str) -> str:
    return join(
        get_benchmark_results_dir(model_name, testset_name),
        "identification_open_set",
    )


@created_dir
def get_figures_dir(testset_name: str, *subdir_names: list[str]) -> str:
    return join(REPORTS_DIR, "figures", testset_name, *subdir_names)


@created_parent_dir
def get_debug_file(*args: list[str]) -> str:
    return join(DEBUG_DIR, *args)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization/det_curve_plotting
文件名: DET.py
--------------------------------------------------------------------------------
# -*- coding: utf-8 -*-
#
# This script is based on:
#
# A. Nautsch, D. Meuwly, D. Ramos, J. Lindh, and C. Busch:
# Making Likelihood Ratios digestible for Cross-Application Performance Assessment,
# IEEE Signal Processing Letters, 24(10), pp. 1552-1556, Oct. 2017.
#          see: https://codeocean.com/2017/09/29/verbal-detection-error-tradeoff-lpar-det-rpar/metadata
#          license: HDA-OPEN-RESEARCH
#
# The code (in the source above) is based on the sidekit implementation of the bosaris toolkit
# sidekit: http://www-lium.univ-lemans.fr/sidekit
#          license: LGPL
# bosaris: https://sites.google.com/site/bosaristoolkit
#          license: https://sites.google.com/site/bosaristoolkit/home/License.txt
#
# Copyright 2018 Andreas Nautsch, Hochschule Darmstadt
#           2019 Andreas Nautsch, EURECOM


import numpy
import copy
from scipy.special import erfinv
from collections import namedtuple
import matplotlib.pyplot as mpl
from tikzplotlib import save as tikz_save
import logging


__license__ = "HDA-OPEN-RESEARCH"
__author__ = "Andreas Nautsch"
__copyright__ = "Copyright 2018 Andreas Nautsch, Hochschule Darmstadt"
__maintainer__ = "Andreas Nautsch"
__email__ = "andreas.nautsch@h-da.de"
__status__ = "Production"
__docformat__ = "reStructuredText"
__credits__ = ["Niko Brummer", "Anthony Larcher", "Edward de Villiers"]


# helper functions
# see: sidekit.bosaris.detplot
colorStyle = [
    ((0, 0, 0), "-", 2),  # black
    ((0, 0, 1.0), "--", 2),  # blue
    ((0.8, 0.0, 0.0), "-.", 2),  # red
    ((0, 0.6, 0.0), ":", 2),  # green
    ((0.5, 0.0, 0.5), "-", 2),  # magenta
    ((0.3, 0.3, 0.0), "--", 2),  # orange
    ((0, 0, 0), ":", 2),  # black
    ((0, 0, 1.0), ":", 2),  # blue
    ((0.8, 0.0, 0.0), ":", 2),  # red
    ((0, 0.6, 0.0), "-", 2),  # green
    ((0.5, 0.0, 0.5), "-.", 2),  # magenta
    ((0.3, 0.3, 0.0), "-", 2),  # orange
]

grayStyle = [
    ((0, 0, 0), "-", 2),  # black
    ((0, 0, 0), "--", 2),  # black
    ((0, 0, 0), "-.", 2),  # black
    ((0, 0, 0), ":", 2),  # black
    ((0.3, 0.3, 0.3), "-", 2),  # gray
    ((0.3, 0.3, 0.3), "--", 2),  # gray
    ((0.3, 0.3, 0.3), "-.", 2),  # gray
    ((0.3, 0.3, 0.3), ":", 2),  # gray
    ((0.6, 0.6, 0.6), "-", 2),  # lighter gray
    ((0.6, 0.6, 0.6), "--", 2),  # lighter gray
    ((0.6, 0.6, 0.6), "-.", 2),  # lighter gray
    ((0.6, 0.6, 0.6), ":", 2),  # lighter gray
]

Box = namedtuple("Box", "left right top bottom")


def probit(p):
    # see: sidekit.bosaris.detplot.__probit__
    y = numpy.sqrt(2) * erfinv(2 * p - 1)
    return y


def pavx(y):
    # see: sidekit.bosaris.detplot.pavx; with fixed bugs
    assert y.ndim == 1, "Argument should be a 1-D array"
    assert y.shape[0] > 0, "Input array is empty"
    n = y.shape[0]

    index = numpy.zeros(n, dtype=int)
    length = numpy.zeros(n, dtype=int)

    ghat = numpy.zeros(n)

    ci = 0
    index[ci] = 1
    length[ci] = 1
    ghat[ci] = y[0]

    for j in range(1, n):
        ci += 1
        index[ci] = j + 1
        length[ci] = 1
        ghat[ci] = y[j]
        while (ci >= 1) & (ghat[numpy.max(ci - 1, 0)] >= ghat[ci]):
            nw = length[ci - 1] + length[ci]
            ghat[ci - 1] = ghat[ci - 1] + (length[ci] / nw) * (ghat[ci] - ghat[ci - 1])
            length[ci - 1] = nw
            ci -= 1

    height = copy.deepcopy(ghat[: ci + 1])
    width = copy.deepcopy(length[: ci + 1])

    while n >= 0:
        for j in range(index[ci], n + 1):
            ghat[j - 1] = ghat[ci]
        n = index[ci] - 1
        ci -= 1

    return ghat, width, height


def rocch(tar_scores, nontar_scores, laplace=True):
    # see: sidekit.bosaris.detplot.rocch

    # init PAV isotonic regression
    Nt = tar_scores.shape[0]
    Nn = nontar_scores.shape[0]
    N = Nt + Nn
    scores = numpy.concatenate((tar_scores, nontar_scores))
    Pideal = numpy.concatenate((numpy.ones(Nt), numpy.zeros(Nn)))
    perturb = numpy.argsort(scores, kind="mergesort")
    Pideal = Pideal[perturb]
    if laplace:
        Pideal = numpy.concatenate(([1, 0], Pideal, [1, 0]))

    Popt, width, foo = pavx(Pideal)

    if laplace:
        Popt = Popt[2:-2]

    # ROCCH points
    nbins = width.shape[0]
    pmiss = numpy.zeros(nbins + 1)
    pfa = numpy.zeros(nbins + 1)
    left = 0
    fa = Nn
    miss = 0
    for i in range(nbins):
        pmiss[i] = miss / Nt
        pfa[i] = fa / Nn
        left = int(left + width[i])
        miss = numpy.sum(Pideal[:left])
        fa = N - left - numpy.sum(Pideal[left:])
    pmiss[nbins] = miss / Nt
    pfa[nbins] = fa / Nn

    return pmiss, pfa


def rocch_tradeoff(
    tar, non, pfa_min=5e-4, pfa_max=0.5, pmiss_min=5e-4, pmiss_max=0.5, dps=100
):
    # see: sidekit.bosaris.detplot.rocchdet
    assert (
        (pfa_min > 0) & (pfa_max < 1) & (pmiss_min > 0) & (pmiss_max < 1)
    ), "limits must be strictly inside (0,1)"
    assert (pfa_min < pfa_max) & (
        pmiss_min < pmiss_max
    ), "pfa and pmiss min and max values are not consistent"

    pmiss, pfa = rocch(tar, non)

    x = []
    y = []
    box = Box(left=pfa_min, right=pfa_max, top=pmiss_max, bottom=pmiss_min)
    for i in range(pfa.shape[0] - 1):
        xx = pfa[i : i + 2]
        yy = pmiss[i : i + 2]
        xdots, ydots = plotseg(xx, yy, box, dps)
        x = x + xdots.tolist()
        y = y + ydots.tolist()

    return numpy.array(x), numpy.array(y)


def __DETsort__(x, col=""):
    # see: sidekit.bosaris.detplot.__DETsort__
    assert x.ndim > 1, "x must be a 2D matrix"
    if col == "":
        list(range(1, x.shape[1]))

    ndx = numpy.arange(x.shape[0])

    # sort 2nd column ascending
    ind = numpy.argsort(x[:, 1], kind="mergesort")
    ndx = ndx[ind]

    # reverse to descending order
    ndx = ndx[::-1]

    # now sort first column ascending
    ind = numpy.argsort(x[ndx, 0], kind="mergesort")

    ndx = ndx[ind]
    sort_scores = x[ndx, :]
    return sort_scores


def __compute_roc__(true_scores, false_scores):
    # see: sidekit.bosaris.detplot.__compute_roc__
    num_true = true_scores.shape[0]
    num_false = false_scores.shape[0]
    assert num_true > 0, "Vector of target scores is empty"
    assert num_false > 0, "Vector of nontarget scores is empty"

    total = num_true + num_false

    Pmiss = numpy.zeros((total + 1))
    Pfa = numpy.zeros((total + 1))

    scores = numpy.zeros((total, 2))
    scores[:num_false, 0] = false_scores
    scores[:num_false, 1] = 0
    scores[num_false:, 0] = true_scores
    scores[num_false:, 1] = 1

    scores = __DETsort__(scores)

    sumtrue = numpy.cumsum(scores[:, 1], axis=0)
    sumfalse = num_false - (numpy.arange(1, total + 1) - sumtrue)

    Pmiss[0] = 0
    Pfa[0] = 1
    Pmiss[1:] = sumtrue / num_true
    Pfa[1:] = sumfalse / num_false
    return Pfa, Pmiss


def __filter_roc__(pfa, pm):
    # see: sidekit.bosaris.detplot.__filter_roc__
    out = 0
    new_pm = [pm[0]]
    new_pfa = [pfa[0]]

    for i in range(1, pm.shape[0]):
        if (pm[i] == new_pm[out]) | (pfa[i] == new_pfa[out]):
            pass
        else:
            # save previous point, because it is the last point before the
            # change.  On the next iteration, the current point will be saved.
            out += 1
            new_pm.append(pm[i - 1])
            new_pfa.append(pfa[i - 1])

    out += 1
    new_pm.append(pm[-1])
    new_pfa.append(pfa[-1])
    pm = numpy.array(new_pm)
    pfa = numpy.array(new_pfa)
    return pfa, pm


def plotseg(xx, yy, box, dps):
    # see: sidekit.bosaris.detplot.plotseg
    assert (xx[1] <= xx[0]) & (yy[0] <= yy[1]), "xx and yy should be sorted"

    XY = numpy.column_stack((xx, yy))
    dd = numpy.dot(numpy.array([1, -1]), XY)
    if numpy.min(abs(dd)) != 0:
        seg = numpy.linalg.solve(XY, numpy.array([[1], [1]]))

    # segment completely outside of box
    if (
        (xx[0] < box.left)
        | (xx[1] > box.right)
        | (yy[1] < box.bottom)
        | (yy[0] > box.top)
    ):
        xdots = numpy.array([])
        ydots = numpy.array([])
    else:
        if xx[1] < box.left:
            xx[1] = box.left
            yy[1] = (1 - seg[0] * box.left) / seg[1]

        if xx[0] > box.right:
            xx[0] = box.right
            yy[0] = (1 - seg[0] * box.right) / seg[1]

        if yy[0] < box.bottom:
            yy[0] = box.bottom
            xx[0] = (1 - seg[1] * box.bottom) / seg[0]

        if yy[1] > box.top:
            yy[1] = box.top
            xx[1] = (1 - seg[1] * box.top) / seg[0]

        dx = xx[1] - xx[0]
        xdots = xx[0] + dx * numpy.arange(dps + 1) / dps
        ydots = (1 - seg[0] * xdots) / seg[1]

    return xdots, ydots


def clean_segment(xseg, yseg, minimum_point_distance=0.01):
    # see: https://codeocean.com/algorithm/154591c8-9d3f-47eb-b656-3aff245fd5c1/code
    # motivated by matlab2tikz
    keep_idx = numpy.isfinite(xseg) & numpy.isfinite(yseg)
    xseg = xseg[keep_idx]
    yseg = yseg[keep_idx]

    # remove all points within Euclidean range of minimum_point_distance
    xdist = numpy.diff(xseg)
    ydist = numpy.diff(yseg)
    dist = numpy.sqrt(xdist**2 + ydist**2)
    idx_keep = numpy.concatenate(([True], dist >= minimum_point_distance))

    # find first omitted, and check on which idx is in valid range again from there
    first_pop_idx = numpy.where(idx_keep == False)[0]
    while len(first_pop_idx) > 0:
        xdist = xseg[first_pop_idx[0] - 1] - xseg[first_pop_idx]
        ydist = yseg[first_pop_idx[0] - 1] - yseg[first_pop_idx]
        dist = numpy.sqrt(xdist**2 + ydist**2)
        tmp_keep = numpy.where(dist >= minimum_point_distance)[0]
        if len(tmp_keep) > 0:
            idx_to_keep = first_pop_idx[tmp_keep[0]]
            idx_keep[idx_to_keep] = True
            first_pop_idx = first_pop_idx[numpy.where(first_pop_idx > idx_to_keep)[0]]
        else:
            first_pop_idx = []

    if len(xseg) > 0:
        xseg = xseg[idx_keep]
        yseg = yseg[idx_keep]

    return xseg, yseg


class DET:
    """
    Class for creating DET plots
    see: A. Martin, G. Doddington, T. Kamm, M. Ordowski, M. Przybocki:
         "The DET Curve in Assessment of Detection Task Performance",
         Proc. EUROSPEECH, pp. 1895-1898, 1997
    """

    def __init__(
        self,
        biometric_evaluation_type=None,
        abbreviate_axes=False,
        plot_title=None,
        plot_eer_line=False,
        plot_rule_of_30=False,
        cleanup_segments_distance=0.01,
    ):
        self.num_systems = 0
        self.system_labels = []
        self.axes_transform = probit
        self.plot_eer_line = plot_eer_line
        self.plot_rule_of_30 = plot_rule_of_30
        self.cleanup_segments_distance = cleanup_segments_distance

        self.plot_title = plot_title
        self.x_limits = numpy.array([1e-8, 0.5])
        self.y_limits = numpy.array([1e-8, 0.5])
        self.x_ticks = numpy.array(
            [
                1e-7,
                1e-6,
                1e-5,
                1e-4,
                1e-3,
                1e-2,
                5e-2,
                20e-2,
                40e-2,
                65e-2,
                85e-2,
                95e-2,
            ]
        )
        self.x_ticklabels = numpy.array(
            [
                "0.00001",
                "0.0001",
                "0.001",
                "0.01",
                "0.1",
                "1",
                "5",
                "20",
                "40",
                "65",
                "85",
                "95",
            ]
        )
        self.y_ticks = numpy.array(
            [
                1e-7,
                1e-6,
                1e-5,
                1e-4,
                1e-3,
                1e-2,
                5e-2,
                20e-2,
                40e-2,
                65e-2,
                85e-2,
                95e-2,
            ]
        )
        self.y_ticklabels = numpy.array(
            [
                "0.00001",
                "0.0001",
                "0.001",
                "0.01",
                "0.1",
                "1",
                "5",
                "20",
                "40",
                "65",
                "85",
                "95",
            ]
        )

        if biometric_evaluation_type == "algorithm":
            if abbreviate_axes:
                self.x_label = "FMR (in %)"
                self.y_label = "FNMR (in %)"
            else:
                self.x_label = "False Match Rate (in %)"
                self.y_label = "False Non-Match Rate (in %)"
        elif biometric_evaluation_type == "system":
            if abbreviate_axes:
                self.x_label = "FAR (in %)"
                self.y_label = "FRR (in %)"
            else:
                self.x_label = "False Acceptance Rate (in %)"
                self.y_label = "False Rejection Rate (in %)"
        elif biometric_evaluation_type == "PAD":
            if abbreviate_axes:
                self.x_label = "APCER (in %)"
                self.y_label = "BPCER (in %)"
            else:
                self.x_label = "Attack Presentation Classification Error Rate (in %)"
                self.y_label = "Bona Fide Presentation Classification Error Rate (in %)"
        elif biometric_evaluation_type == "identification":
            if abbreviate_axes:
                self.x_label = "FPIR (in %)"
                self.y_label = "FNIR (in %)"
            else:
                self.x_label = "False Positive Identification Rate (in %)"
                self.y_label = "False Negative Identification Rate (in %)"
        else:
            self.x_label = "Type I Error Rate (in %)"
            self.y_label = "Type II Error Rate (in %)"
            self.x_limits = numpy.array([1e-8, 0.99])
            self.y_limits = numpy.array([1e-8, 0.99])

    def create_figure(self):
        """
        Creates empty DET plot figure
        """
        self.figure = mpl.figure()
        ax = self.figure.add_subplot(111)
        ax.set_aspect("equal")

        mpl.axis(
            [
                self.axes_transform(self.x_limits[0]),
                self.axes_transform(self.x_limits[1]),
                self.axes_transform(self.y_limits[0]),
                self.axes_transform(self.y_limits[1]),
            ]
        )

        ax.set_xticks(self.axes_transform(self.x_ticks))
        ax.set_xticklabels(self.x_ticklabels, size="x-small")

        ax.set_yticks(self.axes_transform(self.y_ticks))
        ax.set_yticklabels(self.y_ticklabels, size="x-small")

        mpl.grid(True)  # grid_color = '#b0b0b0'
        mpl.xlabel(self.x_label)
        mpl.ylabel(self.y_label)
        if self.plot_title is not None:
            mpl.title(self.plot_title)

        mpl.gca().set_xlim(
            left=self.axes_transform(self.x_limits[0]),
            right=self.axes_transform(self.x_limits[1]),
        )
        mpl.gca().set_ylim(
            bottom=self.axes_transform(self.y_limits[0]),
            top=self.axes_transform(self.y_limits[1]),
        )
        mpl.gca().set_aspect("equal")
        if self.plot_eer_line:
            eer_line = numpy.linspace(
                min(
                    self.axes_transform(self.x_limits[0]),
                    self.axes_transform(self.y_limits[0]),
                ),
                max(
                    self.axes_transform(self.x_limits[1]),
                    self.axes_transform(self.y_limits[1]),
                ),
                1000,
            )
            mpl.plot(
                eer_line,
                eer_line,
                color="#b0b0b0",
                linestyle="-",
                linewidth=0.6,
                label=None,
            )

    def plot(
        self,
        tar,
        non,
        label="",
        style="color",
        plot_args="",
        dissimilarity_scores=False,
        plot_rocch=False,
    ):
        if dissimilarity_scores:
            tar = -numpy.array(tar)
            non = -numpy.array(non)
        if not (isinstance(plot_args, tuple) & (len(plot_args) == 3)):
            if style == "gray":
                plot_args = grayStyle[self.num_systems]
            else:
                plot_args = colorStyle[self.num_systems]
        if plot_rocch:
            x, y = rocch_tradeoff(
                tar,
                non,
                self.x_limits[0],
                self.x_limits[1],
                self.y_limits[0],
                self.y_limits[1],
            )
        else:
            # steppy ROC
            x, y = __compute_roc__(tar, non)
            x, y = __filter_roc__(x, y)

        # transform to DET scale
        xseg = self.axes_transform(x)
        yseg = self.axes_transform(y)

        if self.cleanup_segments_distance:
            xseg, yseg = clean_segment(
                xseg, yseg, minimum_point_distance=self.cleanup_segments_distance
            )

        mpl.plot(
            xseg,
            yseg,
            label=label,
            color=plot_args[0],
            linestyle=plot_args[1],
            linewidth=plot_args[2],
        )

        if self.plot_rule_of_30:
            self.__plot_x_rule_of_30__(num_scores=non.shape[0])
            self.__plot_y_rule_of_30__(num_scores=tar.shape[0])

        self.num_systems += 1
        self.system_labels.append(label)

    def show(self):
        """
        onl
        """
        mpl.show()

    def __plot_x_rule_of_30__(
        self, num_scores, plot_args=((0, 0, 0), "--", 1), legend_string=""
    ):
        # see: https://codeocean.com/algorithm/154591c8-9d3f-47eb-b656-3aff245fd5c1/code
        # see: sidekit.bosaris.detplot.DETplot

        pfaval = 30.0 / num_scores

        if not (pfaval < self.y_limits[0]) | (pfaval > self.y_limits[1]):
            drx = self.axes_transform(pfaval)
            mpl.vlines(
                drx,
                ymin=self.axes_transform(self.y_limits[0]),
                ymax=self.axes_transform(self.y_limits[1]),
                color=plot_args[0],
                linestyle=plot_args[1],
                linewidth=plot_args[2],
                label=None,
            )

    def __plot_y_rule_of_30__(
        self, num_scores, plot_args=((0, 0, 0), "--", 1), legend_string=""
    ):
        # see: https://codeocean.com/algorithm/154591c8-9d3f-47eb-b656-3aff245fd5c1/code
        # see: sidekit.bosaris.detplot.DETplot

        pmissval = 30.0 / num_scores

        if not (pmissval < self.x_limits[0]) | (pmissval > self.x_limits[1]):
            dry = self.axes_transform(pmissval)
            mpl.hlines(
                y=dry,
                xmin=self.axes_transform(self.x_limits[0]),
                xmax=self.axes_transform(self.x_limits[1]),
                color=plot_args[0],
                linestyle=plot_args[1],
                linewidth=plot_args[2],
                label=None,
            )

    def legend(self, enable, **kwargs):
        if enable:
            self.legend_on(**kwargs)
        else:
            self.legend_off()

    def legend_on(self, **kwargs):
        kwargs.setdefault("loc", 0)
        mpl.legend(**kwargs)

    def legend_off(self):
        mpl.legend().remove()

    def save(self, filename, type="tikz", dpi=None, width="120pt", height="120pt"):
        if type in ["pdf", "png"]:
            mpl.savefig(filename + "." + type, bbox_inches="tight")
        elif type in ["tikz", "tex", "latex", "LaTeX"]:
            self.__save_as_tikzpgf__(
                outfilename=filename + ".tex", dpi=dpi, width=width, height=height
            )
        else:
            raise ValueError("unknown save format")

    def __save_as_tikzpgf__(
        self,
        outfilename,
        dpi=None,
        width="140pt",
        height="140pt",
        extra_axis_parameters=["xticklabel style={rotate=90}"],
        extra_tikzpicture_parameters=["[font=\\scriptsize]"],
    ):
        # see: https://codeocean.com/algorithm/154591c8-9d3f-47eb-b656-3aff245fd5c1/code

        """
        def replace_tick_label_notation(tick_textpos):
            tick_label = tick_textpos.get_text()
            if 'e' in tick_label:
                tick_label = int(tick_label.replace('1e', '')) - 2
                tick_textpos.set_text('%f' % (10 ** (int(tick_label) - 2)))
        """

        mpl.xlabel(mpl.axes().get_xlabel().replace("%", "\%"))
        mpl.ylabel(mpl.axes().get_ylabel().replace("%", "\%"))

        if dpi is not None:
            mpl.gcf().set_dpi(dpi)

        mpl.gca().set_title("")

        """
        for tick_textpos in mpl.gca().get_xmajorticklabels():
            replace_tick_label_notation(tick_textpos)
        for tick_textpos in mpl.gca().get_ymajorticklabels():
            replace_tick_label_notation(tick_textpos)
        """

        tikz_save(
            outfilename,
            figurewidth=width,
            figureheight=height,
            extra_axis_parameters=extra_axis_parameters,
            extra_tikzpicture_parameters=extra_tikzpicture_parameters,
        )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization/det_curve_plotting
文件名: __init__.py
--------------------------------------------------------------------------------

================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: plot_DET_curve.py
--------------------------------------------------------------------------------
import numpy as np

from flx.visualization.det_curve_plotting.DET import DET
from flx.benchmarks.verification import VerificationResult
from flx.benchmarks.identification import IdentificationResult

BBOX_TO_ANCHOR = (1.05, -0.15)


def plot_verification_results(
    path: str,
    results: list[VerificationResult],
    model_labels: list[str],
    plot_title: str,
) -> None:
    det = DET(
        biometric_evaluation_type="algorithm",
        plot_title=plot_title,
        plot_eer_line=True,
    )

    # Customize axes
    det.x_limits = np.array([1e-5, 0.5])
    det.y_limits = np.array([1e-5, 0.5])
    det.x_ticks = np.array([1e-4, 1e-3, 1e-2, 5e-2, 20e-2, 40e-2])
    det.x_ticklabels = np.array(["0.01", "0.1", "1", "5", "20", "40"])
    det.y_ticks = np.array([1e-4, 1e-3, 1e-2, 5e-2, 20e-2, 40e-2])
    det.y_ticklabels = np.array(["0.01", "0.1", "1", "5", "20", "40"])

    # Plot
    det.create_figure()
    for model_label, res in zip(model_labels, results):
        det.plot(
            tar=np.array(res.get_mated_scores()),
            non=np.array(res.get_non_mated_scores()),
            label=model_label,
        )
    det.legend_on(bbox_to_anchor=BBOX_TO_ANCHOR)
    det.save(path, "png")
    det.save(path, "pdf")


def plot_identification_results(
    path: str,
    results: list[IdentificationResult],
    model_labels: list[str],
    plot_title: str,
) -> None:
    det = DET(
        biometric_evaluation_type="identification",
        plot_title=plot_title,
    )

    # Customize axes
    det.x_limits = np.array([1e-5, 0.5])
    det.y_limits = np.array([1e-5, 0.5])
    det.x_ticks = np.array([1e-4, 1e-3, 1e-2, 5e-2, 20e-2, 40e-2])
    det.x_ticklabels = np.array(["0.01", "0.1", "1", "5", "20", "40"])
    det.y_ticks = np.array([1e-4, 1e-3, 1e-2, 5e-2, 20e-2, 40e-2])
    det.y_ticklabels = np.array(["0.01", "0.1", "1", "5", "20", "40"])

    # Plot
    det.create_figure()
    for model_label, res in zip(model_labels, results):
        det.plot(
            tar=np.array(res.get_mated_similarities()),
            non=np.array(res.get_highest_non_mated_similarities()),
            label=model_label,
        )
    det.legend_on(bbox_to_anchor=BBOX_TO_ANCHOR)
    det.save(path, "png")
    det.save(path, "pdf")
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: plot_distribution_scores.py
--------------------------------------------------------------------------------
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np

from flx.benchmarks.verification import VerificationResult


def _plot_kde(
    distributions: list[np.ndarray], labels: list[str], title: str, filename: str
) -> None:
    plt.close()
    fig = plt.figure()
    ax = fig.add_subplot()
    for dis, l in zip(distributions, labels):
        sns.histplot(dis, ax=ax, label=f"{l} (n = {dis.shape[0]})")
    if title is not None:
        plt.title(title)
    fig.tight_layout()
    plt.legend()
    plt.savefig(filename)
    plt.close()


def plot_similarity_scores_results(
    paths: str, results: list[VerificationResult], plot_titles: list[str]
):
    assert len(paths) == len(results)
    assert len(paths) == len(plot_titles)
    for path, res, title in zip(paths, results, plot_titles):
        _plot_kde(
            distributions=[res.get_mated_scores(), res.get_non_mated_scores()],
            labels=["Genuine", "Impostor"],
            title=title,
            filename=path,
        )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: plot_heatmap.py
--------------------------------------------------------------------------------
import numpy as np
import matplotlib
import matplotlib as mpl
import matplotlib.pyplot as plt


def _heatmap(
    data, row_labels, col_labels, ax=None, cbar_kw=None, cbarlabel="", **kwargs
):
    """
    Create a heatmap from a numpy array and two lists of labels.

    Parameters
    ----------
    data
        A 2D numpy array of shape (M, N).
    row_labels
        A list or array of length M with the labels for the rows.
    col_labels
        A list or array of length N with the labels for the columns.
    ax
        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If
        not provided, use current axes or create a new one.  Optional.
    cbar_kw
        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.
    cbarlabel
        The label for the colorbar.  Optional.
    **kwargs
        All other arguments are forwarded to `imshow`.
    """

    if ax is None:
        ax = plt.gca()

    if cbar_kw is None:
        cbar_kw = {}

    # Plot the heatmap
    im = ax.imshow(data, **kwargs)

    # Create colorbar
    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")

    # Show all ticks and label them with the respective list entries.
    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)
    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)

    # Let the horizontal axes labeling appear on top.
    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=-30, ha="right", rotation_mode="anchor")

    # Turn spines off and create white grid.
    ax.spines[:].set_visible(False)

    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)
    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)
    ax.grid(which="minor", color="w", linestyle="-", linewidth=3)
    ax.tick_params(which="minor", bottom=False, left=False)

    return im, cbar


def _annotate_heatmap(
    im,
    data=None,
    valfmt="{x:.2f}",
    textcolors=("black", "white"),
    threshold=None,
    **textkw
):
    """
    A function to annotate a heatmap.

    Parameters
    ----------
    im
        The AxesImage to be labeled.
    data
        Data used to annotate.  If None, the image's data is used.  Optional.
    valfmt
        The format of the annotations inside the heatmap.  This should either
        use the string format method, e.g. "$ {x:.2f}", or be a
        `matplotlib.ticker.Formatter`.  Optional.
    textcolors
        A pair of colors.  The first is used for values below a threshold,
        the second for those above.  Optional.
    threshold
        Value in data units according to which the colors from textcolors are
        applied.  If None (the default) uses the middle of the colormap as
        separation.  Optional.
    **kwargs
        All other arguments are forwarded to each call to `text` used to create
        the text labels.
    """

    if not isinstance(data, (list, np.ndarray)):
        data = im.get_array()

    # Normalize the threshold to the images color range.
    if threshold is not None:
        threshold = im.norm(threshold)
    else:
        threshold = im.norm(data.max()) / 2.0

    # Set default alignment to center, but allow it to be
    # overwritten by textkw.
    kw = dict(horizontalalignment="center", verticalalignment="center")
    kw.update(textkw)

    # Get the formatter in case a string is supplied
    if isinstance(valfmt, str):
        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)

    # Loop over the data and create a `Text` for each "pixel".
    # Change the text's color depending on the data.
    texts = []
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])
            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
            texts.append(text)

    return texts


def plot_heatmap(
    data: list[list[float]],
    xtick_labs: list[str],
    ytick_labs: list[str],
    xlabel: str,
    ylabel: str,
    title: str,
    scale_label: str,
    filename: str,
) -> None:
    fig, ax = plt.subplots()

    im, cbar = _heatmap(
        data, xtick_labs, ytick_labs, ax=ax, cmap="Oranges", cbarlabel=scale_label
    )
    texts = _annotate_heatmap(im, valfmt="{x:.1f}")

    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    if title is not None:
        plt.title(title)
    fig.tight_layout()
    plt.savefig(filename, bbox_inches="tight")


if __name__ == "__main__":
    vegetables = [
        "cucumber",
        "tomato",
        "lettuce",
        "asparagus",
        "potato",
        "wheat",
        "barley",
    ]
    farmers = [
        "Farmer Joe",
        "Upland Bros.",
        "Smith Gardening",
        "Agrifun",
        "Organiculture",
        "BioGoods Ltd.",
        "Cornylee Corp.",
    ]

    harvest = np.array(
        [
            [0.8, 2.4, 2.5, 3.9, 0.0, 4.0, 0.0],
            [2.4, 0.0, 4.0, 1.0, 2.7, 0.0, 0.0],
            [1.1, 2.4, 0.8, 4.3, 1.9, 4.4, 0.0],
            [0.6, 0.0, 0.3, 0.0, 3.1, 0.0, 0.0],
            [0.7, 1.7, 0.6, 2.6, 2.2, 6.2, 0.0],
            [1.3, 1.2, 0.0, 0.0, 0.0, 3.2, 5.1],
            [0.1, 2.0, 0.0, 1.4, 0.0, 1.9, 6.3],
        ]
    )
    plot_heatmap(
        harvest,
        vegetables,
        farmers,
        "Vegetables",
        "Farmers",
        "Harvest of local farmers (in tons/year)",
        "harvest t/year",
        "test.png",
    )
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: plot_minutiae.py
--------------------------------------------------------------------------------
import cv2
import numpy as np
import torch
from typing import List, Tuple

from flx.visualization.show_with_opencv import (
    _tensor_to_2Dnumpy_array,
    _normalized_array_to_grayscale,
)

# Define the constants for the colors and circle/line parameters
MINUTIA_DOT_COLOR = (255, 0, 0)  # Blue in BGR format
MINUTIA_DOT_RADIUS = 5  # Pixels
LINE_COLOR = (0, 255, 0)  # Green in BGR format
LINE_THICKNESS = 2  # Pixels
LINE_LENGTH = 8  # Pixels


def _plot_points_and_lines(
    img: cv2.Mat, coords: list[list[int]], angles: list[float], out_path: str
) -> None:
    # Read the image from the filepath

    # Loop through the coordinates and angles arrays
    for i in range(len(coords)):
        # Get the current point and angle
        x, y = coords[i]
        angle = angles[i]

        # Draw a circle at the point with the defined color and radius
        cv2.circle(img, (x, y), MINUTIA_DOT_RADIUS, MINUTIA_DOT_COLOR, -1)

        # Calculate the end point of the line using trigonometry
        x2 = int(x + LINE_LENGTH * np.cos(angle))
        y2 = int(y + LINE_LENGTH * np.sin(angle))

        # Draw a line from the point to the end point with the defined color and thickness
        cv2.line(img, (x, y), (x2, y2), LINE_COLOR, LINE_THICKNESS)

    # Save the updated image to the output filepath
    cv2.imwrite(out_path, img)


def plot_minutiae(
    out_path: str, image: cv2.Mat, locs: np.ndarray, oris: np.ndarray
) -> None:
    if isinstance(image, torch.Tensor):
        image = _normalized_array_to_grayscale(_tensor_to_2Dnumpy_array(image))

    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    locs = locs.astype(int)
    _plot_points_and_lines(image, locs, oris, out_path)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: plot_ranks.py
--------------------------------------------------------------------------------
import tqdm
import numpy as np
from matplotlib import pyplot as plt
import seaborn

from flx.benchmarks.identification import IdentificationResult

MAX_N = 20


def _get_rank_n_identification_rates(
    mated_ranks: list[int], max_n: int
) -> tuple[list[float], list[float]]:
    ranks_array = np.array(mated_ranks, dtype=int)
    identification_rates = []
    for n in tqdm.tqdm(range(1, max_n + 1)):
        rank_n_or_lower = np.sum(ranks_array <= n)
        identification_rates.append(rank_n_or_lower / len(mated_ranks))
    return identification_rates


def plot_rank_n_identification_rates(
    path: str,
    results: list[IdentificationResult],
    model_labels: list[str],
    plot_title: str,
) -> None:
    plt.close()  # In case some other function was not tidy
    seaborn.set_style("whitegrid")
    ax = plt.subplot()
    for label, res in zip(model_labels, results):
        identification_rates = _get_rank_n_identification_rates(
            res.get_mated_ranks(), MAX_N
        )
        seaborn.lineplot(
            x=list(range(1, MAX_N + 1)),
            y=[r * 100 for r in identification_rates],
            label=label,
            ax=ax,
        )

    plt.title(plot_title)
    plt.legend(loc="lower right")
    plt.xscale("linear")
    plt.xticks(list(range(1, MAX_N + 1)), [str(i) for i in range(1, MAX_N + 1)])
    plt.yscale("linear")
    plt.xlabel("Rank", fontweight="bold")
    plt.ylabel("Identificiation rate (in %)", fontweight="bold")
    plt.savefig(path + ".png")
    plt.savefig(path + ".pdf")
    plt.close()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/flx/visualization
文件名: show_with_opencv.py
--------------------------------------------------------------------------------
from os.path import splitext
import math

import torch
import cv2
import numpy as np

from flx.setup.config import INTERACTIVE_VIS


@torch.no_grad()
def _make_grid(images: list[np.ndarray], ncols: int) -> np.ndarray:
    """
    Orders images of equal size into a grid with ncol columns
    """
    nrows = math.ceil(len(images) / ncols)
    imshape = images[0].shape
    gridshape = (imshape[0] * nrows, imshape[1] * ncols)

    assert gridshape[0] < 4000
    assert gridshape[1] < 4000

    outim = np.zeros(shape=gridshape, dtype=np.uint8)
    imidx = 0
    for i in range(nrows):
        for j in range(ncols):
            if imidx < len(images):
                outim[
                    i * imshape[0] : (i + 1) * imshape[0],
                    j * imshape[1] : (j + 1) * imshape[1],
                ] = images[imidx]
            imidx += 1
    return outim


def _normalized_array_to_grayscale(arr: np.ndarray) -> np.ndarray:
    amin = arr.min()
    amax = arr.max()
    if amin < 0.0 or amax > 1.0 or amax - amin < 0.1:
        if amax - amin != 0:
            arr = (arr - amin) * (1 / (amax - amin))
    arr = arr * 255
    return arr.astype(np.uint8)


@torch.no_grad()
def _tensor_to_2Dnumpy_array(tensor: torch.Tensor) -> np.ndarray:
    tensor = tensor.to(torch.device("cpu"))
    arr = tensor.numpy()
    if len(arr.shape) != 2:
        new_shape = [s for s in arr.shape if s != 1]
        if len(new_shape) != 2:
            raise RuntimeError(
                f"show_tensor_as_image: Cannot convert tensor with shape {arr.shape} to 2D numpy array!"
            )
        arr = np.resize(arr, new_shape=new_shape)
    return arr


def show_minutia_maps(minu_maps: np.ndarray) -> None:
    """
    @param minu_maps : np.ndarray of type np.uint8 must have shape (height, width, n_layers)
    """
    if not INTERACTIVE_VIS:
        return
    n_layers = minu_maps.shape[2]
    for i in range(n_layers):
        cv2.imshow(
            f"Minutia map {int(i * 360 / n_layers):3} Degrees Orientation (0 is x Axis)",
            minu_maps[:, :, i],
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()


@torch.no_grad()
def show_minutia_maps_from_tensor(minu_maps: torch.Tensor) -> None:
    """
    @param minu_maps : [0, 1] normalized torch.Tensor; must have shape (n_layers, height, width)
    """
    if not INTERACTIVE_VIS:
        return
    for i, layer in enumerate(minu_maps[:]):
        cv2.imshow(
            f"Minutia map {int(i * 360 / minu_maps.shape[0]):3} Degrees Orientation (0 is x Axis)",
            _tensor_to_2Dnumpy_array(layer),
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()


@torch.no_grad()
def save_3Dtensor_as_image_grid(
    tensor: torch.Tensor, filename: str = "Image.png"
) -> None:
    imgs = []
    for i in range(tensor.shape[0]):
        imgs.append(_normalized_array_to_grayscale(_tensor_to_2Dnumpy_array(tensor[i])))
    ncols = math.ceil(math.sqrt(len(imgs)))
    cv2.imwrite(filename, _make_grid(imgs, ncols))

    with open(f"{splitext(filename)[0]}_cols_{ncols}", "w") as file:
        file.write(str(tensor.shape))


def save_2Darray_as_image(array: np.ndarray, filename: str = "Image.png") -> None:
    cv2.imwrite(
        filename, _normalized_array_to_grayscale(_normalized_array_to_grayscale(array))
    )


@torch.no_grad()
def save_2Dtensor_as_image(tensor: torch.Tensor, filename: str = "Image.png") -> None:
    cv2.imwrite(
        filename, _normalized_array_to_grayscale(_tensor_to_2Dnumpy_array(tensor))
    )


@torch.no_grad()
def show_tensor_as_image(
    tensor: torch.Tensor, winname: str = "Image", wait=True
) -> None:
    if not INTERACTIVE_VIS:
        return
    if len(tensor.shape) == 2:
        cv2.imshow(
            winname, _normalized_array_to_grayscale(_tensor_to_2Dnumpy_array(tensor))
        )
    elif len(tensor.shape) == 3:
        imgs = []
        for i in range(tensor.shape[0]):
            imgs.append(
                _normalized_array_to_grayscale(_tensor_to_2Dnumpy_array(tensor[i]))
            )
        ncols = math.ceil(math.sqrt(len(imgs)))
        cv2.imshow(winname, _make_grid(imgs, ncols=ncols))
    if wait:
        k = cv2.waitKey(0)
        cv2.destroyWindow(winname)
        if chr(k) == "q":
            exit(0)
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors
文件名: run_my_fvc.py
--------------------------------------------------------------------------------
import os
import sys
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from sklearn.metrics import roc_curve

sys.path.append(os.getcwd())
from flx.extractor.fixed_length_extractor import get_DeepPrint_TexMinu
from flx.setup.datasets import get_fvc2004_db1a
from flx.data.image_helpers import pad_and_resize_to_deepprint_input_size
from flx.data.image_loader import FVC2004Loader

# ========================= 路径配置 =========================
MODEL_PATH = "./models/DeepPrint_Tex_512/best_model.pyt"
DATA_DIR   = "./data/fingerprints/FVC2000/Dbs/Db1_a"

# ========================= 视角增强（TTA） =========================
TTA_SCALES = [340, 370, 400, 430]
TTA_ANGLES = [-15, 0, 15]
V = len(TTA_SCALES) * len(TTA_ANGLES)

# ========================= 融合参数 =========================
TARGET_FMR = 2e-5
ALPHA       = 5.0

TEX_PAIR_TOPK  = 6

TEX_SPIKE_CLIP_ENABLE = True
TEX_DELTA = 0.08

# ============================================================
#                     评估工具
# ============================================================

def compute_eer_from_scores(y_scores, y_true):
    fpr, tpr, thr = roc_curve(y_true, y_scores, pos_label=1)
    fnr = 1 - tpr
    idx = np.nanargmin(np.abs(fpr - fnr))
    eer = (fpr[idx] + fnr[idx]) / 2.0
    return float(eer), float(thr[idx])

def eval_at_target_from_scores(y_scores, y_true, target_fmr):
    fpr, tpr, thr = roc_curve(y_true, y_scores, pos_label=1)
    fnr = 1 - tpr
    
    order = np.argsort(fpr)
    fpr, fnr, thr = fpr[order], fnr[order], thr[order]

    k = np.searchsorted(fpr, target_fmr)
    k = min(max(k, 1), len(fpr) - 1)

    w = (target_fmr - fpr[k-1]) / (fpr[k] - fpr[k-1] + 1e-12)
    fnr_i = (1 - w) * fnr[k-1] + w * fnr[k]
    thr_i = (1 - w) * thr[k-1] + w * thr[k]
    return float(fnr_i), float(thr_i)

# ============================================================
#                   图像预处理
# ============================================================

def make_loader(crop_size_target, angle_target):
    @staticmethod
    def _load(filepath):
        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
        if img is None:
            raise ValueError(f"Failed to load image: {filepath}")
            
        img = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8)).apply(img)

        if angle_target != 0:
            h, w = img.shape
            M = cv2.getRotationMatrix2D((w // 2, h // 2), angle_target, 1.0)
            img = cv2.warpAffine(img, M, (w, h), borderValue=255)

        blur = cv2.GaussianBlur(img, (5, 5), 0)
        _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        coords = np.argwhere(th > 0)

        cy, cx = img.shape[0] // 2, img.shape[1] // 2
        if len(coords) > 0:
            cy, cx = coords.mean(0).astype(int)

        cs = crop_size_target
        sy = max(0, min(cy - cs // 2, img.shape[0] - cs))
        sx = max(0, min(cx - cs // 2, img.shape[1] - cs))
        img = img[sy:sy + cs, sx:sx + cs]

        return pad_and_resize_to_deepprint_input_size(img, fill=1.0)

    return _load

# ============================================================
#                         模型与特征
# ============================================================

@torch.no_grad()
def load_model():
    extractor = get_DeepPrint_TexMinu(8000, 256)
    ckpt = torch.load(MODEL_PATH, map_location="cpu")
    sd = ckpt["model_state_dict"] if "model_state_dict" in ckpt else ckpt

    md = extractor.model.state_dict()
    md.update({k: v for k, v in sd.items() if k in md and v.shape == md[k].shape})
    extractor.model.load_state_dict(md)
    extractor.model.eval().cuda()
    return extractor

@torch.no_grad()
def extract_views_tex_minu(extractor):
    tex_views, minu_views = [], []
    for s in TTA_SCALES:
        for a in TTA_ANGLES:
            print(f"[View] scale={s} angle={a}")
            FVC2004Loader._load_image = make_loader(s, a)
            ds = get_fvc2004_db1a(DATA_DIR)
            
            if len(ds) == 0:
                raise ValueError("Dataset is empty. Check DATA_DIR.")

            tex, minu = extractor.extract(ds)

            t = tex._array if torch.is_tensor(tex._array) else torch.from_numpy(tex._array)
            m = minu._array if torch.is_tensor(minu._array) else torch.from_numpy(minu._array)

            tex_views.append(F.normalize(t.cuda(), p=2, dim=1))   # [800,256]
            minu_views.append(F.normalize(m.cuda(), p=2, dim=1))  # [800,256]

    return torch.stack(tex_views, dim=1), torch.stack(minu_views, dim=1)  # [800,V,256]

# ============================================================
#          multi-view view-pair robust score
# ============================================================

@torch.no_grad()
def viewpair_topkmean(enroll_views, probe_views, topk=6):
    sim = torch.einsum("nvd,mwd->nmvw", enroll_views, probe_views)  # [N,M,V,V]
    flat = sim.reshape(sim.shape[0], sim.shape[1], -1)             # [N,M,V*V]
    kk = min(topk, flat.shape[-1])
    return flat.topk(kk, dim=-1).values.mean(dim=-1)               # [N,M]

@torch.no_grad()
def viewpair_gapaware(enroll_views, probe_views, lam=0.35):
    sim = torch.einsum("nvd,mwd->nmvw", enroll_views, probe_views)  # [N,M,V,V]
    flat = sim.reshape(sim.shape[0], sim.shape[1], -1)             # [N,M,V*V]
    top2 = flat.topk(2, dim=-1).values                              # [N,M,2]
    top1 = top2[..., 0]
    top2v = top2[..., 1]
    gap = top1 - top2v
    return top1 - lam * gap                                         # [N,M]

@torch.no_grad()
def viewpair_top2_clip(enroll_views, probe_views, delta=0.08):
    sim = torch.einsum("nvd,mwd->nmvw", enroll_views, probe_views)
    flat = sim.reshape(sim.shape[0], sim.shape[1], -1)
    top2 = flat.topk(2, dim=-1).values
    m1 = top2[..., 0]
    m2 = top2[..., 1]
    return torch.minimum(m1, m2 + float(delta))                    # [N,M]

# ============================================================
#                       主流程
# ============================================================

def main():
    extractor = load_model()

    # 1. 提取所有 800 张图的特征 (GPU)
    # 结构: [800, V, 256] -> 800个样本(100指x8次), V个视角, 256维
    tex_views, minu_views = extract_views_tex_minu(extractor)   
    device = tex_views.device

    y_scores_gen = []
    y_scores_imp = []

    print("\n================ 8-FOLD 1:1 EVALUATION ================\n")
    print("Mode: One Enroll vs All Others (Leave-One-Out)")
    
    # 8折交叉验证：每次取每个手指的第 k 次按压作为注册（Template），其余作为查询（Probe）
    for k in range(8):
        # --- 构建索引 ---
        # 注册集: 100个手指，每个手指取第 k 张
        enroll_idx = (torch.arange(100, device=device) * 8 + k)  # [100]
        enroll_labels = torch.arange(100, device=device)         # [100] 对应ID 0..99
        
        # 查询集: 100个手指，每个手指取除了 k 以外的 7 张
        probe_idx_list = []
        probe_label_list = []
        for f in range(100):
            base = f * 8
            for imp in range(8):
                if imp == k: continue 
                probe_idx_list.append(base + imp)
                probe_label_list.append(f)

        probe_idx = torch.tensor(probe_idx_list, device=device, dtype=torch.long)      # [700]
        probe_labels = torch.tensor(probe_label_list, device=device, dtype=torch.long) # [700]

        # --- 提取对应的特征子集 ---
        enroll_tex = tex_views[enroll_idx]      # [100, V, 256]
        probe_tex  = tex_views[probe_idx]       # [700, V, 256]
        enroll_minu = minu_views[enroll_idx]
        probe_minu  = minu_views[probe_idx]

        # --- 计算 N x M 相似度矩阵 ---
        # 这一步计算了 100个注册样本 与 700个查询样本 之间的所有可能两两配对
        if TEX_SPIKE_CLIP_ENABLE:
            s_tex_mat = viewpair_top2_clip(enroll_tex, probe_tex, delta=TEX_DELTA) 
        else:
            s_tex_mat = viewpair_topkmean(enroll_tex, probe_tex, topk=TEX_PAIR_TOPK)

        s_minu_mat = viewpair_gapaware(enroll_minu, probe_minu)
        
        # 融合分数矩阵
        # [100, 700]
        scores_mat = s_tex_mat + float(ALPHA) * s_minu_mat
        
        # --- 利用掩码提取 1:1 配对 ---
        # enroll_labels: [100], probe_labels: [700]
        mask_genuine = (enroll_labels.unsqueeze(1) == probe_labels.unsqueeze(0)) # [100, 700]
        
        # 提取正样本分数 (Same Finger, Different Impression)
        # 数量: 100 * 7 = 700 个
        gen_scores = scores_mat[mask_genuine].detach().cpu().numpy()
        y_scores_gen.append(gen_scores)
        
        # 提取负样本分数 (Different Finger)
        # 数量: 100 * 700 - 700 = 69,300 个
        imp_scores = scores_mat[~mask_genuine].detach().cpu().numpy()
        y_scores_imp.append(imp_scores)

        print(f"[Fold {k}] Gen: {len(gen_scores)} pairs | Imp: {len(imp_scores)} pairs")

    y_scores_gen = np.concatenate(y_scores_gen) # Total: 5,600
    y_scores_imp = np.concatenate(y_scores_imp) # Total: 554,400
    
    y_scores = np.concatenate([y_scores_gen, y_scores_imp])
    y_true = np.concatenate([np.ones_like(y_scores_gen), np.zeros_like(y_scores_imp)])

    print("\n================ 1:1 RESULT ================")
    for t, name in [(0.05, "5%"), (1e-3, "1/1k"), (1e-4, "1/10k"), (2e-5, "1/50k")]:
        fn, th = eval_at_target_from_scores(y_scores, y_true, t)
        print(f"{name:<6}  FNMR={fn*100:6.2f}%  thr≈{th:.4f}")

    eer, eer_thr = compute_eer_from_scores(y_scores, y_true)
    fn_50k, th_50k = eval_at_target_from_scores(y_scores, y_true, TARGET_FMR)

    print("-------------------------------------------------------------")
    print(f"[@1/50k] FNMR={fn_50k*100:.2f}% thr≈{th_50k:.4f}")
    print(f"EER = {eer*100:.2f}%   (threshold ≈ {eer_thr:.4f})")
    print("=============================================================\n")

if __name__ == "__main__":
    main()
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors
文件名: setup.py
--------------------------------------------------------------------------------
from setuptools import setup, find_packages

setup(name="fingerprint-flx", version="1.0", packages=find_packages())
================================================================================

================================================================================
文件路径: /home/tom/fingerprint-deepleaning/fixed-length-fingerprint-extractors/fixed-length-fingerprint-extractors/tests
文件名: test_datasets.py
--------------------------------------------------------------------------------
from os.path import join, dirname
from os import remove

import torch
import cv2
import numpy as np

from flx.data.transformed_dataset import TransformedImageLoader
from flx.data.dataset import Identifier, IdentifierSet
from flx.data.label_index import LabelIndex
from flx.data.embedding_loader import (
    EmbeddingLoader,
)
from flx.data.image_loader import (
    ImageLoader,
    SFingeLoader,
    FVC2004Loader,
    NistSD4Dataset,
    MCYTOpticalLoader,
    MCYTCapacitiveLoader,
)
from flx.data.minutia_map_loader import (
    SFingeMinutiaMapLoader,
    MCYTOpticalMinutiaMapLoader,
    MCYTCapacitiveMinutiaMapLoader,
)
from flx.data.pose_dataset import PoseLoader
from flx.setup.paths import get_fingerprint_dataset_path, get_pose_dataset_path
from flx.image_processing.augmentation import (
    RandomPoseTransform,
    PoseTransform,
    RandomQualityTransform,
)
from flx.visualization.show_with_opencv import (
    show_tensor_as_image,
    show_minutia_maps_from_tensor,
)

TEST_DATA_DIR = join(dirname(__file__), "data")


def assert_is_image_tensor(tensor: torch.Tensor) -> None:
    assert tensor.ndim == 3
    assert tensor.shape[0] == 1 or tensor.shape[0] == 0
    assert tensor.shape[1] > 0
    assert tensor.shape[2] > 0


def assert_identifiers(ds: IdentifierSet, num_subjects: int, num_samples: int):
    assert len(ds) == num_samples
    assert ds.num_subjects == num_subjects


def test_identifier_set():
    all_ids = [
        Identifier(4, 1),
        Identifier(3, 1),
        Identifier(5, 2),
        Identifier(3, 0),
        Identifier(1, 2),
        Identifier(4, 0),
        Identifier(1, 0),
        Identifier(1, 1),
        Identifier(2, 0),
        Identifier(2, 1),
        Identifier(5, 0),
        Identifier(5, 1),
    ]

    ds = IdentifierSet(all_ids)
    assert_identifiers(ds, 5, 12)

    # Test filter by subject
    subset = ds.filter_by_subject([2, 4])
    assert_identifiers(subset, 2, 4)
    assert subset[0] == Identifier(2, 0)
    assert subset[1] == Identifier(2, 1)
    assert subset[2] == Identifier(4, 0)
    assert subset[3] == Identifier(4, 1)

    print("\nFilter by id")
    print("Ids for subset " + " ".join(str(i) for i in all_ids[4:6]))
    subset = ds.filter_by_id(IdentifierSet(all_ids[4:6]))
    assert_identifiers(subset, 2, 2)
    assert subset[0] == all_ids[4]
    assert subset[1] == all_ids[5]

    subset = ds.filter_by_index(range(3))
    assert_identifiers(subset, 1, 3)
    assert subset[0] == Identifier(1, 0)
    assert subset[1] == Identifier(1, 1)
    assert subset[2] == Identifier(1, 2)


def test_label_dataset():
    label_index: LabelIndex = LabelIndex(
        IdentifierSet(
            [
                Identifier(4, 1),
                Identifier(3, 1),
                Identifier(5, 2),
                Identifier(3, 0),
                Identifier(1, 2),
            ]
        )
    )
    assert len(label_index.ids) == 5
    assert label_index.ids.num_subjects == 4
    assert label_index.get(Identifier(1, 2)) == 0
    assert label_index.get(Identifier(3, 0)) == 1
    assert label_index.get(Identifier(3, 1)) == 1
    assert label_index.get(Identifier(4, 1)) == 2
    assert label_index.get(Identifier(5, 2)) == 3


def test_embedding_dataset():
    ids = [Identifier(0, 0), Identifier(0, 1), Identifier(1, 0), Identifier(1, 1)]
    embeddings = np.array([np.random.random(4) for _ in ids])

    loader = EmbeddingLoader(IdentifierSet(ids), embeddings)
    for id, emb in zip(loader.ids, loader.numpy()):
        print(id)
        print(emb)

    dataset2 = EmbeddingLoader.combine(loader, loader)
    for id, emb in zip(dataset2.ids, dataset2.numpy()):
        print(id)
        print(emb)


def test_minutia_map_sfinge():
    loader = SFingeMinutiaMapLoader(join(TEST_DATA_DIR, "SFingeExample"))
    minutia_map, mask = loader.get(Identifier(0, 0))
    print(minutia_map.shape)
    show_minutia_maps_from_tensor(minutia_map)


def test_minutia_map_mcyt_capactive():
    loader = MCYTCapacitiveMinutiaMapLoader(
        join(TEST_DATA_DIR, "MCYTCapacitiveExample")
    )
    minutia_map, mask = loader.get(Identifier(0, 0))
    print(minutia_map.shape)
    show_minutia_maps_from_tensor(minutia_map)


def test_minutia_map_mcyt_optical():
    loader = MCYTOpticalMinutiaMapLoader(join(TEST_DATA_DIR, "MCYTOpticalExample"))
    minutia_map, mask = loader.get(Identifier(0, 8))
    print(minutia_map.shape)
    show_minutia_maps_from_tensor(minutia_map)


def test_pose_dataset():
    def make_random_pose_dataset(
        ids: list[Identifier], pose_distribution: RandomPoseTransform
    ) -> PoseLoader:
        return PoseLoader(ids, [pose_distribution.sample() for _ in ids])

    print("Testing PoseDataset")
    image_loader = SFingeLoader(join(TEST_DATA_DIR, "SFingeExample"))
    pose_loader = make_random_pose_dataset(
        image_loader.ids, pose_distribution=RandomPoseTransform()
    )
    pose_loader.save(get_pose_dataset_path("PoseTest"))

    pose_dataloader = PoseLoader.load(get_pose_dataset_path("PoseTest"))
    for bid in image_loader.ids:
        fp = image_loader.get(bid)
        show_tensor_as_image(
            pose_loader.get(bid)(fp), winname=f"Random pose: {bid}", wait=False
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    remove(get_pose_dataset_path("PoseTest"))


def test_transformed_dataset():
    images = SFingeLoader(join(TEST_DATA_DIR, "SFingeExample"))

    transformed_images = TransformedImageLoader(images)
    for bid in transformed_images.ids:
        show_tensor_as_image(
            transformed_images.get(bid),
            winname=f"NoPoseTrafo, NoQualityTrafo: {bid}",
            wait=False,
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    transformed_images = TransformedImageLoader(
        images, poses=RandomPoseTransform(), transforms=[RandomQualityTransform()]
    )
    for bid in transformed_images.ids:
        show_tensor_as_image(
            transformed_images.get(bid),
            winname=f"PoseDistribution, QualityDistribution: {bid}",
            wait=False,
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    pose_loader = PoseLoader(
        images.ids,
        [
            PoseTransform(pad=80, angle=i / len(images.ids) * 90)
            for i in range(len(images.ids))
        ],
    )
    transformed_images = TransformedImageLoader(images, poses=pose_loader)
    for bid in transformed_images.ids:
        show_tensor_as_image(
            transformed_images.get(bid),
            winname=f"PoseDataset, NoQualityTrafo: {bid}",
            wait=False,
        )
    cv2.waitKey(0)
    cv2.destroyAllWindows()


def _test_image_dataset(
    dataset_type: type,
    root_dir_name: str,
    expected_num_subjects: int,
    expected_num_samples: int,
):
    if root_dir_name.startswith("test_data_dir:"):
        root_dir_name = join(TEST_DATA_DIR, root_dir_name[14:])
    dataset: ImageLoader = dataset_type(get_fingerprint_dataset_path(root_dir_name))
    assert dataset.ids.num_subjects == expected_num_subjects
    assert len(dataset.ids) == expected_num_samples
    image: torch.Tensor = dataset.get(dataset.ids[0])
    assert_is_image_tensor(image)
    show_tensor_as_image(image, f"{root_dir_name}", wait=True)


def test_dataset_SFingeExample():
    _test_image_dataset(SFingeLoader, "test_data_dir:SFingeExample", 4, 4 * 2)


# # Uncomment to test that the datasets are present on the system
#
# def test_dataset_SFingev2ValidationSeparateSubjects():
#     _test_image_dataset(SFingeDataset, "SFingev2ValidationSeparateSubjects", 2000, 2000 * 4)
#
# def test_dataset_SFingev2():
#     _test_image_dataset(SFingeDataset, "SFingev2", 8000, 8000 * 10)
#
# def test_dataset_MCYT330_Optical():
#     _test_image_dataset(MCYTOpticalDataset, "mcyt330_optical", 3300, 3300 * 12)
#
# def test_dataset_MCYT330_Capacitive():
#     _test_image_dataset(MCYTCapacitiveDataset, "mcyt330_capacitive", 3300, 3300 * 12)
================================================================================

